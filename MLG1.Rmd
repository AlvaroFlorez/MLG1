--- 
title: "Notas de clase: Modelo lineal general I"
author: "Alvaro J. Flórez"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
        bookdown::pdf_document2
bibliography: [ModeloLineal.bib]
biblio-style: apalike
link-citations: yes

header-includes:
  - \newcommand{\bx}{\boldsymbol x}
  - \newcommand{\bX}{\boldsymbol X}
  - \newcommand{\bZERO}{\boldsymbol 0}
  - \newcommand{\bONE}{\boldsymbol 1}
  - \newcommand{\tr}{\mbox{tr}}
  - \newcommand{\bb}{\boldsymbol b}
  - \newcommand{\hatbb}{\widehat{\bb}}
  - \newcommand{\bc}{\boldsymbol c}
  - \newcommand{\bC}{\boldsymbol C}
  - \newcommand{\be}{\boldsymbol e}
  - \newcommand{\bH}{\boldsymbol H}
  - \newcommand{\bI}{\boldsymbol I}
  - \newcommand{\br}{\boldsymbol r}
  - \newcommand{\bR}{\boldsymbol R}
  - \newcommand{\bT}{\boldsymbol T}
  - \newcommand{\by}{\boldsymbol y}
  - \newcommand{\bZ}{\boldsymbol Z}
  - \newcommand{\bV}{\boldsymbol V}
  - \newcommand{\bW}{\boldsymbol W}
  - \newcommand{\bz}{\boldsymbol z}
  - \newcommand{\tildey}{\widetilde{y}}
  - \newcommand{\tildeby}{\widetilde{\by}}
  - \newcommand{\haty}{\widehat{y}}
  - \newcommand{\hatby}{\widehat{\by}}
  - \newcommand{\bbeta}{\boldsymbol \beta}
  - \newcommand{\hatbeta}{\widehat{\beta}}
  - \newcommand{\hatbbeta}{\widehat{\bbeta}}
  - \newcommand{\bgamma}{\boldsymbol \gamma}
  - \newcommand{\hatmu}{\widehat{\mu}}
  - \newcommand{\hatsigma}{\widehat{\sigma}}
  - \newcommand{\hatlambda}{\widehat{\lambda}}
  - \newcommand{\boldtheta}{\boldsymbol \theta}
  - \newcommand{\bvarepsi}{\boldsymbol \varepsilon}
  - \renewcommand{\figurename}{Figura}
  - \renewcommand{\tablename}{Tabla}
---

# Introducción {-}
Estas son las notas de clase del curso Modelo Lineal General I. Las temáticas que se tratan son:

1. Modelo lineal simple
2. Modelo lineal múltiple
3. Evaluación de los supuestos del modelo lineal
4. Transformaciones y mínimos cuadrados ponderados
5. Evaluación de puntos influyentes y atípicos

Tenga en cuenta que el propósito de estas notas de clase no es reemplazar los textos guías. Para el estudio más detallado de los temas revisados, se recomiendan las siguientes lecturas:

- *Introduction to Linear Regression Analysis*, Fifth Ed., 2012, by Montgomery, D. C., Peck, E. A. and Vining, G. G. **(Texto guía)**
- *Applied Regression Analysis*, Third Ed., 1998, by Draper, N. R. and Smith, H., Wiley.
- *Theory and Applications of the Linear Models*, 2000, by Graybill, F. A., Duxbury.
- *Applied Linear Statistical Models*, Fifth Ed., 2005, by Kutner, M. H, Nachtsheim, C. J., Neter, J. and Li, W., McGraw-Hill.
- *Análisis de Regresión. Introducción Teórica y Práctica basada en R*, 2011, by F. Tusell.
- *Applied Linear Regression*, Fourth Ed.,  2014, by S. Weisberg.
- *Applied Regression Analysis \& Generalized Linear Models*, 2016, by J. Fox.

<!--chapter:end:index.Rmd-->

# Modelo lineal simple

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::write_bib(c("knitr", "stringr"), "", width = 60)
```

```{r birthweight0, echo=FALSE,include = FALSE}
library(pBrackets)
birthweight = read.csv("birthweight.csv",header = T)
```

## Datos de peso al nacer {-}
Los datos `birthweight` (Disponible [aquí](https://raw.githubusercontent.com/AlvaroFlorez/MLG1/master/Birthweight.csv){target="_blank"}) contienen el peso y la edad gestacional de $42$ recién nacidos. El objetivo del estudio es investigar cómo la edad gestational del feto influyen en el peso al nacer durante las últimas semanas del embarazo. Aunque la base de datos contiene otras variables, por ahora solo consideramos el peso y la edad gestacional.

La Figura \@ref(fig:birthweightFigure) muestra la relación entre el peso (en kilogramos) y la edad gestacional (en semanas) del recién nacido. Por medio de este gráfico vemos que hay una relación aproximadamente lineal positiva (la correlación es igual a `r round(cor(birthweight$weight,birthweight$age),2)`). Es decir que cuando la edad gestacional aumenta, el peso del recién nacido también lo hace. Por lo tanto, sería razonable describir el valor esperado del peso al nacer como una función lineal de la edad gestacional:
\[
E(\mbox{weight}|\mbox{age}=x) = \beta_{0} + \beta_{1}x.
\]

La Figura \@ref(fig:birthweightFigure) se puede hacer con el siguiente código:
```{r birthweightFigure, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Gráfico de dispersion del peso del recien nacido y la edad gestacional."}
birthweight = read.csv("birthweight.csv",header = T)

plot(weight~age,data=birthweight,pch=20,xlab="Edad gestacional(semanas)",
     ylab='Peso(kilogramos)')
```
Con este conjunto de datos podemos plantear las siguientes preguntas:

- ¿Cómo afecta la edad gestional al peso del neonato?
- Si la edad gestacional aumenta en una unidad ¿en cuanto aumenta el peso del recién nacido? ¿ese aumento puede considerarse significativo?
- ¿Se puede predecir el peso al nacer por medio de la edad gestacional?

Estas preguntas se pueden resolver a partir de un análisis de regresión lineal.

## Regresion lineal simple
En un análisis de regresión simple estamos interesados en modelar la relación entre una variable de entrada (regresor, variable independiente o covariable) $X$ y una variable de salida (respuesta o variable dependiente) $Y$. Este modelo nos permite:

- Evaluar cuanto cambia el valor esperado de $Y$ debido a cambios en $X$,
- Predecir $Y$ (o su valor esperado) en función de $X$.

## Modelo lineal simple
Sea $(y_{i},x_{i})$ la i-ésima observación de la variable respuesta $(y)$ y la covariable $(x)$, para $i=1,\ldots,n$, con $n$ igual al número total de observaciones. **El modelo lineal simple** se puede expresar de la forma:
\[
y_{i} = \beta_{0} + x_{i}\beta_{1}+\varepsilon_{i},
\]
donde $\beta_{0}$ es el intercepto, $\beta_{1}$ es la pendiente y $\varepsilon_{i}$ es el componente de error. Generalmente, los supuestos que acompañan este modelo son:

a. $E(\varepsilon_{i}) = 0$,
b. $V(\varepsilon_{i}) = \sigma^{2}$,
c. $cov(\varepsilon_{i},\varepsilon_{j}) = 0$, para todo $i \neq j$,
d. $\varepsilon_{i}$ se distribuye normalmente.

Por lo tanto, $\varepsilon_{i} \sim N(0,\sigma^{2})$. 

A partir de (a) se tiene que:
\[
E(Y|X=x_i) = \beta_{0} + x_i\beta_{1}.
\]
Entonces, para $x=0$, el valor esperado de $Y$ es igual a $\beta_0$. Cuando $X$ incrementa en una unidad (de $x$ a $x+1$), el valor esperado de $Y$ incrementa en:
\[
E(Y|X=x+1) - E(Y|X=x) = \beta_{0} + (x+1)\beta_{1} - (\beta_{0} + x\beta_{1}) = \beta_{1}.
\]
Lo que indica que $\beta_{1}$ representa el cambio en el valor esperado de $Y$ por un cambio unitario en $X$.

Dado (b), tenemos que $V(Y|X=x_{i}) = \sigma^{2}$. Es decir que para cualquier valor de $x$, la varianza de $Y$ es la misma (homocedasticidad). Puesto que los errores están incorrelacionados (c), entonces las observaciones de $Y$ también lo están. 

Debido a (d), tenemos que la variable respuesta se distribuye de forma normal. Específicamente, tenemos que: $y_{i} \sim N(\beta_{0} + \beta_{1}x_i, \sigma^{2})$.

El proceso de generación de datos del modelo de regresión lineal se ilustra en la Figura \@ref(fig:gendataFigure). El valor esperado de $Y|X$ está representado por la linea negra. Tenemos que, cada observación de $y$ (puntos negros) es una realización de la distribución de $Y|X=x_{i} \sim N(\beta_{0}+\beta_{1}x_{i},\sigma^{2})$ (curvas rojas). En esté ejemplo, suponemos que $\beta_{0}=0$, $\beta_{1}=1$, y $\sigma=0.1$.

```{r gendata, echo=FALSE}
x1 = 0.6+dnorm(seq(0.3,0.9,length.out = 100),0.6,0.1)*0.03
y1 = seq(0.3,0.9,length.out = 100)
x2 = 0.3+dnorm(seq(0,0.6,length.out = 100),0.3,0.1)*0.03
y2 = seq(0,0.6,length.out = 100)
```

```{r gendataFigure,echo=FALSE, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Proceso generador datos del modelo lineal simple."}
plot(NULL,NULL,ylim=c(0,1),xlim=c(0,1),xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(0,1,lwd=1.5)
points(0.6,0.4,pch=20)
points(0.3,0.4,pch=20)
lines(x1,y1,col=2)
segments(0.6,0.3,0.6,0.9,lty=3)
segments(0.6,0.6,max(x1),0.6,lty=2)
lines(x2,y2,col=2)
segments(0.3,0,0.3,0.6,lty=3)
segments(0.3,0.3,max(x2),0.3,lty=2)
text(max(x2)-0.015,0.25,pos=4,bquote(Y~'|'~x~'='~x[1]~'~'~'N(0.3,0.1)'),cex=0.7,col=2)
text(max(x1)-0.015,0.6,pos=4,bquote(Y~'|'~x~'='~x[2]~'~'~'N(0.6,0.1)'),cex=0.7,col=2)
text(0.6,0.4,pos=2,bquote('('~y[2]~','~x[2]~')'),cex=0.7)
text(0.3,0.4,pos=2,bquote('('~y[1]~','~x[1]~')'),cex=0.7)
```

## Estimación de los parámetros
Los parámetros $\beta_{0}$ y $\beta_{1}$ son desconocidos y deben estimarse a partir de los datos. Para esto utilizamos el método de **mínimos cuadrados ordinarios (MCO)**. 

La función objetivo es la siguiente:
\begin{equation}
S(\beta_{0},\beta_{1}) = \sum_{i=1}^{n} \left( y_{i} - \beta_{0} - \beta_{1}x_{i} \right)^{2} = \sum_{i=1}^{n} e_{i}^{2}.
 (\#eq:rss)
\end{equation}
Entonces, tenemos que encontrar la combinación de $\beta_{0}$ y $\beta_{1}$ que minimizan \@ref(eq:rss). Para esto, primero debemos derivar $S(\beta_{0},\beta_{1})$ con respecto a $\beta_{0}$ y $\beta_{1}$, e igualar estas ecuación a cero. De esta forma obtenemos las **ecuaciones normales**:
\begin{equation}
\frac{\partial S(\beta_{0},\beta_{1})}{\partial \beta_{0}} = - 2\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i} \right),
(\#eq:normeq1)
\end{equation}
y
\begin{equation}
\frac{\partial S(\beta_{0},\beta_{1})}{\partial \beta_{1}} = - 2\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1}x_{i} \right)x_{i}.
(\#eq:normeq2)
\end{equation}

Los estimadores por MCO ($\widehat{\beta}_{0}$ y $\widehat{\beta}_{1}$) se obtienen resolviendo el sistema de ecuaciones \@ref(eq:normeq1) y \@ref(eq:normeq2):
\[
\widehat{\beta}_{0} = \bar{y} - \widehat{\beta}_{1}\bar{x},
\]
y
\[
\widehat{\beta}_{1} = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} = \frac{S_{xy}}{S_{xx}} =  cor(X,Y)\frac{s_{Y}}{s_{X}},
\]
donde $s_{X}$ y $s_{Y}$ son las desviaciones estándar muestrales de $X$ y $Y$.

La diferencia entre el valor observado $(y_{i})$ y el valor ajustado correspondiente $(\widehat{y}_{i})$ es llamado **residuo** (o residual):
\[
e_{i}= y_{i} - (\hatbeta_{0} + \hatbeta_{1}x_{i}) = y_{i}-\haty_{i}, \mbox{ para }i=1,\ldots,n.
\]
La Figura \@ref(fig:Figresiduos) presenta los residuos de forma gráfica. Estos juegan un papel importante para la evaluar la bondad del ajuste del modelo (detectar posibles desviaciones a los supuestos asumidos).

```{r Figresiduos, echo=FALSE, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Diagrama gráfico de un ajuste por MCO. La recta representa la estimación por MCO, y las lineas discontinuas verticales entre los puntos observados y la recta estimada son los residuos."}
set.seed(5)     
x<-1:10
y<-3*x + 2 + rnorm(10,0,6)
m<-lm(y ~ x)
yhat<-m$fitted.values
diff<-y-yhat  
plot(x,y,xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(m,col=2)
segments(x, y, x, predict(m),lty=2)
```

## Estimación de $\sigma^{2}$
La estimación de $\sigma^{2}$ se hace a partir de la suma de cuadrados de los residuos:
\[
SS_{res} = \sum_{i=1}^{n}e^{2}_{i} = \sum_{i=1}^{n} (y_{i}-\widehat{\beta}_{0} - \widehat{\beta}_{1}x_{i})^{2}.
\]
El valor esperado de $SS_{res}$ es $E(SS_{res})=(n-2)\sigma^{2}$ (Para el caso de regresión múltiple, ver Sección C.3 de @montgomery_introduction_2012). Por lo tanto, un estimador insesgado de $\sigma^{2}$ es:
\[
\hatsigma^{2} = \frac{SS_{res}}{n-2} = MS_{res}.
\]
La cantidad $MS_{res}$ es llamada **cuadrado medio de los residuos**.

\rule{\textwidth}{0.4pt}

### Datos de peso al nacer. Modelo y estimación de parametros {-}
Para los datos de peso al nacer se propone el siguiente modelo:
\begin{equation}
\mbox{weight}_{i}= \beta_{0} + \beta_{1}\mbox{age}_{i} + \varepsilon_{i},
(\#eq:modBirthweight)
\end{equation}
donde $\varepsilon_{i} \sim N(0,\sigma^{2})$.

En R, la estimación por MCO se realiza a través de la función `lm`:
```{r MCO1}
mod = lm(weight~age, data=birthweight)
mod
```
De aquí obtenemos que $\widehat{\beta}_{0} =`r round(mod$coeff[1],2)`$ y $\widehat{\beta}_{1} =`r round(mod$coeff[2],2)`$. Note que la estimación del intercepto es negativa, lo que es físicamente imposible. Además tampoco tiene sentido una edad gestacional igual a cero. Por lo cual, este parámetro no tiene interpretación en este caso. $\beta_{0}$ solo tiene interpretación cuando las observaciones de $x$ están alrededor de cero.

A partir de $\hatbeta_{1}$, podemos concluir que la edad gestacional tiene un efecto positivo sobre el peso del recién nacido (el coeficiente estimado es positivo). Por cada incremento de una semana en la edad gestacional, el valor esperado del peso del recién nacido aumenta `r round(mod$coeff[2],2)` kilogramos.

La representación gráfica del modelo estimado se presenta en la Figura \@ref(fig:FigMCO2).

```{r FigMCO2,fig.height = 3.5, fig.width = 4,fig.align = "center",fig.cap = "Gráfico de dispersion del peso del recien nacido y la edad gestacional. La linea representa la estimación por MCO."}
plot(weight~age,data=birthweight,pch=20,xlab="edad gestacional(semanas)",
     ylab='peso(kilogramos)')
abline(mod,lwd=2)
```
La estimación de $\sigma$ es:
```{r MCO3}
 sqrt(sum(mod$residuals^2)/40)
```

\rule{\textwidth}{0.4pt}

## Propiedades de los estimadores por MCO
Los estimadores de $\hatbeta_{0}$ y $\hatbeta_{1}$ son una combinación lineal de las observaciones:
\begin{equation}
\hatbeta_{1} = \sum_{i=1}^{n} \frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} = \sum_{i=1}^{n} \frac{(x_{i}-\bar{x})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}y_{i} = \sum_{i=1}^{n}c_{i}y_{i}.
(\#eq:beta1cl)
\end{equation}
y 
\[
\hatbeta_{0} = \bar{y} - \hatbeta_{1}\bar{x} = \sum_{i=1}^{n}\left( \frac{1}{n} - c_{i}\bar{x} \right)y_{i} = \sum_{i=1}^{n}d_{i}y_{i}.
\]
Además, se tiene que $\sum_{i=1}^{n}c_{i}=0$ y $\sum_{i=1}^{n}c_{i}x_{i}=1$. Los valores ajustados también son combinaciones lineales de los datos: 
\[
\widehat{y}_{i} = \hatbeta_{0} + \hatbeta_{1}x_{i} = \sum_{i=1}^{n}(d_{i}+c_{i}x_{i})y_{i}.
\]
Puesto que los estimadores de $\beta_{0}$ y $\beta_{1}$ dependen de los errores, estos también son variables aleatorias. Por lo tanto debemos calcular el valor esperado y varianza de $\hatbeta_{0}$ y $\hatbeta_{1}$. 

Si los supuestos del modelo se cumplen, tenemos que el valor esperado de $\hatbeta_{1}$ es:
\begin{equation}
\begin{split}
E(\hatbeta_{1}) &= E\left( \sum_{i=1}^{n}c_{i}y_{i} \right) = \sum_{i=1}^{n}c_{i}E(y_{i}) = \sum_{i=1}^{n} c_{i}(\beta_{0} + \beta_{1}x_{i}) \\
&=\beta_{0}\sum_{i=1}^{n}c_{i} + \beta_{1}\sum_{i=1}^{n}c_{i}x_{i} = \beta_{1}.
\end{split}
\nonumber
\end{equation}
El valor esperado de $\hatbeta_{0}$ es:
\begin{equation}
\begin{split}
E(\hatbeta_{0}) &= E\left(\bar{y} - \hatbeta_{1}\bar{x} \right) = \frac{1}{n}\sum_{i=1}^{n}E(y_{i}) - \beta_{1}\bar{x} \\
&= \frac{1}{n}\sum_{i=1}^{n}\left(\beta_{0} + \beta_{1}x_{i}\right) - \beta_{1}\bar{x} = \beta_{0}.
\end{split}
\nonumber
\end{equation}

Es decir que $\hatbeta_{0}$ y $\hatbeta_{1}$ son estimadores insesgado de $\beta_{0}$ y $\beta_{1}$,respectivamente.

La varianza de $\hatbeta_{1}$ y $\hatbeta_{0}$ son:
\[
V(\hatbeta_{1})= V \left( \sum_{i=1}^{n}c_{i}y_{i}\right) = \sum_{i=1}^{n}c_{i}^{2}V(y_{i}) = \sigma^{2}\sum_{i=1}^{n}c_{i}^{2} = \frac{\sigma^{2}}{S_{xx}},
\]
y
\begin{equation}
\begin{split}
V(\hatbeta_{0}) &= V \left( \bar{y} - \hatbeta_{1}\bar{x}\right) = V (\bar{y}) + \bar{x}^{2}V(\hatbeta_{1}) - 2\bar{x}Cov(\bar{y},\hatbeta_{1}) \\
&= \sigma^{2} \left(\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}} \right),
\end{split}
\nonumber
\end{equation}
respectivamente. Finalmente, la covarianza entre $\hatbeta_{0}$ y $\hatbeta_{1}$ es:
\begin{equation}
\begin{split}
Cov(\hatbeta_{0},\hatbeta_{1}) &= Cov\left(\bar{y} - \hatbeta_{1}\bar{x}, \hatbeta_{1} \right) = Cov\left( \bar{y}, \hatbeta_{1}\right) - \bar{x}V\left(\hatbeta_{1}\right) \\
&=  - \sigma^{2}\frac{\bar{x}}{S_{xx}}.
\end{split}
\nonumber
\end{equation}

Si se cumple que $E(\varepsilon_{i}) = 0$, $V(\varepsilon_{i}) = \sigma^{2}$ y $Cov(\varepsilon_{i},\varepsilon_{j})=0$, se puede probar que los estimadores por MCO son insesgado y de varianza mínima (**teorema de Gauss-Markov**). Para la demostración en el caso de regresión múltiple, ver Sección C4 de @montgomery_introduction_2012. Esto quiere decir que, comparado con todos los posibles estimadores insesgados que son combinación lineal de las observaciones, $\hatbeta_{0}$ y $\hatbeta_{1}$ tienen las varianzas más pequeñas. Por esto los estimadores por MCO son considerados los **mejores estimadores lineales insesgados**. 

## Inferencia
También podemos hacer pruebas de hipótesis e intervalos de confianza para los parámetros del modelo y/o pronósticos. 

Por ejemplo, en los datos del peso al nacer podemos estar interesados en evaluar si la edad gestacional tiene un efecto positivo sobre el peso al nacer. Por lo tanto, debemos probar si $\beta_{1} > 0$. También podríamos estar interesados en el valor esperado de un recién nacido para cierto valor especifico de edad gestacional, por ejemplo 38 semanas. Entonces, podemos calcular un intervalo de confianza para $E(Y|x=38)$.

### Pruebas de hipótesis
Suponga la siguiente hipótesis:
\begin{equation}
H_{0}: \beta_{1} = \beta_{10} \mbox{  } H_{1}: \beta_{1} \neq \beta_{10}.
(\#eq:H0beta1)
\end{equation}
Dado que $\hatbeta_{1}$ es una combinación lineal de $y_{i}$ \@ref(eq:beta1cl), podemos concluir que:
\[
\hatbeta_{1} \sim N\left(\beta_{1}, \frac{\sigma^{2}}{S_{xx}}\right).
\]
Además,
\begin{equation}
t_{0} = \frac{\hatbeta_{1}-\beta_{10}}{\sqrt{\frac{MS_{res}}{S_{xx}}}}\sim t_{n-2}.
(\#eq:distB1)
\end{equation}
Por lo tanto, $t_{0}$ es el estadístico de prueba para las hipótesis \@ref(eq:H0beta1). Entonces, rechazamos $H_{0}$ si $|t_{0}| \ge t_{1-\alpha/2,n-p}$ (o por medio del valor-$p$ asociado).

De igual forma, para evaluar:
\[
H_{0}: \beta_{0} = \beta_{00} \mbox{  } H_{1}: \beta_{0} \neq \beta_{00},
\]
el estadístico de prueba es:
\begin{equation}
t_{0} = \frac{\hatbeta_{0}-\beta_{00}}{\sqrt{MS_{res}\left(\frac{1}{n}+\frac{\bar{x}^{2}}{S_{xx}} \right)}} \sim t_{n-2}.
 (\#eq:distB0)
\end{equation}

### Análisis de varianza
El análisis de varianza se basa en la partición de la variabilidad total de la variable respuesta $y$ en dos componentes, uno debido al modelo ajustado y otro al error. Primero, empecemos con la siguiente identidad:
\begin{equation}
y_{i} - \bar{y} = (y_{i} - \haty_{i}) + (\haty_{i} - \bar{y}).
(\#eq:decomposion)
\end{equation}
La Figura \@ref(fig:anovafig) muestra la partición \@ref(eq:decomposion) en el punto $i=3$. Aquí vemos que una parte de la diferencia entre $y_{3}$ y $\bar{y}$ es explicada por el modelo (línea discontinua roja).
```{r anovafig, echo=FALSE, fig.height = 5, fig.width =10,fig.align = "center",fig.cap = "Representación gráfica de la descompocisión \\@ref(eq:decomposion)."}
plot(x,y,xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(m,col=1)
abline(h=mean(y),lty=2)
abline(h=y[3],lty=2)
segments(3, y[3], 3, predict(m)[3],col=2,lty=2)
segments(3, predict(m)[3], 3, mean(y),col=4,lty=2)
brackets(3, y[3], 3, predict(m)[3],h=0.2,ticks=0.5,type=1,xpd=F)
text(3-0.1,y[3]+3,expression(y[3]-hat(y)[3]),pos=2,cex=0.9)
brackets(3, predict(m)[3], 3, mean(y),h=0.2,ticks=0.5,type=1,xpd=F)
text(3-0.1,predict(m)[3]+2.5,expression(hat(y)[3]-bar(y)),pos=2,cex=0.9)
brackets(1.7, y[3], 1.7, mean(y),h=0.2,ticks=0.5,type=1,xpd=F)
text(1.7,predict(m)[3],expression(y[3]-bar(y)),pos=2,cex=0.9)
```

Ahora evelavamos al cuadrado \@ref(eq:decomposion)  y sumamos todos los componentes:
\begin{equation}
\begin{split}
SSY &= \sum_{i=1}^{n} (y_{i} - \bar{y})^{2} \\
&= \sum_{i=1}^{n} (y_{i} - \haty_{i})^{2} + \sum_{i=1}^{n}(\haty_{i} - \bar{y})^{2} + 2 \sum_{i=1}^{n}(y_{i} - \haty_{i})(\haty_{i} - \bar{y}) \\
&= \sum_{i=1}^{n} (y_{i} - \haty_{i})^{2} + \sum_{i=1}^{n}(\haty_{i} - \bar{y})^{2} \\
SST &=  SS_{res} + SS_{R},
\end{split}
(\#eq:ss)
\end{equation}
donde $SST$ es llamada la **suma de cuadrados totales** (con $n-1$ grados de libertad), $SS_{R}$ es la **suma de cuadrados de la regresión** (con $1$ grados de libertad), y $SS_{res}$ es la **suma de cuadrados residual o del error** (con $n-2$ grados de libertad).

El análisis de varianza nos permite evaluar la siguiente hipótesis:
\begin{equation}
H_{0}: \beta_{1} = 0 \mbox{  }H_{1}:  \beta_{1} \neq 0.
(\#eq:anova)
\end{equation}

Se puede demostrar que si $H_{0}$ es cierta:
\[
\frac{SS_{res}}{\sigma^{2}} = \frac{(n-2)MS_{res}}{\sigma^{2}} \sim \chi^{2}_{n-2} \mbox{,} \ \  \frac{SS_{R}}{\sigma^{2}} \sim \chi^{2}_{1},
\]
y que $SS_{res}$ y $SS_{R}$ son independientes. Por lo tanto:
\[
F_{0} = \frac{SS_{R}/1}{SS_{res}/(n-2)} = \frac{MS_{R}}{MS_{res}} \sim F_{(1,n-2)}.
\]
Además, $E(MS_{res})=\sigma^{2}$ y $E(MS_{R})=\sigma^{2} + \beta_{1}^{2}S_{xx}$. 

Entonces, podemos utilizar $F_{0}$ como estadístico de prueba de \@ref(eq:anova). Rechazamos $H_{0}$ si $F_{0} > F_{\alpha,1,n-2}$.

Si $H_{0}$ es falsa, $F_{0}$ sigue una distribución F no central con $1$ y $n-2$ grados de libertad, y parámetro de no centralidad igual a $\lambda=(\beta_{1}^{2}S_{xx})/\sigma^{2}$. 

Estos resultados se pueden resumir en la Tabla \@ref(tab:tableAnova).

```{r tableAnova, echo=FALSE, results='asis'}
  cat(' Table: (\\#tab:tableAnova) Tabla de ANOVA
  
  | Fuente de variación | g.l. | SS | MS | $F$ |
  |---:|---:|---:|---:|---:|
  | regresión | 1 | $SS_{R}$ | $MS_{R}$ | $F_{0}$ | 
  | residuos | n-2 | $SS_{res}$ | $MS_{res}$ | |
  | Total | n-1 | $SST$ | | |')
```

La cantidad:
\[
R^{2} = \frac{SS_{R}}{SST} = 1 - \frac{SS_{res}}{SST},
\]
es llamada **coeficiente de determinación**, y cuantifica la cantidad de variabilidad de $y$ que es explicada por $x$. Dado que $0 \leq SS_{res} \leq SST$, se tiene que $0 \leq R^{2} \leq 1$. Por lo tanto, valores cercanos a $1$ implican que el modelo explica gran parte de la variabilidad de $y$.

\rule{\textwidth}{0.4pt}
### Datos de peso al nacer. Pruebas de hipótesis y ANOVA {-}
Se quiere probar que la edad gestacional tiene influencia sobre el peso al nacer del recién nacido. Esto es: 
\[
H_{0}: \beta_{1} = 0 \qquad H_{1}: \beta_{1} \neq 0. 
\]
Tanto la prueba de hipótesis basada en $t_{0}$, el valor $F_{0}$ del ANOVA, y el $R^{2}$ se pueden observar usando la función `summary`:
```{r summary}
summary(mod)
```

```{r summary2, echo=FALSE}
ANOVA = summary(mod)
t0 = round(ANOVA$coefficients[2,3],3)
t0.valorp = round(ANOVA$coefficients[2,4],5)
R2 = round(ANOVA$r.squared,3)
R2p = 100*R2
F0 = round(ANOVA$fstatistic,2)[1]
```
Del resultado anterior, tenemos que:

- $t_{0}=`r t0`$ con un valor-$p$ asociado de `r t0.valorp`. Por lo tanto, rechazamos $H_{0}$ y concluimos que la edad gestacional tiene un efecto significativo sobre el peso al nacer. 
- La función `summary` no arroja como resultado una tabla ANOVA. Pero podemos observar el valor $F_{0} = `r F0`$ con un valor $p$ asociado de `r t0.valorp`.
- $R^{2} = `r R2`$, lo que indica que el `r R2p`\% de la variabilidad del peso al nacer es explicada por la edad gestacional.
\rule{\textwidth}{0.4pt}


### Intervalos de confianza

#### Intervalos de confianza para $\beta_{0}$, $\beta_{1}$ y $\sigma^{2}$ {-}
Los intervalos de confianza para $\beta_{0}$ y $\beta_{1}$ se construyen a partir de las distribuciones de probabilidad de $\hatbeta_{0}$ y $\hatbeta_{1}$. Esto es, \@ref(eq:distB0) y \@ref(eq:distB1), respectivamente.

Por lo tanto, el intervalo del $100(1-\alpha)\%$ de confianza para $\beta_{j}$ es:
\[
\hatbeta_{j} \pm t_{1-\alpha/2,n-2}\sqrt{V(\hatbeta_{j})}, \mbox{ para }j=0,1.
\]
Se puede demostrar que $(n-2)MS_{res}/\sigma^{2}\sim \chi^{2}_{n-2}$. Por lo tanto:
\[
P \left\{ \chi^{2}_{\alpha/2,n-2} \leq (n-2)MS_{res}/\sigma^{2} \leq \chi^{2}_{1-\alpha/2,n-2} \right\} = 1-\alpha.
\]
Entonces, el intervalo del $100(1-\alpha)\%$ de confianza para $\sigma^{2}$ es:
\[
\left\{ \frac{(n-2)MS_{res}}{\chi^{2}_{1-\alpha/2,n-2}}; \frac{(n-2)MS_{res}}{\chi^{2}_{\alpha/2,n-2}} \right\}.
\]

#### Intervalos de confianza para $E(y_{i})$ y una predicción futura {-}
Cuando el objetivo de ajustar un modelo de regresión es hacer predicciones, es posible hacer intervalos de confianza para la respuesta media, esto es $E(Y|x_{0})=\mu_{Y|x_{0}}$, donde $x_{0}$ es un valor de la covariable dentro del rango de valores observados de $x$ en los datos.

Una estimación insesgada de $\mu_{Y|x_{0}}$ es:
\[
\widehat{\mu}_{Y|x_{0}} = \hatbeta_{0} + \hatbeta_{1}x_{0}.
\]
La varianza de $\widehat{\mu}_{Y|x_{0}}$ es:
\begin{equation}
\begin{split}
V(\widehat{\mu}_{Y|x_{0}}) &= V(\hatbeta_{0} + \hatbeta_{1}x_{0}) = V(\hatbeta_{0}) + x_{0}^{2}V(\hatbeta_{1}) + 2x_{0}Cov(\hatbeta_{0},\hatbeta_{1}) \\
&= \sigma^{2}\left(\frac{1}{n} + \frac{\bar{x}^{2}}{S_{xx}} \right) + \sigma^{2}x_{0}^{2}\frac{1}{S_{xx}} - 2\sigma^{2}x_{0}\frac{\bar{x}}{S_{xx}} \\
&= \sigma^{2}\left[\frac{1}{n} + \frac{(x_{0}-\bar{x})^{2}}{S_{xx}} \right].
\end{split}
\nonumber
\end{equation}
El intervalo de confianza se construye a partir de la siguiente distribución muestral:
\[
\frac{\widehat{\mu}_{Y|x_{0}} - E(Y|x_{0})}{\sqrt{MS_{res}[1/n+(x_{0}-\bar{x})^{2}/S_{xx}]}} \sim t_{n-2}.
\]
Por lo tanto, el intervalo del $100(1-\alpha)\%$ de confianza para $\mu_{Y|x_{0}}$ es:
\begin{equation}
\widehat{\mu}_{Y|x_{0}} \pm t_{1-\alpha/2,n-2}\sqrt{MS_{res}[1/n+(x_{0}-\bar{x})^{2}/S_{xx}]}.
\label{eq:CImean}
\end{equation}
Note que la longitud del intervalo de confianza de $\widehat{\mu}_{Y|x_{0}}$ depende del punto $x_{0}$. La menor longitud se obtiene en el punto $x_{0}=\bar{x}$, y el intervalo es cada vez mas ancho a medida que nos alejamos de ese punto.

Ahora consideremos hacer una predicción de una observación futura de $y$ para cierto valor de $x$. Si queremos hacer la predicción para $x=x_{0}$, entonces la predicción de la nueva observación es:
\[
\widetilde{y}_{0} = \hatbeta_{0} + \hatbeta_{1}x_{0}.
\]
Asumiendo que el modelo es correcta, el verdadero valor de $y_{0}$ es:
\[
\widetilde{y}_{0} = \hatbeta_{0} + \hatbeta_{1}x_{0} + \varepsilon_{0}.
\]
y su varianza es:
\begin{equation}
V(\widetilde{y}_{0}) = \sigma^{2}+\sigma^{2}\left[\frac{1}{n} + \frac{(x_{0}-\bar{x})^{2}}{S_{xx}} \right].
(\#eq:varpred)
\end{equation}
El primer término a la derecha de \@ref(eq:varpred) corresponde a la variabilidad de $\varepsilon_{0}$ y el segundo al error de estimación de los coeficientes $\beta_{0}$ y $\beta_{1}$. A partir de estos resultados, el intervalo del $100(1-\alpha)\%$ de predicción de una observación futura en $x=x_{0}$ es:
\[
\widehat{\mu}_{Y|x_{0}} \pm t_{1-\alpha/2,n-2}\sqrt{MS_{res}[1+1/n+(x_{0}-\bar{x})^{2}/S_{xx}]}.
\]

\rule{\textwidth}{0.4pt}
### Datos de peso al nacer. Intervalos de confianza {-}
El intervalo del 95% de confianza para los parámetros del modelo \@ref(eq:modBirthweight) son:
```{r ICBeta, echo=TRUE}
confint(mod)
```
Para $\beta_{1}$, con un nivel de confianza del 95\% podemos decir que cuando la edad gestacional aumenta en una unidad, el peso medio del recién nacido aumenta entre 123 y 228 gramos. Como mencionamos antes, $\beta_{0}$ no tiene interpretación en este modelo.

El intervalo del 95\% de confianza para $\sigma$ se calcula "a pie" de la siguiente forma:
```{r ICSigma, echo=TRUE}
var.limInf = sum(mod$residuals^2)/qchisq(0.975,df=mod$df.residual)
var.limSup = sum(mod$residuals^2)/qchisq(0.025,df=mod$df.residual)
sqrt(c(var.limInf,var.limSup))
```
Se quiere predecir el peso medio de los recién nacidos en la semana gestacional 36. La estimación puntual es:
\[
\widehat{\mu}_{Y|x_{0}=36} = \hatbeta_{0} + \hatbeta_{1}(36) = -3.6312 + 0.1751*36 = 2.672.
\]
El intervalo del 95\% de confianza para $\widehat{\mu}_{Y|x_{0}=36}$ se puede calcular de la siguiente forma:
```{r ICPredMed, echo=TRUE}
x.nuevo = data.frame(age=36)
pred.media = predict(mod,x.nuevo,interval = 'confidence')
pred.media
```
Esto quiere decir que, con un nivel de confianza del 95\%, el peso medio de los recién nacidos en la semana gestacional 36 está entre `r round(pred.media[2],2)` y `r round(pred.media[3],2)` kilogramos.

Ahora, se quiere predecir el peso de un recién nacido en la semana gestacional 38, para esto calculamos un intervalo de predicción del 95\%:
```{r ICPredPred, echo=TRUE}
x.nuevo = data.frame(age=38)
pred.nuevaObs = predict(mod,x.nuevo,interval = 'prediction')
pred.nuevaObs
```
Por lo tanto, con un nivel de confianza del 95\% el peso de un recién nacido en la semana gestacional 38 está entre `r round(pred.nuevaObs[2],2)` y `r round(pred.nuevaObs[3],2)` kilogramos.

Gráficamente, podemos ver los intervalos de confianza y predicción de la siguiente forma. Primero hacemos la predicciones para edades gestacionales entre 33 y 45 semanas:
```{r ICprevCalc, echo=TRUE}
x.nuevo = data.frame(age=seq(from=33,to=45,length.out = 100))
pred.media = predict(mod,x.nuevo,interval = 'confidence')
pred.nuevaObs = predict(mod,x.nuevo,interval = 'prediction')
```
Luego hacemos el gráfico:
```{r birthweightIC, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "\\label{fig:BWdata2} Intervalos del 95\\% de confianza (línea discontinua) y predicción (línea punteada) para el peso del recien nacido en función de la edad gestacional."}
plot(weight~age,data=birthweight,pch=20,xlab="Edad gestacional(semanas)",
     ylab='Peso(kilogramos)')
abline(mod)
lines(x.nuevo$age,pred.media[,2],lty=2)
lines(x.nuevo$age,pred.media[,3],lty=2)
lines(x.nuevo$age,pred.nuevaObs[,2],lty=3)
lines(x.nuevo$age,pred.nuevaObs[,3],lty=3)
```
\rule{\textwidth}{0.4pt}

## Estimador por máxima verosimilitud
Si consideramos que $\varepsilon_{i}\sim N(0,\sigma^{2})$, entonces las observación también se distribuyen de forma normal $y_{i}\sim N(\beta_{0}+\beta_{1}x_{i},\sigma^{2})$. Si además asumimos que las observaciones $(y_{i},x_{i})$ son independientes, entonces la función de verosimilitud es:
\begin{equation}
\begin{split}
L(\btheta) =& \prod_{i=1}^{n} \left(\sqrt{2\pi\sigma^{2}} \right)^{-1/2}\exp\left[ - \frac{1}{2\sigma^{2}}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2} \right] \\
=& \left( \sqrt{2\pi\sigma^{2}} \right)^{-n/2}\exp\left[ - \frac{1}{2\sigma^{2}} \sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}\right],
\end{split}
\nonumber
\end{equation}
para $\btheta=(\beta_{0},\beta_{1},\sigma^{2})'$. La log-verosimilitud es:
\[
\ell (\btheta) = - \left(\frac{n}{2}\right)\left[ \log (2 \pi) - \log \sigma^{2}\right] - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(y_{i} - \beta_{0}-\beta_{1}x_{i})^{2}. 
\]
El estimador de máxima verosimilitud de $\btheta$ debe satisfacer:
\[
\left. \frac{\partial \ell(\btheta)}{\partial \beta_{0}} \right|_{\widetilde{\btheta}} = \frac{1}{\widetilde{\sigma}^{2}}\sum_{i=1}^{n}(y_{i} - \beta_{0}-\beta_{1}x_{i}) = 0,
\]
\[
\left. \frac{\partial \ell(\btheta)}{\partial \beta_{1}} \right|_{\widetilde{\btheta}} = \frac{1}{\widetilde{\sigma}^{2}}\sum_{i=1}^{n}(y_{i} - \beta_{0}-\beta_{1}x_{i})x_{i} = 0,
\]
y
\[
\left. \frac{\partial \ell(\btheta)}{\partial \sigma^{2}} \right|_{\widetilde{\btheta}} = -\frac{n}{2\widetilde{\sigma}^{2}} + \frac{n}{2\widetilde{\sigma}^{4}}
\sum_{i=1}^{n}(y_{i} - \beta_{0}-\beta_{1}x_{i})^{2} = 0,
\]
Luego de solucionar las ecuaciones anteriores se obtienen los estimadores por máxima verosimilitud:
\[
\widetilde{\beta}_{0} = \bar{y} - \widetilde{\beta}_{1}\bar{x},
\]
\[
\widetilde{\beta}_{1} = \frac{\sum_{i=1}^{n}y_{i}(x_{i}-\bar{x})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}},
\]
y
\[
\widetilde{\sigma}^{2} = \frac{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}x_{i})^{2}}{n}.
\]
Aquí observamos que los estimadores por máxima verosimilitud para $\beta_{0}$ y $\beta_{1}$ son equivalente a los estimadores por MCO. El estimador de $\sigma^{2}$ es sesgado, sin embargo el sesgo disminuye a medida que $n$ crece.

Por lo general, los estimadores por máxima verosimilitud tienen mejores propiedades que los estimadores por MCO. Son asintoticamente insesgados, consistentes y asintoticamente de mínima varianza. Sin embargo, estos requieren de supuestos distribucionales completos. Recordemos que los estimadores por MCO solo requieren de una correcta especificación de los dos primeros momentos (valor esperado, varianza y covarianza).


## Algunas consideraciones finales

- Las conclusiones sobre los modelos de regresión se hacen sobre el rango de valores observados de las covariables (interpolación). Por ejemplo, en los datos de los recién nacidos, se pueden hacer inferencias sobre el peso al nacer para bebés que nacen en entre las semanas `r min(birthweight$age)` y `r max(birthweight$age)`. Cuando hacemos predicciones fuera de este rango estaríamos extrapolando.

Por extrapolación nos referimos a hacer predicciones fuera del rango observado de $x$. La Figura \@ref(fig:extrapolacion) muestra el problema que se puede cometer cuando extrapolamos. Si tenemos datos en el rango $x_{min} \leq x \leq x_{max}$, un modelo lineal es una buena aproximación de $E(Y|x)$. Pero, esa aproximación no es buena para $x>x_{max}$. Por lo tanto, se estaría cometiendo errores graves cuando hacemos predicciones de $Y$ para valores de $x$ mayores a $x_{max}$ (por ejemplo en el punto $x_{0}$).


```{r extrapolacion, echo=F, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Peligro de extrapolar. La curva negra representa $E(Y|x)$ y la línea roja es el ajuste del modelo lineal con los datos observados de $x$. La predicción en el punto $x_{0}$ es bastante sesgada."}
set.seed(2)
x = seq(0,4,length.out = 100)
y = sin(0.5*x[x<1.8]) + rnorm(length(x[x<1.8]),0,0.05)
plot(x,sin(0.5*x),type='l',xlab='covariable = X',ylab = 'variable respuesta = Y',ylim=c(0,1.5),
     xaxt='n',yaxt='n')
points(x[x<1.8],y)
xi = x[x<1.8]
mod = lm(y~xi)
abline(mod,col=2,lwd=2)
ymin = predict(mod,data.frame(xi=0))
y0 = predict(mod,data.frame(xi=1.8))
y1 = predict(mod,data.frame(xi=3))
segments(0,0,0,ymin,lty=2)
segments(1.8,0,1.8,y0,lty=2)
segments(3,0,3,y1,lty=2)
points(3,y1,col=2,pch=18)
axis(1,min(x),labels = expression(x[min]))
axis(1,1.8,labels = expression(x[max]))
axis(1,3,labels = expression(x[0]))
```
- La posición de los valores de $x$ tienen una influencia sobre el ajuste por MCO. Particularmente, la estimación de $\beta_{1}$ está fuertemente influenciada por los valores alejados de $x$ y $y$. A estos  puntos se les denomina **puntos influyentes**. En la Figura \@ref(fig:influyentes)(a) podemos ver como un solo punto tiene una influencia alta en la estimación de los parámetros del modelo.


- Los **valores atípicos** son observaciones que difieren considerablemente del resto de los datos (generalmente en $y$). Un punto atípico puede afectar la estimación de $\beta_{0}$ (ver Figura \@ref(fig:influyentes)(b) y la estimación de $\sigma^{2}$. 

```{r influyentes0, echo=FALSE}
set.seed(2)
x = runif(24,0,1)
y = 1+0.05*x + rnorm(24,0,0.2)
x = c(x,2)
y = c(y,1+0.05*2+2)
mod1 = lm(y[-25]~x[-25])
mod2 = lm(y~x)
x2 = runif(24,0,1)
y2 = 1+2*x2 + rnorm(24,0,0.2)
x2 = c(x2,0.5)
y2 = c(y2,1+0.5*2+2)
mod12 = lm(y2[-25]~x2[-25])
mod22 = lm(y2~x2)
```

```{r influyentes, echo=F, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "(a) Efecto de un punto influyente, la línea negra  es la estimación sin el punto A y la linea roja es la estimación incluyento el punto A. (b) Efecto de un punto atípico, la línea negra  es la estimación sin el punto B y la linea roja es la estimación incluyento el punto B."}
par(mfrow=c(1,2))
plot(x,y,xlab='covariable = X',ylab = 'variable respuesta = Y', xaxt='n',yaxt='n',main='(a)')
abline(mod1,col=1,lwd=2)
abline(mod2,col=2,lwd=2)
text(2,1+0.05*2+2,expression(A),pos=2)

plot(x2,y2,xlab='covariable = X',ylab = 'variable respuesta = Y', xaxt='n',yaxt='n',main='(b)')
abline(mod12,col=1,lwd=2)
abline(mod22,col=2,lwd=2)
text(0.5,1+0.5*2+2,expression(B),pos=2)
```
Los métodos para la detección de puntos atípicos e influyentes se presentarán en un capítulo posterior.

- Una fuerte relación entre dos variables no necesariamente implica que la relación entre las variables es de causa-efecto. Un modelo de regresión nos permite modelar variables que estén correlacionadas, pero no se puede concluir que, necesariamente, es una relación causal.

<!--chapter:end:01-RegresionSimple.Rmd-->

# Modelo lineal múltiple

```{r setup, include=FALSE}
birthweight = read.csv("birthweight.csv",header = T)
```
\rule{\textwidth}{0.4pt}
## Bajo peso al nacer {-}
Retomemos la base de datos de bajo peso al nacer. Aparte de la edad gestacional, el peso del recién nacido puede estar explicado con otros factores. Por ejemplo, el peso de los padres, salud de la madre, entre otros. A parte de la edad gestacional y el peso del recién nacido, vamos a observar también la variable peso de la madre antes del embarazo.

La Figura \@ref(fig:BWdataFig) muestra la relación entre las variables de estudio. Aquí podemos observar una relación lineal positiva fuerte entre el peso al nacer y la edad gestacional (correlación igual a `r round(cor(birthweight[,c(3,4)])[1,2],2)`). La relación entre el peso al nacer y el peso de la madre es lineal positiva, aunque no tan fuerte como la anterior (correlación igual a `r round(cor(birthweight[,c(3,8)])[1,2],2)`). 

La Figura \@ref(fig:BWdataFig) y la matriz de correlación se pueden hacer con los siguientes códigos:
```{r BWdataFig, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Gráfico de dispersion del peso del recien nacido y la edad gestacional."}
birthweight = read.csv("birthweight.csv",header = T)
pairs(birthweight[,c(3,4,8)])
cor(birthweight[,c(3,4,8)])
```
Por lo tanto, junto con la edad gestacional, ahora vamos a incluir peso de la madre antes del embarazo (en kgs, `mppwt`) como covariable. Por lo tanto, el modelo propuesto es:
\[
\mbox{weight}_{i} = \beta_{0} + \beta_{1}\mbox{age}_{i} + \beta_{2}\mbox{mppwt}_{i} + \varepsilon_{i}, \mbox{ para }i=1,\ldots,42,
\]
con $\varepsilon_{i}\sim N(0,\sigma^{2})$, y $cov(\varepsilon_{j},\varepsilon_{k})=0$ para todo $j\neq k$.
\rule{\textwidth}{0.4pt}

## Modelo lineal múltiple
En general, se puede relacionar la variable respuesta ($y$), con $p-1$ covariables (o variables predictoras). El modelo lineal múltiple se expresa de la siguiente forma:

\begin{equation}
\begin{split}
y_{i} =& \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \ldots + \beta_{p-1} x_{i,p-1} + \varepsilon_{i} \\
=& \bx_{i}'\bbeta + \varepsilon_{i}, \qquad i=1,\ldots,n, \\
\end{split}
(\#eq:modMultiple)
\end{equation}

donde $\bx_{i} = (1,x_{i1},x_{i2},\ldots,x_{i,p-1})'$ es el vector de dimensión $p$ de covariables del individuo $i$ y $\bbeta = (\beta_{0},\beta_{1},\ldots,\beta_{p-1})'$ es el vector de dimensión $p$ de coeficientes de regresión.

Los supuestos del modelo son los mismos que se plantearon en el capítulo anterior. Estos es: $\varepsilon_{i} \sim N\left(0,\sigma^{2} \right)$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$, para todo $j \neq k$.

Dado que $E(\varepsilon_{i})=0$, el valor esperado de $Y$ es:
\begin{equation}
E(Y| x_{i1},x_{i2},\ldots,x_{i,p-1}) = E(Y| \bx_{i}) = \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \ldots + \beta_{p-1} x_{i,p-1} = \bx_{i}'\bbeta.
(\#eq:expValue)
\end{equation}
El intercepto $\beta_{0}$ es el valor esperado de $Y$ cuando $x_{i}=(1,0,0,\ldots,0)'$, es decir cuando todos las covariables toman el valor $0$. 

El parámetro de pendiente $\beta_{j}$ indica el cambio en el valor esperado de $Y$ debido a un aumento unitario en la covariable $x_{j}$ cuando todas las demás variables predictoras se mantienen constantes. Sean $x_{i,j} = (1,x_{i1},\ldots,x_{ij},\ldots,x_{i,p-1})$ y $x_{i,j+1} = (1,x_{i1},\ldots,x_{ij}+1,\ldots,x_{i,p-1})$. A partir de \@ref(eq:expValue), tenemos:
\[
E(Y|x_{i,j}) = \beta_{0} + \beta_{1}x_{i1} + \ldots + \beta_{j}x_{ij} + \ldots + \beta_{p-1} x_{i,p-1},
\]
y
\[
E(Y|x_{i,j+1}) = \beta_{0} + \beta_{1}x_{i1} + \ldots + \beta_{j}(x_{ij}+1) + \ldots + \beta_{p-1} x_{i,p-1}.
\]
De aquí tenemos que:
\[
E(Y|x_{i,j+1}) - E(Y|x_{i,j}) = \beta_{j}.
\]
Es conveniente escribir el modelo de regresión múltiple \@ref(eq:modMultiple) de forma matricial:
\[
\by = \bX\bbeta + \bvarepsi,
\]
donde:
\begin{gather}
\begin{aligned}
\by = \begin{pmatrix}
y_{1} \\ y_{2} \\ \vdots \\ y_{n}
\end{pmatrix}, & \bX = \begin{pmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1,p-1} \\ 1 & x_{21} & x_{22} & \ldots & x_{2,p-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & x_{n2} & \ldots & x_{n,p-1}
\end{pmatrix},
\bbeta = \begin{pmatrix}
\beta_{0} \\ \beta_{1} \\ \beta_{2} \\ \vdots \\ \beta_{p-1}
\end{pmatrix}, & \bvarepsi = \begin{pmatrix}
\varepsilon_{1} \\ \varepsilon_{2} \\ \vdots \\ \varepsilon_{n}
\end{pmatrix}.
\end{aligned}
\nonumber
\end{gather}
Además, los supuestos sobre los errores se pueden expresar como $\bvarepsi \sim N(\bZERO, \sigma^{2}\bI)$, donde $\bZERO$ es un vector con todas las entradas iguales a cero, y $\bI$ es la matriz identidad.

## Estimación de los parámetros de regresión
La estimación de $\bbeta$ se hace a través del método de mínimos cuadrados ordinarios. Por lo tanto, debemos encontrar el vector $\hatbbeta$ que minimice:
\[
S(\bbeta) =  \sum_{i=1}^{n}\left(y_{i} - \bx_{i}'\bbeta \right)^2 = \sum_{i=1}^{n}e_{i}^2.
\]
En forma matricial, tenemos:
\begin{equation}
\begin{split}
S(\bbeta) &= \be'\be = (\by - \bX\bbeta)'(\by - \bX\bbeta) \\
&= \by'\by - \bbeta'\bX'\by - \by'\bX\bbeta + \bbeta'\bX'\bX\bbeta \\
&= \by'\by - 2\bbeta'\bX'\by + \bbeta'\bX'\bX \bbeta.
\end{split}
\nonumber
\end{equation}
Por lo tanto, $\hatbbeta$ debe satisfacer:
\[
\left. \frac{\partial S}{\partial \bbeta} \right|_{\hatbbeta} = - 2\bX'\by + 2\bX'\bX \hatbbeta = \bZERO.
\]
A partir de aquí obtenemos las **ecuaciones normales**: 
\[
\bX'\bX \hatbbeta = \bX'\by.
\]
En más detalle:
\begin{gather}
\begin{pmatrix}
n & \sum x_{i1} & \sum_{i=1}^{n}x_{i2} & \ldots & \sum_{i=1}^{n}x_{i,p-1} \\
\sum x_{i1} & \sum x_{i1}^2 & \sum x_{i1}x_{i2} & \ldots & \sum x_{i1}x_{i,p-1} \\
\sum x_{i2} & \sum x_{i1}x_{i2} & \sum x_{i2}^2 & \ldots & \sum x_{i2}x_{i,p-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\sum x_{i,p-1} & \sum x_{i1}x_{i,p-1} & \sum x_{i2}x_{i,p-1} & \ldots & \sum x_{i,p-1}^{2} \\
\end{pmatrix} \begin{pmatrix}
\hatbeta_{0} \\ \hatbeta_{1} \\ \vdots \\ \hatbeta_{p-1} \end{pmatrix} = \begin{pmatrix}
\sum y_{i} \\ \sum x_{i1}y_{i} \\ \vdots \\ \sum x_{i,p-1}y_{i}
\end{pmatrix}
\nonumber
\end{gather}
Por lo cual, el estimador por mínimos cuadrados es:
\[
\hatbbeta = (\bX'\bX)^{-1}\bX'\by.
\]
Note que es necesario que $\bX$ sea de rango completo, $\mbox{rango}(\bX) = p \leq n$. Esta restricción es necesaria para asegurar que $\bX'\bX$ sea no singular. Si $\bX'\bX$ es singular, implica que existe una combinación lineal entre las columnas de $\bX$, o que $\mbox{rango}(\bX) < p$.

El valor ajustado de $y$ para el vector de covariables $\bx_{i}$ es $\haty_{i}= \bx_{i}'\hatbbeta$. Definiendo $\hatby = (\haty_{1},\haty_{2},\ldots,\haty_{n})$, tenemos que:
\[
\hatby = \bX\hatbbeta = \bX(\bX'\bX)^{-1}\bX'\by = \bH\by.
\]
La matriz $(n\times n)$ $\bH = \bX(\bX'\bX)^{-1}\bX'$ es llamada **matriz hat** (sombrero) y desempeña un papel importante en el análisis de regresión. 

Los residuos del modelo $(e_{i}=y_{i}-\haty_{i})$ también se pueden expresar en forma matricial:
\[
\be = \by - \bX'\hatbeta = \by - \bX(\bX'\bX)^{-1}\bX'\by = \by - \bH\by = (\bI_{n} - \bH)\by.
\]

### Estimación de $\sigma^{2}$
Al igual que en la regresión simple, el estimador de $\sigma^{2}$ es el cuadrado medio del error, definido como:
\[
MS_{res} = \frac{SS_{res}}{n-p},
\]
donde:
\begin{equation}
\begin{split}
SS_{res} &= \sum_{i=1}^{n}e^{2}_{i} = \be'\be = (\by - \bX\hatbbeta)'(\by - \bX\hatbbeta) \\
&= (\by - \bH\by)'(\by - \bH\by) = \by'(\bI_{n}-\bH)'(\bI_{n}-\bH)\by = \by'(\bI_{n} - \bH)\by.
\end{split}
\nonumber
\end{equation}
Se puede demostrar que $MS_{res}$ es un estimador insesgado de $\sigma^{2}$, es decir $E(MS_{res})=\sigma^{2}$. Para esto debemos calcular el valor esperado de $SS_{res}$.

Sabemos que $E(\by) = \bX\bbeta$ y $V(\by) = \sigma^{2}\bI_{n}$, entonces:
\[
E(SS_{res}) = E\left[\by'(\bI_{n} - \bH)\by\right] = \sigma^{2}\tr\left( \bI_{n} - \bH \right) + \bbeta'\bX'(\bI_{n} - \bH)\bX\bbeta = (n-p)\sigma^{2}.
\]
Por lo tanto, $E(MS_{res}) = E(SS_{res})/(n-p) = \sigma^{2}$. 


\rule{\textwidth}{0.4pt}
### Bajo peso al nacer - estimación de parámetros
Para ajustar el modelo:
\[
\mbox{weight}_{i} = \beta_{0} + \beta_{1}\mbox{age}_{i} + \beta_{2}\mbox{mppwt}_{i} + \varepsilon_{i}, \mbox{ para }i=1,\ldots,42,
\]
con $\varepsilon_{i}\sim N(0,\sigma^{2})$, y $cov(\varepsilon_{j},\varepsilon_{k})=0$ para todo $j\neq k$, usamos la función `lm` de R:
```{r fitBirthweight, echo=TRUE}
mod = lm(weight ~ age + mppwt, data=birthweight)
mod
```
De aquí temenos que: 
\[
E(\mbox{weight} | \mbox{age}, \mbox{mppwt})= -3.978+0.167\mbox{age}+0.011\mbox{mppwt}.
\]
Aquí vemos que ambas covariables tienen un efecto positivo sobre el peso del bebé al nacer. Especificamente, tenemos que:

- Si la edad gestacional aumenta en una semana y el peso de la madre se mantiene constante, el valor esperado del peso al nacer crece `r round(mod$coefficients[2],3)*1000` gramos.
- Por cada incremento de un kilogramo en el peso de la madre y manteniendo la edad gestacional constante, el peso al nacer medio aumenta `r round(mod$coefficients[3],3)*1000` gramos.

Además, la estimación de $\sigma^{2}$ es:
```{r fitBirthweight2}
 sqrt(sum(mod$residuals^2)/39)
```
Note que al adicionar la covariable `mppwt` se redujo el $MS_{res}$.
\rule{\textwidth}{0.4pt}

### Propiedades de los estimadores por MCO
El valor esperado de $\hatbbeta$ es:
\begin{equation}
\begin{split}
E(\hatbbeta) =& E\left[ (\bX'\bX)^{-1}\bX'\by \right] = E\left[ \bX'\bX)^{-1}\bX'(\bX\bbeta + \bvarepsi) \right] \\
=& E\left[ (\bX'\bX)^{-1}\bX'\bX\bbeta + (\bX'\bX)^{-1}\bX'\bvarepsi \right] = \bbeta
\end{split}
\nonumber
\end{equation}
Por lo tanto, $\hatbbeta$ es un estimador insesgado de $\bbeta$ (si el modelo está bien espeficado).

La matriz de varianzas-covarianzas de $\hatbbeta$ es:
\begin{equation}
\begin{split}
V(\hatbbeta) &= V\left[ (\bX'\bX)^{-1}\bX'\by \right] = (\bX'\bX)^{-1}\bX'V(\by)\bX(\bX'\bX)^{-1}  \\
&= \sigma^{2}(\bX'\bX)^{-1}\bX'\bX(\bX'\bX)^{-1} = \sigma^{2}(\bX'\bX)^{-1}.
\end{split}
\nonumber
\end{equation}
Si $\bC=(\bX'\bX)^{-1}$, entonces $V(\hatbeta_{j}) = \sigma^{2}c_{jj}$ y $Cov(\hatbeta_{j},\hatbeta_{k})=\sigma^{2}c_{jk}$, donde $c_{jk}$ es la entrada $(j,k)$ de la matriz $\bC$.

#### Teorema de Gauss-Markov {-}
Si, $E(\bvarepsi) = \bZERO$ y $V(\bvarepsi) = \sigma^{2}\bI_{n}$, el estimador por MCO, $\hatbbeta = (\bX'\bX)\bX'\by$, es el mejor estimador lineal insesgado de $\bbeta$. Esto quiere decir que es el estimador con menor varianza entre la clase de estimador insesgados que son combinaciones lineales de $y$. Para la demostración, ver Sección C4 de @montgomery_introduction_2012.

Además, si $\bvarepsi \sim N(\bZERO, \sigma^{2}\bI_{n})$, el estimador por MCO coincide con el estimador por máxima verosimilitud.

## Pruebas de hipótesis
Después de estimar el modelo podemos preguntarnos:

- ¿el modelo hace un buen ajuste de los datos?
- ¿cuales regresores específicos parecen importantes?

Para resolver estas preguntas podemos realizar pruebas de hipótesis. Generalmente, estos test requieren que $\bvarepsi \sim N(\bZERO,\sigma^{2}\bI_{n})$.

### Análisis de varianza
Para probar la significancia del modelo (determinar si que la relación entre $y$ y algunas de las covariables es lineal) se plantean las siguientes hipótesis:
\begin{equation}
\begin{split}
H_{0}:& \beta_{1}=\beta_{2}=\ldots=\beta_{p-1} = 0 \\
H_{1}:& \beta_{j}\neq 0 \mbox{ para al menos un }j.
\end{split}
(\#eq:HANOVA)
\end{equation}
El rechazo de esta hipótesis nula implica que al menos uno de los regresores $x_1, x_2,\ldots , x_{p-1}$ contribuye significativamente al modelo.

Igual que en la regresión simple, el estadístico de prueba se encuentra a partir de la partición de la suma de cuadrados totales:
\[
SS_{T}  = SS_{R} + SS_{res},
\]
donde:

- $SS_{T} = \sum_{i=1}^{n}(y_{i}-\bar{y})^{2} = (\by  - \frac{1}{n}\bONE'\by)'(\by  - \frac{1}{n}\bONE'\by)$,
- $SS_{R} = \sum_{i=1}^{n}(\haty_{i}-\bar{y})^{2} = (\bH\by - \frac{1}{n}\bONE'\by)'(\bH\by - \frac{1}{n}\bONE'\by)$,
- $SS_{res} = \sum_{i=1}^{n}(y_{i}-\haty_{i})^{2} = (\by  - \bH\by)'(\by  - \bH\by)$,

y $\bONE$ es un vector cuyas entradas son iguales a $1$.

Si $H_{0}$ es cierta, tenemos que:
\[
\frac{SS_{res}}{\sigma^{2}}\sim\chi^{2}_{n-p} \mbox{ y } \frac{SS_{R}}{\sigma^{2}} \sim \chi^{2}_{p-1},
\]
además, $SS_{res}$  y $SS_{R}$ son independientes. Por lo tanto,
\[
F_{0} = \frac{SS_{R}/(p-1)}{SS_{res}/(n-p)} = \frac{MS_{R}}{MS_{res}} \sim F_{p-1,n-p}.
\]
También se puede probar que:
\[
E(MS_{res}) =  \sigma^{2} \mbox{ y }E(MS_{R}) =  \sigma^{2} + \frac{\bbeta^{*'}\bX_{c}'\bX_{c}\bbeta^{*}}{(p-1)\sigma^{2}},
\]
donde $\bbeta^{*} = (\beta_{1},\beta_{2},\ldots,\beta_{p-1})'$ y
\[
\bX_{c} = \begin{pmatrix}
x_{11} - \bar{x}_{1} & x_{12} - \bar{x}_{2} & \ldots & x_{1,p-1} - \bar{x}_{p-1} \\ 
x_{21} - \bar{x}_{1} & x_{22} - \bar{x}_{2} & \ldots & x_{2,p-1} - \bar{x}_{p-1} \\ 
\vdots & \vdots & \ddots & \vdots \\
x_{i1} - \bar{x}_{1} & x_{i2} - \bar{x}_{2} & \ldots & x_{i,p-1} - \bar{x}_{p-1} \\ 
\vdots & \vdots & \ddots & \vdots \\
x_{n1} - \bar{x}_{1} & x_{n2} - \bar{x}_{2} & \ldots & x_{n,p-1} - \bar{x}_{p-1} \\ 
\end{pmatrix}.
\]

Si $H_{0}$ no es cierta, tenemos que $F_{0}$ sigue una distribución $F$ no-central con $p-1$ y $n-p$ grados de libertad y parámetro de no centralidad:
\[
\lambda = \frac{\bbeta'\bX_{c}'\bX_{c}\bbeta}{\sigma^{2}}.
\]
Estos nos resultados nos indican que si el valor $F_{0}$ es grande, entonces al menos un $\beta_{j}$ es diferente de cero.

Por lo tanto, para probar las hipótesis \@ref(eq:HANOVA) calculamos el estadístico de prueba $F_{0} = \frac{MS_{R}}{MS_{res}}$, y rechazamos $H_{0}$ si $F_{0} > F_{1-\alpha,p-1,n-p}$.

A partir de las sumas de cuadrados podemos calcular el coeficiente de determinación:
\[
R^{2} = 1-\frac{SS_{res}}{SS_{T}}.
\]
A medida que agregamos mas covariables al modelo el $R^{2}$ aumenta (o permanece igual), sin importar si la covariable agregada tiene una contribución importante en el ajuste. Esto hace que sea difícil determinar si el incremento en el $R^{2}$ al agregar una covariable sea relevante. Por esta razón, también podemos usar el coeficiente de determinación ajustado:
\[
R^{2}_{adj} = 1-\frac{SS_{res}/(n-p)}{SS_{T}/(n-1)} = 1 - \frac{MS_{res}}{SS_T/(n-1)}.
\]
Aunque no tiene interpretación, el $R^{2}_{adj}$ puede usarse para comparar modelos al agregar covariables. Dado que $SS_{T}/(n-1)$ es constante, el $R^{2}_{adj}$ solo aumentará al agregar una covariable nueva al modelo si la adición de la covariable reduce el $MS_{res}$.

### Pruebas individuales sobre los coeficientes
Al rechazar $H_{0}$ de la prueba de hipótesis \@ref(eq:HANOVA) concluimos que al menos un coeficiente es diferente de cero. Por lo tanto, una o más covariables tienen un aporte significativo en el modelo. El paso que sigue es identificar estas covariables.

Para esto podemos plantear las siguientes hipótesis individuales sobre los coeficientes del modelo:
\[
H_{0}: \beta_{j} = 0 \qquad H_{1}: \beta_{j} \neq 0.
\]
El estadística de prueba es:
\[
t_{0} = \frac{\hatbeta_{j}}{se(\hatbeta_{j})} = \frac{\hatbeta_{j}}{\sqrt{MS_{res}c_{jj}}},
\]
donde $c_{jj}$ es la entrada $(j,j)$ de la matriz $\bC = (\bX'\bX)^{-1}$. Rechazamos $H_{0}$ si $|t_{0}| > t_{1-\alpha/2,n-p}$.

Este es una prueba parcial, puesto que estamos evaluando la significancia de $x_{j}$ cuando las demás covariables $x_{k}$, para $k\neq j$, ya están incluidas en el modelo. Por lo tanto, si no rechazamos $H_{0}$, podemos concluir que, cuando los demás regresores están en el modelo, la covariable $x_{j}$ no tiene un aporte significativo. Por lo tanto, podríamos retirarla del modelo.


### Pruebas sobre subconjuntos de coeficientes
Para probar la significancia de un subconjunto de coeficientes del modelo hacemos uso de la **suma de cuadrados extra**. Primero, consideremos el siguiente modelo de regresión:
\[
\by = \bX \bbeta + \bvarepsi,
\]
donde $\bX$ es una matrix $n \times p$ y $\bbeta$ es el vector de coeficientes de longitud $p$. Queremos probar si un subconjunto $r < p$ de covariables tienen un aporte significativo en el modelo. Para esto hacemos la siguiente partición del vector $\bbeta$:
\[
\bbeta = \begin{pmatrix} \beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p-r-1} \\ \hline \beta_{p-r} \\ \beta_{p-r+1} \\ \vdots \\  \beta_{p} \end{pmatrix} =  \begin{pmatrix} \bbeta_{1} \\ \hline \bbeta_{2}\end{pmatrix},
\]
donde $\bbeta_{1}$ y $\bbeta_{2}$ son vector de dimensión $(p-r)$ y $(r)$, respectivamente. Por lo tanto, queremos realizar la siguiente prueba de hipótesis:
\begin{equation}
H_{0}:  \bbeta_{2} = \bZERO \qquad H_{1}:  \bbeta_{2} \neq \bZERO.
(\#eq:Hsubset)
\end{equation}
El modelo anterior se puede re-escribir de la siguiente forma:
\[
\by = \bX \bbeta + \bvarepsi = \bX_{1}\bbeta_{1}+ \bX_{2}\bbeta_{2} + \bvarepsi,
\]
donde $\bX_{1}$ es la matriz $n\times (p-r)$ que contiene las columnas de $\bX$ asociadas con $\bbeta_{1}$, y $\bX_{2}$ es la matriz $n\times r$ que contiene las columnas de $\bX$ asociadas con $\bbeta_{2}$. Este es llamado el **modelo completo**.

Para el modelo completo tenemos:

- Estimador de $\bbeta$: 
\[
\hatbbeta = (\bX'\bX)^{-1}\bX'\by.
\]
- Suma de cuadrados del modelo: 
\[
SS_{R}(\bbeta) = \hatbeta'\bX'\by \mbox{ (con }p\mbox{ grados de libertad)}.
\]
- Cuadrado medio del error: 
\[
MS_{res} = \frac{\by'\by - \hatbbeta'\bX'\by}{n-p}.
\]

Para evaluar la contribución de los regresores asociados a $\bbeta_{2}$, ajustamos el modelo asumiendo que $H_{0}$ es cierta. De esta forma tenemos el **modelo reducido**:
\[
\by = \bX_{1}\bbeta_{1} + \bvarepsi.
\]
Para el modelo reducido tenemos:

- Estimador de $\bbeta_{1}$: 
\[
\hatbbeta_{1} = (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\by.
\]
- Suma de cuadrados del modelo: 
\[
SS_{R}(\bbeta_{1}) = \hatbbeta_{1}'\bX_{1}'\by \mbox{ (con $p-r$ grados de libertad)}.
\]

Entonces, la suma de cuadrados debido a $\bbeta_{2}$ dado que $\bbeta_{1}$ ya está en el modelo es:
\[
SS_{R}(\bbeta_{2}| \bbeta_{1}) = SS_{R}(\bbeta) - SS_{R}(\bbeta_{1}),
\]
con $p-(p-r)=r$ grados de libertad. Esta suma de cuadrados es llamada la suma de cuadrados extra debido a $\bbeta_2$ puesto que mide el incremento en la suma de cuadrados de la regresión como resultado de adicionar los regresores $\bX_{2}$ en el modelo que ya contiene $\bX_{1}$.  

Dado que $SS_{R}(\bbeta_{2}| \bbeta_{1})$ y $MS_{res}$ son independientes, podemos utilizar el siguiente estadístico de prueba:
\[
F_{0} = \frac{SS_{R}(\bbeta_{2}|\bbeta_{1})/r}{MS_{res}}.
\]
Si $H_{0}$ es cierta entonces $F_{0} \sim F_{r,n-p}$. Si $H_{0}$ no es cierta, entonces $F_{0}$ sigue una distribución $F$ no-central con parámetro de no centralidad igual a:
\[
\lambda = \frac{1}{\sigma^{2}}\bbeta_{2}'\bX_{2}'\left[ \bI_{n} - \bX_{1}(\bX_{1}'\bX_{1})^{-1}\bX_{1}'\right]\bX_{2}\bbeta_{2}.
\]
Note que si hay una relación casi colineal entre $\bX_{1}$ y $\bX_{2}$ (multicolinealidad), $\lambda$ es cercano a cero pesar que $\bbeta_{2}$ sea marcadamente distinto de cero. Es decir, que la prueba tiene poca capacidad de indicar diferencias (poco poder) en presencia de multicolinealidad. Caso contrario, el máximo poder se alcanza cuando $\bX_{1}$ y $\bX_{2}$ son ortogonales (es decir $\bX_{2}'\bX_{1} = \bZERO$).

Entonces, si $F_{0} > F_{1-\alpha,r,n-p}$ rechazamos $H_{0}$ y concluimos que al menos un coeficiente en $\bbeta_{2}$ es diferente de cero. Consecuentemente, al menos una de las covariables en $\bX_{2}$ tiene un aporte significativo dentro del modelo.

#### Ejemplo {-}
Considere el modelo:
\[
y_{i} = \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \beta_{3}x_{i3} + \varepsilon_{i}.
\]
La suma de cuadrados del modelo se puede descomponer de la siguiente forma:
\[
SS_{R}=SS_{R}(\beta_{1},\beta_{2},\beta_{3}| \beta_{0}) = SS_{R}(\beta_{1}|\beta_{0}) + SS_{R}(\beta_{2}|\beta_{0},\beta_{1}) + SS_{R}(\beta_{3}|\beta_{0},\beta_{1},\beta_{2}),
\]
donde cada suma de cuadrados en el lado derecho tiene un grado de libertad. Además, el order de los regresores en estos componentes marginales es arbitrario. Por lo que la siguiente descomposición alternativa es también válida:
\[
SS_{R}(\beta_{1},\beta_{2},\beta_{3}| \beta_{0})=SS_{R}(\beta_{2}|\beta_{0}) + SS_{R}(\beta_{3}|\beta_{0},\beta_{2}) + SS_{R}(\beta_{1}|\beta_{0},\beta_{2},\beta_{3}).
\]

Sin embargo, la siguiente partición de la suma de cuadrados de la regrsión es generalmente inválida:
\[
SS_{R}(\beta_{1},\beta_{2},\beta_{3}| \beta_{0})\neq SS_{R}(\beta_{1}|\beta_{0},\beta_{2},\beta_{3}) + SS_{R}(\beta_{2}|\beta_{0},\beta_{1},\beta_{3}) + SS_{R}(\beta_{3}|\beta_{0},\beta_{1},\beta_{2}).
\]

## Prueba de hipótesis lineal general
Suponga que estamos interesados en las siguientes hipótesis:
\begin{equation}
H_{0}: \bT\bbeta=\bZERO \qquad H_{1}: \bT\bbeta\neq \bZERO,
(\#eq:hipGeneral1)
\end{equation}
donde $\bT$ es una matriz $m \times p$ de constantes, tal que $r$ de las $m$ ecuaciones de $\bT\bbeta=\bZERO$ son independientes.

El **modelo completo (FM)** es:
\[
\by=\bX\bbeta+\bvarepsi,
\]
El estimador de $\bbeta$ es $\hatbbeta = (\bX'\bX)^{-1}\bX'\by$, y la suma de cuadrados de los residuos es $SS_{res}(FM)$ (con $n-p$ grados de libertad). 

El **modelo reducido (RM)** se obtiene al resolver las $r$ ecuaciones independientes de $\bT\bbeta = \bZERO$ para los $r$ coeficientes en el modelo completo en términos de los $p-r$ coeficientes restantes. Esto lleva al siguiente RM: 
\[
\by=\bZ\bgamma+\bvarepsi,
\]
donde $\bZ$ es una matriz $n\times (p-r)$ y $\bgamma$ es un vector de dimiensión $(p-r)$ de coeficientes de regresión. La suma de cuadrados de los residuos de este modelo es $SS_{res}(RM)$ (con $n-p+r$ grados de libertad).

Dado que el modelo reducido tiene menos parámetros que el modelo completo, $SS_{res}(RM) \geq SS_{res}(FM)$. Para probar \@ref(eq:hipGeneral1) usamos la diferencia entre las sumas de cuadrados de los residuos:
\[
SS_{H} = SS_{res}(RM) - SS_{res}(FM),
\]
con $r$ grados de libertad. $SS_{H}$ es llamado la suma de cuadrados debido a $H_{0}:\bT\bbeta=\bc$. El estadístico de prueba es:
\[
F_{0} = \frac{SS_{H}/r}{SS_{res}(FM)/(n-p)} = \frac{\hatbeta'\bT[\bT(\bX'\bX)\bT']^{-1}\bT\hatbbeta/r}{SS_{res}(FM)/(n-p)}.
\]
Rechazamos $H_{0}$ si $F_{0} > F_{1-\alpha,r,n-p}$.

La hipótesis anterior se puede generalizar de la siguiente forma:
\begin{equation}
H_{0}: \bT\bbeta=\bc \qquad H_{1}: \bT\bbeta\neq \bc,
(\#eq:hipGeneral2)
\end{equation}
Para este caso, el estadístico de prueba es:
\[
F_{0} = \frac{(\bT\hatbbeta-\bc)'[\bT(\bX'\bX)\bT']^{-1}(\bT\hatbbeta-\bc)/r}{SS_{res}(FM)/(n-p)}.
\]
Si $H_{0}$ es cierta, $F_{0}\sim F_{r,n-p}$. Por lo tanto, rechazamos $H_{0}$ si $F_{0} > F_{1-\alpha,r,n-p}$.

#### ejemplo {-}
Considere el modelo:
\[
y_{i} = \beta_{0} + \beta_{1}x_{i1}+ \beta_{2}x_{i2}+ \beta_{3}x_{i3}+ \beta_{3}x_{i3} + \varepsilon_{i},
\]
y queremos probar las siguientes hipótesis:
\begin{align*}
H_{0}:& \beta_{1}=0 & H_{1}:&\beta_{1}\neq 0 \\
      & 2\beta_{2}-\beta_{3}=3 & &2\beta_{2}-\beta_{3}\neq 3 \\
\end{align*}
De aquí tenemos que:
\[
\bT = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 2 & -1 & 0 \\
\end{pmatrix} \mbox{ y } \bc = \begin{pmatrix} 0 \\ 3  \end{pmatrix}.
\]
Si no rechazamos $H_{0}$, podríamos estimar $\bbeta$ sujo a las restricciones impuestar por la hipótesis nula (usando mínimos cuadrados restringidos).

\rule{\textwidth}{0.4pt}
### Bajo peso al nacer - pruebas de hipótesis {-}
Los resultados de las pruebas de hipótesis individuales sobre los coeficientes, análisis de varianza y coeficientes de determianción se obtiene a partir del resumen del modelo:
```{r,echo=FALSE}
mod.summary = summary(mod)
F0 = mod.summary$fstatistic[1]
R2 = mod.summary$r.squared
t2 = mod.summary$coefficients[3,3]
t2vp = mod.summary$coefficients[3,4]
t1 = mod.summary$coefficients[2,3]
t1vp = mod.summary$coefficients[2,4]
mod0 = summary(lm(weight~age,data=birthweight))
R20 = mod0$r.squared
```

```{r}
summary(mod)
```
A partir de estos resutados tenemos que $F_{0}=`r round(F0,4)`$ con un valor-$p$ asociado de $0.000$, es decir que al menos uno de los coeficientes de regresión es diferente de cero. Además, el $`r round(R2*100)`$\% de la variabilidad del peso al nacer es explicada por la edad gestacional y el peso de la madre antes del embarazo $(R^{2}=`r round(R2,4)`)$.  Note que hubo un incremento leve en el $R^{2}$ respecto al modelo que solo incluye la edad gestacional como covariable ($R^{2} = `r round(R20,3)`$).

A partir de las pruebas de hipótesis individuales, podemos decir que el peso de la madre antes del embarazo no tiene un aporte significativo cuando el modelo ya incluye la covariable edad gestacional ($t_{0}=`r round(t2,2)`$ con un valor-$p$ asociado de `r round(t2vp,4)`). Por otro lado, el efecto de la edad gestacional si es significativo ($t_{0}=`r round(t1,2)`$ con un valor-$p$ asociado de `r round(t1vp,4)`).

Ahora consideremos un modelo ingresando dos covariables más:
\[
y_{i} = \beta_{0} + \beta_{1}\mbox{age}_{i} + \beta_{2}\mbox{mppwt}_{i} + \beta_{3}\mbox{motherage}_{i} + \beta_{2}\mbox{mnocig}_{i} + \varepsilon_{i},
\]
donde $\mbox{motherage}_{i}$ y $\mbox{mnocig}_{i}$ es la edad (en años) y el número medio de cigarrillos fumados por mes de la $i$-ésima madre, respectivamente. El resumen del modelo ajustado es:
```{r}
mod.completo = lm(weight~age + mppwt + motherage + mnocig,data=birthweight)
summary(mod.completo)
```
Respecto al modelo anterior, hay un aumento del $R^{2}$ y el $R^{2}_{adj}$. Por lo cuál podemos concluir que al ingresar estas covariables el ajuste mejoró. Aunque, los efectos del peso y la edad de la madre no son significativos a partir de las pruebas individuales. 

Esto no necesariamente quiere decir que podemos eliminar estas dos covariables del modelo, recordemos que las pruebas $t$ son individuales (se evalúa el efecto de la covariable cuando el modelo ya incluye las restantes). Para determinar si podemos eliminar `mppwt` y `motherage` del modelo, realizamos la siguiente prueba de hipótesis:
\[
H_{0}: \beta_{2} = \beta_{3} = 0 \qquad H_{0}: \beta_{j} \neq 0 \mbox{ para algún }j=2,3.
\]
En R podemos hacer esto a través de la función `anova`:
```{r,echo=FALSE}
mod.reducido = lm(weight~age + mnocig,data=birthweight)
ANOVA1= anova(mod.reducido,mod.completo)
ANOVA1vp = round(ANOVA1$"Pr(>F)"[2],4)
ANOVA1vpF = round(ANOVA1$F[2],3)
```
```{r}
mod.reducido = lm(weight~age + mnocig,data=birthweight)
anova(mod.reducido,mod.completo)
```
Aquí vemos que $F_{0}= `r ANOVA1vpF`$ con un valor-$p$ asociado de $`r ANOVA1vp`$. Por lo tanto, no tenemos evidencia suficiente para rechazar $H_{0}$ y podemos retirar las dos covariables del modelo.
\rule{\textwidth}{0.4pt}

## Intervalos de confianza
Al igual que en caso del modelo lineal simple, también podemos hacer estimaciones por intervalos de confianza para los coeficientes del modelo, valor esperado de $Y$ y observaciones futuras.

Para que los intervalos de confianza sean válidos se requiere que se cumplan todos los supuestos del modelo, esto es $\bvarepsi\sim N(\bZERO, \sigma^{2}\bI_{n})$.

### Intervalos de confianza para $\beta_{j}$
El intervalo de confianza de $\beta_{j}$ parte de:
\[
\frac{\hatbeta_{j} - \beta_{j}}{\sqrt{V(\hatbeta_{j})}} =  \frac{\hatbeta_{j} - \beta_{j}}{\sqrt{MS_{res}c_{jj}}} \sim t_{n-p},
\]
donde $c_{jj}$ es la entrada $(j,j)$ de la matriz $\bC=(\bX'\bX)^{-1}$. Entonces, el intervalo del $(1-\alpha)100\%$ de confianza para $\hatbeta_{j}$ es:
\[
\hatbeta_{j} \pm t_{1-\alpha/2,n-p}\sqrt{MS_{res}c_{jj}}.
\]


### Intervalos de confianza para el valor esperado de $Y$ y una observación futura
Ahora queremos construir un intervalo de confianza para la respuesta media de $Y$ para un punto particular $\bx_{0}=(1,x_{01},x_{02},\ldots,x_{0,p-1})$. La estimación puntual en $\bx_{0}$ es:
\[
\hatmu_{Y|\bx_0} = \bx_{0}'\hatbbeta.
\]
Además, tenemos que $\hatmu_{Y|\bx_0} \sim N[\bx_{0}'\bbeta, V(\hatmu_{Y|\bx_0})]$ con:
\[
V(\hatmu_{Y|\bx_0}) = \sigma^{2}\bx_{0}'(\bX'\bX)^{-1}\bx_{0}.
\]
Por lo tanto, el intervalo del $(1-\alpha)100\%$ de confianza para $E(Y|\bx_{0})$ es:
\[
\hatmu_{Y|\bx_0} \pm t_{1-\alpha/2,n-p}\sqrt{MS_{res}\bx_{0}'(\bX'\bX)^{-1}\bx_{0}}.
\]
De igual forma, el intervalo del $(1-\alpha)100\%$ de confianza para una observación futura en $\bx_{0}$ es:
\[
\hatmu_{Y|\bx_0} \pm t_{1-\alpha/2,n-p}\sqrt{MS_{res}\left[1+ \bx_{0}'(\bX'\bX)^{-1}\bx_{0}\right]}.
\]

\rule{\textwidth}{0.4pt}
### Bajo peso al nacer - intervalos de confianza {-}
Siguiendo con el modelo inicial $y_{i}=\beta_{0}+\beta_{1}\mbox{age}_{i}+\beta_{2}\mbox{mppwt}_{i}+\varepsilon_{i}$, los intervalos del 95\% de confianza para los coeficientes son:
```{r,echo=FALSE}
CIbeta = confint(mod)
x0 = data.frame(age=36,mppwt=50)
pred0 = predict(mod,x0,interval='confidence')
x0pred = data.frame(age=38,mppwt=65)
pred0pred = predict(mod,x0pred,interval='prediction')
```

```{r}
confint(mod)
```
Si peso de la madre permance constante, por cada aumento de una semana en la edad gestacional, el peso medio del recién nacido incrementa entre $`r round(CIbeta[2,1],3)*1000`$ y $`r round(CIbeta[2,2],3)*1000`$ gramos con un nivel de confianza del 95\%. Note que el intervalo de confianza para el coeficiente asociado al peso de la madre contiene el valor $0$ (recordemos que no rechazamos la hipótesis nula de la prueba $t$ sobre $\beta_{2}$).

Queremos determinar el peso medio de los recién nacidos en la semana gestacional $36$ y de madres que pesan 50 kilogramos. Es decir $E(Y|\mbox{age}=36, \mbox{mppwt}=50)$. Para esto podemos contruir un intervalo del 95\% de confianza:
```{r}
x0 = data.frame(age=36,mppwt=50)
predict(mod,x0,interval='confidence')
```
Por lo tanto, el peso medio de los recién nacidos en la semana gestacional 36 y de madres que pesan 50 kilogramos está entre $`r round(pred0[2],2)`$ y $`r round(pred0[3],2)`$ kilogramos con un nivel de confianza del 95\%.

Ahora, estamos interesados en predecir el peso de un recién nacido en la semana $38$ y cuya madre peso $65$ kilogramos. Por lo cuál construimos un intervalo del 95\% de predicción:
```{r}
x0pred = data.frame(age=38,mppwt=65)
predict(mod,x0pred,interval='prediction')
```
El peso del recién nacido con estas caracteristica está entre $`r round(pred0pred[2],2)`$ y $`r round(pred0pred[3],2)`$ kilogramos con un nivel de confianza del 95\%.
\rule{\textwidth}{0.4pt}

## Extrapolación oculta en regresión múltiple
Al igual que en regresión simple, al pronosticar una nueva respuesta en un punto dado $\bx_{0}$ se debe tener cuidado de no extrapolar fuera de la región de los datos originales. En regresión múltiple es fácil extrapolar inadvertidamente, puesto que la región que contiene los datos está definida de forma conjunta por los valores que toman las covariables y no por el rango individual de cada covariable.

La Figura \@ref(fig:extrapolacionOculta) muestra un ejemplo de extrapolación en el caso de un modelo de regresión con dos covariables. Se quiere hacer una predicción en el punto $(x_{01},x_{02})$ que está dentro del rango de ambos regresores, pero que fuera de la región conjunta de los datos (región roja en la figura). Por lo tanto, al realizar la predicción en este punto estaríamos extrapolando.

```{r extrapolacionOculta, echo=F, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Ejemplo de extrapolación en regresión múltiple",warning=FALSE,message = FALSE}
library(car)
plot(c(0,2), c(0,2), type="n", main="",ylab=expression(x[2]),xlab=expression(x[1]),xaxt='n',yaxt='n')
test = ellipse(c(1,1), matrix(c(1,0.5,0.5,1),2,2), radius=pi/4, log="", center.pch=NULL, center.cex=NULL, 
        segments=51, draw=TRUE, xlab="", ylab="", 
        col=2, lwd=1, fill=TRUE, fill.alpha=0.1, grid=TRUE)
segments(range(test[,1]),c(-20,-20),range(test[,1]),c(max(test[,2]),max(test[,2])))
segments(c(-20,-20),range(test[,2]),c(max(test[,1]),max(test[,1])),range(test[,2]))
text(1,1,'Región conjunta de los datos',cex = 0.6)
axis(1,min(test[,1]),labels = expression(paste("min ",x[1])),cex.axis = 0.6)
axis(1,max(test[,1]),labels = expression(paste("max ",x[1])),cex.axis = 0.6)
axis(2,min(test[,2]),labels = expression(paste("min ",x[2])),las=1,cex.axis = 0.6)
axis(2,max(test[,2]),labels = expression(paste("max ",x[2])),las=1,cex.axis = 0.6)
points(1.5,0.4,pch=19,col=1)
segments(-20,0.4,1.5,0.4,lty=2)
segments(1.5,-20,1.5,0.4,lty=2)
axis(1,1.5,labels = expression(x["01"]),cex.axis = 0.6)
axis(2,0.4,labels = expression(x["02"]),las=1,cex.axis = 0.6)
```
Determinar la región conjunta de los datos en regresión múltiple no es fácil, lo que hace díficil saber si se está extrapolando a la hora de hacer de una predicción. Por lo tanto, se ha propuesto determinar la región conjunta de los datos a partir del conjunto convexo mínimo que contiene todos los $n$ datos originales, $(x_{i1}; x_{i2},\ldots,x_{i,p-1})$, para $i = 1,2,\ldots,n$, como la envolvente de las covariables (RVH). Entonces, si un punto $(x_{01},x_{02}, \ldots,\ldots, x_{0,p-1})$ está dentro o en la frontera de la RVH, una prediccón o una estimación implica interpolación, mientras que si está fuera de la RVH, se está extrapolando.

Una aproximación de la RVH es a través de la matriz $\bH$. El conjunto de puntos $\bx$ que satisfacen, $\bx'(\bX'\bX)^{-1}\bx \leq \max(h_{ii})$, prudcen un elipsoide que encierra todos los puntos dentro de la $RVH$.  Entonces, un punto de predicción $\bx_{0}$ está fuera de la RVH si $h_{00} > \max{h_{ii}}$, donde:
\[
h_{00} = \bx'_{0}(\bX'\bX)^{-1}\bx_{0}.
\]

\rule{\textwidth}{0.4pt}
### Bajo peso al nacer - interpolación {-}
Suponga que se quiere hacer una predicción para recién nacidos con las características que muestra la Tabla \@ref(tab:puntosPrediccion). 

```{r puntosPrediccion, echo=FALSE }
PredPoints = data.frame(x01 = c(34,75),x02 = c(36,50),x03 = c(38,60),x04 = c(46,55))
rownames(PredPoints) = c('Edad gestacional (semanas)','Peso de la madre (kg)')
colnames(PredPoints) = c('1','2','3','4')
knitr::kable(
  PredPoints, booktabs = TRUE,label='puntosPrediccion',
  caption = 'Bajo peso al nacer. Punto de predicción.'
)
```
La Figura \@ref(fig:birthweightExtrapolacion) muestra el gráfico de dispersión de las covariables, donde los puntos rojos indican los valores donde se quieren hacer predicciones. Aquí vemos que en los puntos $\bx_{02}$ y $\bx_{03}$ no estaríamos extrapolando. Pero, es difícil de determinar para los puntos $\bx_{01}$ y $\bx_{04}$. Para esto vamos a calcular las aproximaciones de la RVH y verificar si en estos puntos estaríamos extrapolando.

```{r birthweightExtrapolacion,echo=FALSE, fig.height = 4, fig.width = 4,fig.align = "center",fig.cap = "Bajo peso al nacer. Gráfico de dispersion de la edad gestacional y el peso de la madre antes del embarazo. Los puntos donde se quiere hacer predicción están en rojo."}
newPoints = cbind(x=c(34,36,38,46),y=c(75,50,60,55))
plot(mppwt~age,data=birthweight,xlab='edad gestacional (en semanas)',ylab='peso de la madre (en kgs)',
     ylim=c(43,80),xlim=c(32,46))
points(newPoints,col=2,pch=15:18)
```

```{r birthweightExtrapolacion2}
newPoints = cbind(x0=rep(1,4),x1=c(34,36,38,46),x2=c(75,50,60,55))
X = model.matrix(mod)
XtX.inv = solve(t(X)%*%X)
h.values = hatvalues(mod)
hmax = max(h.values)
h0 = apply(newPoints,1,function(x){t(x)%*%XtX.inv%*%x})
h0 >hmax
```
Para la predicción en el punto $\bx_{01}$, tenemos que $h_{0} = (1, 34, 77)(\bX'\bX)^{-1}(1, 34, 77)' = `r round(h0[1],4)`$ $>$ $h_{max} = `r round(hmax,4)`$. Por lo tanto, aquí se estaría extrapolando. Para el resto de punto no hay problemas de extrapolación.
\rule{\textwidth}{0.4pt}

### Coeficientes normalizados de regresión
Los coeficientes de regresión están influenciados por las unidades de medida de las covariables. Exactamente las unidades de medida de $\beta_{j}$ es:
\[
\frac{\mbox{la unidad de medida de }y }{\mbox{la unidad de medida de }x_{j}}.
\]
Dado que, por lo general, las covariables están medidas en unidades diferenes, la comparación de los coeficientes es complicada. En el ejemplo de los datos de los recién nacidos, la edad gestacional está en semanas y el peso de la madre en kilogramos.

Por esta razón, en algunas ocasiones es útil escalar los valores de las covariables y la respuesta para calcular los coeficientes de regresión adimensionales. Hay varias formas de hacer este escalamiento, aquí nos centraremos en el escalamiento de longitud unitaria.

#### Escalamiento de longitud unitaria
Una opción es hacer un **escalamiento de longitud unitaria** a las covariables:
\[
z_{ij} = \frac{x_{ij}-\bar{x}}{\sqrt{S_{jj}}}, i=1,2,\ldots,n \quad j=1,2,\ldots,p-1,
\]
y la variable respuesta:
\[
y_{i}^{*} = \frac{y_{i}-\bar{y}}{\sqrt{SS_{T}}},
\]
donde:
\[
S_{jj} = \sum_{i=1}^{n}(x_{ij} - \bar{x}_{j})^{2}.
\]
Con estas variables transformadas, se puede ajustar el modelo:
\[
y_{i}^{*} = b_{1}z_{i1} + b_{2}z_{i2} + \ldots + b_{p-1}z_{i,p-1} = \bz_{i}'\bb + \bvarepsi.
\]
El estimador por MCO es:
\[
\hatbb = (\bZ'\bZ)^{-1}\bZ'\by^{*}.
\]
Note que con este escalamiento, la matriz $(\bZ'\bZ)$ es igual a la matriz de correlación de las covariables. Esto es:
\[
(\bZ'\bZ) = \bR = \begin{pmatrix}
1 & r_{12} & r_{13} & \ldots & r_{1,p-1} \\
r_{12} & 1 & r_{23} & \ldots & r_{2,p-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
r_{1,p-1} & 1 & r_{2,p-1} & \ldots & 1 \\
\end{pmatrix},
\]
donde $r_{jk}$ es la correlación entre las covariables $x_{j}$ y $x_{k}$. Además, la matriz $\bZ'\by^{*}$ es el vector de correlación entre la variable respuesta y cada covariable. Esto es:
\[
\bZ'\by^{*} = (r_{1y},r_{2y},r_{3y},\ldots,r_{p-1,y})',
\]
donde $r_{jy}$ es la correlación entre la variable respuesta y la covariable $x_j$. 

\rule{\textwidth}{0.4pt}
### Bajo peso al nacer - coeficientes de regresión con variables escaladas {-}
Para estimar los coeficientes de regresión escalados, primero debemos escalar las variables:
```{r birthweightVarEsc}
y = birthweight$weight
Z = apply(X[,-1],2,function(x){(x-mean(x))/sqrt(sum((x-mean(x))^2))})
ys = (y-mean(y))/sqrt(sum((y-mean(y))^2))
```
Ahora procedemos a estimar el modelo con las variables escaladas:
```{r birthweightVarEsc2}
mod.std = lm(ys~Z-1)
summary(mod.std)
```
Las estimaciones de los coeficientes son ahora adimensionales y podemos comparar sus magnitudes. Por lo tanto, parece que la covariable edad gestacional es más importante para determinar el peso al nacer que la covariable peso de la madre. Note que, al escalar las variables, los resultados de las pruebas de hipótesis, estimación de $\sigma^{2}$, y los coeficientes de determinación no se ven alterados.
\rule{\textwidth}{0.4pt}

## Multicolinealidad
Un problema que puede afectar enormente el ajuste de un modelo de regresión es la multicolinealidad. Este se presenta cuando hay una dependencia casi lineal entre las covariables.

Recordemos que el estimador por MCO es $\hatbbeta=(\bX'\bX)^{-1}\bX'\by$. Por lo tanto es necesario que la matriz $\bX'\bX$ sea no singular. En caso contrario, no es posible encontrar la inversa y las ecuaciones normales no tendrán una única solución. Cuando sucede esto se debe a que hay al menos una columna de $\bX$ linealmente dependiente. 

En regresión se utiliza las palabras multicolinealidad cuando hay una dependencia aproximada en las columnas de $\bX$. Es decir que al menos una covariable puede representarse, de forma aproximada, como una relación lineal de las otras:
\[
x_{ij} \approx c_{0} + c_{1}x_{i1} + \ldots + c_{j-1}x_{i,j-1} + c_{j+1}x_{i,j+1} + \ldots + + c_{p-1}x_{i,p-1},
\]
para $i=1,\ldots,n$.

Hay que aclarar que la falta de ortogonalidad no es necesariamente un inconveniente, el problema es cuando la relación lineal entre los regresores es casi perfecta, lo que provoca problemas en las inferencias que se hagan. Uno de estos problemas se ilustra a continuación con un ejemplo. 


### ejemplo {-}
Considere el siguiente modelo de regresión:
\[
y_{i} = \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \varepsilon_{i}, \mbox{ con }\varepsilon_{i}\sim N(0,\sigma^{2}),
\]
y se plantean dos posibles matrices de diseño:
\[
\bX_{1} = \begin{pmatrix}
1& 1 \\ 
1 & 5 \\ 
2 & 1 \\ 
2 & 5 \\
\end{pmatrix} \mbox{ y }
\bX_{2} = \begin{pmatrix}
1& 1 \\ 
1 & 2 \\ 
2 & 4 \\ 
2 & 5 \\
\end{pmatrix}.
\]
Haciendo el escalamiento de longitud unitaria a las covariables tenemos que:
\[
\bZ_{1}'\bZ_{1} = \begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix} \mbox{ y }
\bZ_{1}'\bZ_{1} = \begin{pmatrix}
1 & 0.95 \\ 0.95 & 1
\end{pmatrix}.
\]
Por lo tanto, la varianza de $\hatbb$ para ambos casos es:
\[
V(\hatbb_{1}) = \sigma^{2}(\bZ_{1}'\bZ_{1})^{-1} =  \sigma^{2} \begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}^{-1}  = \sigma^{2} \begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}.
\]
y 
\[
V(\hatbb_{2}) = \sigma^{2}_{0}(\bZ_{2}'\bZ_{2})^{-1} =  \sigma^{2} \begin{pmatrix}
1 & 0.95 \\ 0.95 & 1
\end{pmatrix}^{-1} = \sigma^{2} \begin{pmatrix}
10 & 9.49 \\ 9.49 & 10
\end{pmatrix}.
\]
Aquí podemos ver que la varianza de $\hatbb_{2}$ está inflada debido a la alta correlación entre las columnas de $\bX_{2}$. Es 10 veces mayor que la varianza de $\hatbb_{1}$ (las columnas de $\bX_{1}$ son independientes).

En el ejemplo anterior vemos que los valores de la diagonal de la matriz $(\bZ'\bZ)^{-1}$ nos indican en cuanto aumenta la varianza de las estimaciones de los coeficientes debido a la multicolinealidad. Por esta razón, estos valores toman el nombre de **factores de inflación de varianza (VIFs)** y son uno de los indicadores para el diagnostico de este problema.

Se puede demostrar que el VIF de $\beta_{j}$ se puede calcular como:
\[
\mbox{VIF}_{j} = \frac{1}{1-R^{2}_{j}},
\]
donde $R^{2}_{j}$ es el coeficiente de determinación obtenido ajustado una regresión de $x_{j}$	sobre las demás covariables. Si $x_{j}$ es casi linealmente dependiente de algunos de los otros regresores, entonces $R^{2}_{j}$ será cercano a uno y el $VIF_{j}$ será muy alto. Generalmente, un VIF mayor de 10 indica problemas graves de multicolinealidad.

En un capítulo posterior ahondaremos más en este problema.
\rule{\textwidth}{0.4pt}
### Bajo peso al nacer - factores de inflación de varianza
En el caso del peso de los recién nacidos, tenemos que los VIFs son:
```{r birthweightVIF, messages=FALSE}
library(car)
vif(mod)
```
Lo que nos indica que la varianza de las estimaciones de los coeficientes no se inflan debido a multicolinealidad. Recordemos que la correlación entre las dos covariables no es alta (`r cor(birthweight$age,birthweight$mppwt)`).
\rule{\textwidth}{0.4pt}

<!--chapter:end:02-RegresionMultiple.Rmd-->

# Evaluación de los supuestos del modelo
```{r, include=FALSE}
birthweight = read.csv("birthweight.csv",header = T)
```

\rule{\textwidth}{0.4pt}
## Ejemplo 1. Datos de peso al nacer {-}
Retomemos la base de datos de bajo peso al nacer (disponible en el campus virtual), y consideremos el siguiente modelo:
\[
\mbox{weight}_{i} = \beta_{0} + \beta_{1}\mbox{age}_{i} + \beta_{2}\mbox{motherage}_{i} + \beta_{3}\mbox{mnocig}_{i} + \beta_{4}\mbox{mppwt}_{i} + \varepsilon_{i},
\]
con $\varepsilon_{i} \sim N\left(0,\sigma^{2} \right)$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$, para todo $j \neq k$.

El ajuste del modelo es:
```{r}
Birthweight = read.csv('birthweight.csv')
mod.birthweight = lm(weight ~ age + motherage + mnocig + mppwt, data=Birthweight)
summary(mod.birthweight)
```
La edad gestacional y el número de cigarrillos consumidos por la madre tienen efectos signficativos sobre el peso del recién nacido. El primero es un factor de protección, a mayor edad gestacional mayor será el peso del bebé. Mientras que, el consumo de cigarrillos es un factor de riesgo. A mayor consumo, menor peso tendrá el recién nacido. La edad y el peso de la madre, aunque tienen efectos positivos, no son covariables significativas cuando las otras dos covariables ya están incluidas en el modelo.

\rule{\textwidth}{0.4pt}

## Ejemplo 2. Ventas de helados {-}
La base de datos `icecream` (del paquete `orcutt` de R) recopila la siguiente información tomada cada cuatro semanas durante dos años (marzo 1951 a julio 1953):

- **price**: precio promedio del helado (dolares por bote)
- **cons**: consumo medio de helado (botes por persona)
- **temp**: temperatura promedio (en Fahrenheit)

La Figura \@ref{fig:icecream} muestra la relación entre las variables. Aquí podemos observar que a mayor temperatura, el consumo de helado se incrementa. Por otro lado, la relación con el precio no es tan fuerte.

```{r Icecream, echo=T, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "\\label{fig:icecream} Relación entre las variables de los datos de ventas de helados.",warning=FALSE,message = FALSE}
library(orcutt)
data("icecream")
pairs(icecream[,c(2,1,4)])
```

El objetivo del estudio es explicar el consumo de helado en función del precio y la temperatura. Para esto se propone el siguiente modelo:
\[
\mbox{cons}_{i} = \beta_{0} + \beta_{1}\mbox{price}_{i} + \beta_{2}\mbox{temp}_{i} + \varepsilon_{i},
\]
con $\varepsilon_{i} \sim N\left(0,\sigma^{2} \right)$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$, para todo $j \neq k$.

El resumen del modelo es el siguiente:
```{r Icecream2, warning=FALSE,message = FALSE}
  mod.icecream = lm(cons~price+temp,data=icecream)
summary(mod.icecream)
```
De aquí podemos concluir que alrededor del 70\% de la variabilidad del consumo de helado está explicado por el modelo propuesto. Además, el efecto de la temperatura sobre el consumo de helado es signficativamente positivo. Aunque la relación con el precio es negativa, esta no es significativa.
\rule{\textwidth}{0.4pt}

## Ejemplo 3. Longitud del pez lobina boca chica {-}
La base de datos `wblake` (de la librería `alr4`) contiene la edad (en años) y longitud (en mm) de 439 peces lobina boca chica del lago West Bearskin en el nordeste de Minnesota en 1999. El objetivo del estudio es determinar los patrones de crecimiento de este tipo de pez. 

```{r bassFig, echo=T, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "\\label{fig:bass} Relación entre la edad y la longitud de los peces lobina boca chica.",warning=FALSE,message = FALSE}
library(alr4)
data("wblake")
plot(Length~Age,data=wblake,xlab='edad (años)',ylab='longitud (mm)')
```

La Figura \@ref{fig:bassFig} muestra que la relación entre estas dos variables se puede aproximar a una recta, por lo cuál se propone el siguiente modelo:
\[
\mbox{length}_{i} = \beta_{0} + \beta_{1}\mbox{age}_{i} + \varepsilon_{i}
\]
con $\varepsilon_{i} \sim N\left(0,\sigma^{2} \right)$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$, para todo $j \neq k$.

El resumen del ajuste es el siguiente:
```{r bass2, warning=FALSE,message = FALSE}
mod.bass = lm(Length~Age,data=wblake)
summary(mod.bass)
```
Estos resultados muestran que la edad del pez tiene un efecto significativamente positivo. Por cada año del pez, la longitud aumenta 30 milimetros en promedio. Adicionalmente, esta covariable explica el 81\% de la variabilidad de la longitud.
\rule{\textwidth}{0.4pt}

La validez de las conclusiones hechas en estos ejemplos descansa en el cumplimiento de los supuestos sobre los errores. Por esta razón, es de gran importancia que tengamos herramientas para evaluar si los datos analizados no muestran ningún alejamiento de los supuestos asumidos.

## Supuestos del modelo linea múltiple
En el modelo de regresión lineal múltiple:
\[
y_{i}=\beta_{0} + \beta_{1} x_{i1}+\beta_{2} x_{i2}+\ldots + \beta_{p-1} x_{i,p-1} +\varepsilon_{i},
\]
asumimos que la relación entre la variable respuesta y las covariables es lineal, al menos de forma aproximada. Además,

a. $E(\varepsilon_{i})=0$, para $i=1,\ldots,n$,
b. $Var(\varepsilon_{i})=\sigma^{2}$. Homogeneidad de varianza en los errores,
c. $Cov(\varepsilon_{i},\varepsilon_{j})=0$ para todo $i\neq j$. Los errores están incorrelacionados,
d. $\varepsilon_{i} \sim Normal(0, \sigma^{2})$. Los errores se distribuyen de forma normal.

La importancia de realizar procedimientos para validar los supuestos, radica en que ellos inciden en las cualidades de los estimadores por MCO. En caso de no cumplirse se pueden perder propiedades importantes. Si no se cumple el supuesto (a) se obtienen estimaciones sesgadas. Si no se cumplen (b) y (c) los estimadores MCO pierden la condición de optimalidad. Si no se cumple (d) se pierde eficiencia e imposibilita la aplicación de inferencias basadas en normalidad.

En general, no se puede detectar una violación a los supuestos a partir de estadísticos del ajuste del modelo ($R^2$, $F_0$, valores-$t$, etc). El diagnostico se puede hacer por métodos gráficos y pruebas formales (pruebas de hipótesis). Ambos métodos son complementarios, los gráficos sugieren formas particulares de incumplimiento del supuesto, mientras las pruebas formales evalúan su importancia [@behar_validacion_2002].

## Efectos del incumplimiento de los supuestos

### Sesgo por omisión de variables relevantes
Si $E(\bvarepsi) = \bZERO$, entonces $E(\by | \bX) = \bX\bbeta$, y el estimador por MCO es insesgado. Sin embargo, si omitimos variables relevantes dentro del modelo las estimaciones serán sesgadas. Para ver esto, supongamos que el modelo generador de los datos es:
\[
\by = \bX\bbeta + \bvarepsi =  \bX_{1}\bbeta_{1} + \bX_{2}\bbeta_{2} + \bvarepsi,
\]
donde las columnas de la matriz $n\times p$ de covariables $\bX$ está divida en dos submatrices $\bX_{1}$ y $\bX_{2}$ de dimensiones $n \times (p-r)$ y $n\times r$, respectivamente. Además, asumimos que $\bvarepsi \sim N(\bZERO, \sigma^{2}\bI_{n})$.

Ahora, consideramos estimar el siguiente modelo:
\[
\by = \bX_{1}\bbeta_{1} + \bvarepsi^{*},
\]
es decir estamos omitiendo las covariables contenidas en $\bX_{2}$. El estimador de $\bbeta_{1}$ es:
\[
\hatbbeta_{1} = (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\by,
\]
y el estimador de $\sigma^{2}$ es:
\[
\hatsigma^{2}_{1} = \frac{\by'(\bI_{n} - \bH_{1})\by}{n-(p-r)}, \mbox{ donde }\bH_{1} = \bX_{1}(\bX_{1}'\bX_{1})^{-1}\bX_{1}'.
\]
El valor esperado de $\hatbbeta_{1}$ es:
\begin{equation}
\begin{split}
E(\hatbbeta_{1}) &= E\left[(\bX_{1}'\bX_{1})^{-1}\bX_{1}'\by\right] = (\bX_{1}'\bX_{1})^{-1}\bX_{1}'E(\by) \\
&= (\bX_{1}'\bX_{1})^{-1}\bX_{1}'E(\bX_{1}\bbeta_{1} + \bX_{2}\bbeta_{2} + \bvarepsi) = \bbeta_{1} +  (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\bX_{2}\bbeta_{2}.
\end{split}
\nonumber
\end{equation}
Evidentemente, el sesgo de $\hatbbeta_{1}$ depende de la magnitud de $\bbeta_{2}$. Entre más importante sean los efectos asociados a las covariables omitidas $(\bbeta_{2})$, mayor será el sesgo. Si las columnas de $\bX_{1}$ son ortogonales de las columnas de $\bX_{2}$, tenemos que $\bX_{1}'\bX_{2} = \bZERO$. Así que, en este caso particular, $\hatbbeta_{1}$ es insesgado (así omitamos las covariables en $\bX_{2}$).

El valor esperado de $\hatsigma^{2}_{1}$ es:
\[
E(\hatsigma^{2}_{1}) = \sigma^{2} + \frac{\bbeta_{2}'\bX'_{2}(\bI_{n} - \bH_{1})\bX_{2}\bbeta_{2}}{n-(p-r)}.
\]
Dado que $(\bI - \bH_{1})$ es idempotente y, por lo tanto, positiva semi-definida, entonces $E(\hatsigma_{1}^{2}) > \sigma^{2}$. Lo que quiere decir que $\hatsigma^{2}_{1}$ es un estimador sesgado de $\sigma^{2}$.

Ahora veamos el efecto de omitir covariables relevantes sobre las predicciones de $y$ en el punto $\bx_{0} = (\bx_{01}', \bx_{02}')'$. Tenemos que:
\[
\hatby_{0} = \bx_{01}'\hatbbeta_{1} = \bx_{01}'(\bX_{1}'\bX_{1})^{-1}\bX_{1}'\by.
\]
El valor esperado de $\hatby_{0}$ es:
\[
E(\hatby_{0}) = \bx_{01}'E(\hatbbeta_{1}) = \bx_{01}'\left[ \bbeta_{1} +  (\bX_{1}'\bX_{1})^{-1}\bX_{1}'\bX_{2}\bbeta_{2} \right]. 
\]
Por lo tanto las predicciones también son sesgadas, $E(\hatby_{0}) \neq \bx_{01}'\bbeta_{1} + \bx_{02}'\bbeta_{2}$.

### Incorrecta matriz de varianzas de los errores
Cuando $V(\bvarepsi) = \sigma^{2}\bV$, pero asumimos erroneamente que $V(\bvarepsi) = \sigma^{2}\bI_{n}$, el estimador $\hatbbeta$ sigue siendo insesgado. Pero, tenemos que:
\begin{equation}
\begin{split}
V(\hatbbeta) &= V\left[ (\bX'\bX)^{-1}\bX'\by \right] = (\bX'\bX)^{-1}\bX'V(\by)\bX(\bX'\bX)^{-1} \\
&= \sigma^{2}(\bX'\bX)^{-1}\bX'\bV\bX(\bX'\bX)^{-1},
\end{split}
\nonumber
\end{equation}
es, generalmente, diferente de $\sigma^{2}(\bX'\bX)^{-1}$ (la varianza que asumimos como cierta). Igualmente, el estimador por MCO pierde su condición de optimalidad. Es decir, deja de ser el mejor estimador lineal insesgado.

El estimador de $\sigma^{2}$ es sesgado:
\[
E(\hatsigma^{2}) = \frac{\sigma^{2}}{n-p}E\left[\by'(\bI_{n} - \bH)\by\right] = \frac{\sigma^{2}}{n-p} \tr\left[ \bV(\bI_{n}-\bH)\right].
\]
Las predicciones son insesgadas, pero:
\[
V(\haty_{0}) = V(\bx_{0}'\hatbbeta) = \sigma^{2}\bx_{0}'(\bX'\bX)^{-1}\bX'\bV\bX(\bX'\bX)^{-1} \bx_{0},
\]
que es diferente de $\sigma^{2}\bx_{0}'(\bX'\bX)^{-1}\bx_{0}$ (la varianza que asumimos como cierta). 

Cuando $\bV$ es una matriz diagonal: 
\[
\bV = \begin{pmatrix}
v_{11} & 0 & 0 & \ldots & 0 \\
0 & v_{22} & 0 & \ldots & 0 \\
0 & 0 & v_{33} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & v_{nn}
\end{pmatrix},
\]
tenemos que hay **heterocedasticidad**. Cada error tiene una varianza diferente $V(\varepsilon_{j})=\sigma^{2}v_{jj}$, pero están incorrelacionados. Si los valores fuera de la diagonal de $\bV$ son diferentes de cero, entonces los errores están correlacionados. 

La correlación de los errores puede esperarse en algunas situaciones. Por ejemplo, si las observaciones son tomadas en el tiempo puede presentarse correlación temporal. En situaciones en las que se pueda garantizar que las observaciones $(y_{1},y_{2},\ldots,y_{n})$ constituyen una muestra aleatoria, no existirá correlación entre los errores, es decir, que es posible controlar este aspecto, algunas ocasiones, controlando el procedimiento de selección de la muestra.

### Distribución no normal de los errores
La normalidad de los errores permite la estimación por intervalos de confianza no sólo para los coeficientes de regresión, sino también para la predicción. Igualmente, permite el planteamiento de pruebas de hipótesis sobre los parámetros del modelo. Cuando los errores no son normales, estas inferencias no son exactas y pueden llegar a ser inválidas. 

Sin embargo, el teorema central del límite asegura que, bajo ciertas condiciones muy amplias, la inferencias basadas en el estimador de mínimos cuadrados son aproximadamente válidas si el tamaño de muestra es suficientemente grande. Esto significa que los niveles de las pruebas y cobertura de los intervalos de confianza son aproximadamente correctos. 

De la misma forma, los efectos negativos de la no normalidad dependen de que tan alejados estamos de la normalidad. Si la distribución de los errores es parecida a la normal (por ejemplo, $t$-Student), los efectos negativos no son considerables. 

## Residuos del modelo
Los residuos están definidos como:
\[
e_{i} = y_{i}- \haty_{i}, \mbox{ en forma matricial }\be = \by - \hatby = (\bI_{n} - \bH)\by .
\]
Los residuos representan las desviaciones entre las observaciones y el ajuste. Además, estos son combinaciones lineales de los errores:
\[
\be = (\bI_{n} - \bH)(\bX\bbeta +\bvarepsi) = (\bI_{n} - \bH)\bvarepsi.
\]
Por lo tanto toda desviación de las premisas de los errores se debe reflejar en los residuales. Si $\bvarepsi \sim N(\bZERO,\sigma^{2}\bI_{n})$, entonces:
\[
\be \sim N \left[ \bZERO , \sigma^{2}(\bI_{n} - \bH)  \right].
\]
De aquí tenemos que $V(e_{i}) = (1-h_{ii})\sigma^{2}$ y $Cov(e_{i},e_{j}) = - h_{ij}\sigma^{2}$, para todo $i \neq j$. Lo que indica que, aún cuando los errores sean homogéneos en varianza e incorrelacionados, no implica que los residuos lo sean también.  Note que los residuos asociados a puntos alejados del centro de los datos tienen menor varianza. Lo que hace difícil detectar violaciones.

Cuando $n$ es grande comparado con el número de parámetros en el modelo, los residuos si reflejan a los errores en cuanto al comportamiento de su varianza y correlación. Esto es porque $|h_{ij}| \leq 1$, $\sum_{i=1}^{n}h_{ii} = n-p$, y $\sum_{i=1}^{n}h_{ij}=\sum_{j=1}^{n}h_{ij}=1$. Por lo tanto, cuando $n\rightarrow \infty$, $V(e_{i}) = \sigma^{2}$ y $Cov(e_{i},e_{j}) = 0$.

### Residuos estudentizados
Para evitar el inconveniente de la varianza no constante de los residuos, es preferible utilizar los **residuos estudentizados**:
\[
r_{i} = \frac{e_{i}}{\sqrt{\hat{\sigma}^{2}(1-h_{ii})}}, \qquad i=1,2,\ldots,n.
\]
Entonces, $r_{i}$ tiene varianza constante ($V(r_{i})=1$) independiente del lugar de $\bx_{i}$.

### Residuos PRESS y R-student
Como veremos más adelante, los residuos estudentizados se pueden utilizar detectar puntos atípicos. El problema es que si la $i$-ésima observación es bastante inusual, el ajuste del modelo puede estar muy influenciado por esta observación. Lo que puede producir un residuo pequeño. Por esta razón, también se pueden calcular los residuos de predicción (PRESS). Estos se calculan de la siguiente forma:
\[
e_{(i)} = y_{i} - \haty_{(i)}, \mbox{ para }i=1,\ldots,n,
\]
donde $\haty_{(i)}$ es el valor ajustado para la $i$-ésima observación usando todas las observaciones excepto la $i$-ésima. Esto implicaría que para calcular los residuos PRESS es necesario ajustar $n$ veces el modelo. Sin embargo, esto no es así, ya que se puede demostrar que:
\[
e_{(i)} = \frac{e_{i}}{1-h_{ii}}.
\]
La varianza de los residuos PRESS es:
\[
V(e_{(i)}) = V\left( \frac{e_{i}}{1-h_{ii}} \right) = \frac{1}{(1-h_{ii})^{2}}V(e_{i}) = \frac{1}{(1-h_{ii})^{2}} [\sigma^{2}(1-h_{ii})] = \frac{\sigma^{2}}{(1-h_{ii})}.
\]
Si estudentizamos los residuos PRESS obtenemos los **residuos R-Student**:
\[
t_{i} = \frac{e_{i}}{\sqrt{\hatsigma^{2}_{(i)}(1-h_{ii})}},
\]
donde $\hatsigma^{2}_{(i)}$ es la estimación de $\sigma$ usando todas las observaciones excepto la $i$-ésima. Se puede demostrar que:
\[
\hatsigma^{2}_{(i)} = \frac{(n-p)\hatsigma^{2} - e^{2}_{i}/(1-h_{ii})}{n-p-1}.
\]

## Evaluación del cumplimiento de los supuestos
En esta sección mostramos la evaluación de los supuestos a través del análisis de los residuos del ajuste  (ya sean los residuos estudentizados o los R-Student) usando gráficos y pruebas de hipótesis.

### Gráficos de residuos
Un gráfico de los residuos es una forma efectiva de investigar posibles alejamientos de los supuestos. Generalmente, se grafican los residuos estudentizados $(r_{i})$ contra los valores ajustados $\haty_{i}$ (o contra alguna de las covariables $x_{ij}$). Este tipo de gráfico es de gran ayuda para detectar la correcta especificación del modelo y homocedasticidad. Algunos patrones de residuos se pueden observar en la Figura \@ref(fig:PatronesResiduos). 

```{r PatronesResiduos, echo=F, fig.height = 6, fig.width = 9,fig.align = "center",fig.cap = "Ejemplos de posibles patrones de residuos",warning=FALSE,message = FALSE}
set.seed(1)
x1= runif(100,0,1)
e1 = rnorm(100)
set.seed(4)
x2= runif(100,0,1)
e2 = rnorm(100,0,0.2+2*abs(x2))
set.seed(1)
x3= runif(100,0,1)
e3 = rnorm(100,0,1.5-(3*abs(x3-0.5) ))
set.seed(2)
x4= runif(100,-1,1)
e4 = rnorm(100,0,0.5)+2.5*x4^2
e4 = e4 - mean(e4)
par(mfrow=c(2,2))
### residuos comunes
plot(x1,e1,ylim = c(-3,3),xlab='valores ajustados',ylab='residuos estudentizados',xaxt='n',yaxt='n',main='(a)')
abline(h=0,lty=2)
axis(2,0)
## herocedasticidad 1
plot(x2,e2,xlab='valores ajustados',ylab='residuos estudentizados',main='(b)',ylim=c(-5,5),xaxt='n',yaxt='n')
abline(h=0,lty=2)
axis(2,0)
## herocedasticidad 2
plot(x3,e3,xlab='valores ajustados',ylab='residuos estudentizados',main='(c)',ylim=c(-2.5,3.5),xaxt='n',yaxt='n')
abline(h=0,lty=2)
axis(2,0)
### no linealidad
plot(x4,e4,xlab='valores ajustados',ylab='residuos estudentizados',main='(d)',ylim=c(-2.5,3),xaxt='n',yaxt='n')
abline(h=0,lty=2)
axis(2,0)
```
La Figura \@ref(fig:PatronesResiduos)(a) muestra que los residuos se encuentran alrededor de cero y no se observa ningún patrón claro. Esto es un indicio que el modelo está bien especificado y hay homocedasticidad. En la Figura \@ref(fig:PatronesResiduos)(b) vemos que los residuos están alrededor de cero pero la variabilidad crece a medida que los valores ajustados aumenta. Esto es un indicador de heterocedasticidad. La Figura \@ref(fig:PatronesResiduos)(c) también muestra un patrón de heterocedasticidad, la variabilidad aumenta hasta cierto punto y luego decrece. En la Figura \@ref(fig:PatronesResiduos)(d) observamos que los residuos no fluctuan alrededor de cero, sino que siguen una curva. Esto nos que la relación entre la variable respuesta y las covariables no es lineal.

```{r patronesResiduos2, echo=F, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Ejemplos de posibles patrones de residuos (graficando el valor absoluto de los residuos).",warning=FALSE,message = FALSE}
par(mfrow=c(1,3))
plot(x1,abs(e1),xlab='valores ajustados',ylab='valor absoluto de los residuos estudentizados',xaxt='n',yaxt='n',main='(a)',
     ylim=c(0,3))
plot(x2,abs(e2),xlab='valores ajustados',ylab='valor absoluto de los residuos estudentizados',xaxt='n',yaxt='n',main='(b)',
     ylim=c(0,5))
plot(x3,abs(e3),xlab='valores ajustados',ylab='valor absoluto de los residuos estudentizados',xaxt='n',yaxt='n',main='(c)',
     ylim=c(0,3.5))
```

Adicionalmente, para detectar más facilmente heterocedasticidad, se pueden graficar el valor absoluto de los residuos estudentizados (o al cuadrado) contra los valores ajustados (o las covariables). La Figura \@ref(fig:patronesResiduos2) muestra los mismos patrones pero graficando los residuos en valor absoluto. En las Figuras \@ref(fig:patronesResiduos2)(b-c) se evidencia claramente la heterocedasticidad. 

\rule{\textwidth}{0.4pt}
#### Bajo peso al nacer - gráfico de residuos {-}
Para el modelo ajustado para los datos de peso al nacer, el gráfico de los residuos contra los valores ajustados se obtienen de la siguiente forma:
```{r residuosBWdata, message=FALSE, warning=FALSE, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de peso al nacer. Gráfico de los residuos estudentizados contra los valores ajustados."}
library(MASS)
res.stud.birthweight = studres(mod.birthweight)
mod.fit.birthweight = mod.birthweight$fitted.values
par(mfrow=c(1,2))
plot(mod.fit.birthweight,res.stud.birthweight, ylab='residuos estudentizados',
     xlab='valores ajustados',main='(a)')
abline(h=0,lty=2)
lines(lowess(res.stud.birthweight~mod.fit.birthweight), col = 2)
plot(mod.fit.birthweight,abs(res.stud.birthweight), 
     ylab='valor absoluto de los residuos estudentizados',
     xlab='valores ajustados',main='(b)')
lines(lowess(abs(res.stud.birthweight)~mod.fit.birthweight), col = 2)
```
La Figura \@ref(fig:residuosBWdata)(a) muestra que los residuos están alrededor de cero sin mostrar ningún patrón. Note que este gráfico es similar a la Figura \@ref(fig:PatronesResiduos)(a). La línea roja es una suaviación LOWESS (Locally weighted scatterplot smoothing, más detalle ver Apéndice A.5 de @weisberg_applied_2014). Estas suavizaciones permiten ver fácilmente patrones de comportamiento. Aquí vemos que la suavización está cerca de la recta en cero sin mostrar ninguna tendencia o curvatura muy marcada. Por lo tanto, podemos afirmar que la relación entre el peso del recién nacido y las covariables propuesta es lineal. Además, no se observa un problema notorio de heterocedasticidad en ninguno de los dos gráficos de residuos. 
\rule{\textwidth}{0.4pt}

### Gráficos de residuos parciales
Los gráficos de residuos parciales permiten estudiar el efecto marginal de una covariable sobre la respuesta condicionado a que los demás regresores ya están en el modelo. En caso que el gráfico de los residuos muestre posibles curvaturas (por ejemplo, Figura \@ref(fig:PatronesResiduos)(d)), los residuos parciales permiten verificar si estas se presentan debido a una covariable especifica.

Considere el modelo:
\[
y_{i} = \beta_{0} +\beta_{1}x_{i1}+\beta_{2}x_{i2} + \ldots + \beta_{p-1}x_{i,p-1} + \varepsilon_{i}.
\]
Para calcular los residuos parciales, primero estimamos los parámetros $(\hatbeta_{1},\ldots,\hatbeta_{p-1})$ y los residuos ordinarios $(e_{1},\ldots,e_{n})$. Luego, los residuos parciales para la covariable $x_{j}$ se obtienen de la siguiente forma:
\[
e^{*}_{i}(y|x_{j}) = e_{i} - \hatbeta_{j}x_{ij}.
\]
El gráfico de residuos parciales para la covariable $x_{j}$ se obtiene graficando $e^{*}_{i}(y|x_{j})$ contra $x_{j}$. Si la covariable $x_{j}$ entra al modelo linealmente, entonces el gráfico de residuos parciales debe mostrar una tendencia lineal. Por el contrario, si se observa una curva, $x_{j}$ no entra al modelo de forma lineal.

\rule{\textwidth}{0.4pt}
#### Bajo peso al nacer - gráfico de residuos parciales {-}
El gráfico de residuos parciales se obtiene así:
```{r residuosBWdata2, message=FALSE, warning=FALSE, fig.height = 6, fig.width = 7,fig.align = "center",fig.cap = "Datos de peso al nacer. Gráfico de los residuos parciales para cada covariable."}
library(car)
par(mfrow=c(2,2))
crPlots(mod.birthweight,'age',xlab='edad gestacional')
crPlots(mod.birthweight,'motherage',xlab='edad de la madre')
crPlots(mod.birthweight,'mnocig',xlab='número de cigarrillos por mes')
crPlots(mod.birthweight,'mppwt',xlab='peso de la madre')
```
Igual que en el gráfico de residuos, los residuos parciales no muestran tendencias no lineales muy marcada. Por lo que se puede asumir que la relación entre el peso al nacer y las covariables es lineal.
\rule{\textwidth}{0.4pt}


### Gráficos de normalidad
Los gráficos cuantil-cuantil (qqplot) comparan los cuantiles muestrales contra los cuantiles que se esperarían con la distribución de probabilidad asumida para los datos (cuantiles teóricos). En el caso de regresión lineal, estamos asumiendo que los errores del modelo siguen una distribución normal. Por lo tanto, debemos comparar los cuartiles muestrales de los residuos con los cuartiles teóricos que se esperarían bajo una distribución normal.

Sea $(x_{1},x_{2},\ldots,x_{n})$ una muestra aleatoría de la variable $X$ con función de distribución desconocida $F_{X}(x)$, y sean $(x_{[1]},x_{[2]},\ldots,x_{[n]})$ los estadísticos de orden (observaciones ordenadas de forma creciente). La función empirica de distribución es:
\[
S_{n}(x_{[i]}) = \frac{i}{n} = \frac{\mbox{\# de observaciones }\leq x_{[i]}}{n}.
\]
Si asumimos que $X\sim N(0,1)$, entonces los puntos $(x_{[i]},\Phi^{-1}\left\{S_{n}(x_{[i]})\right\})$, donde $\Phi^{-1}()$ es la inversa de la función acumulativa de una normal estándar, deben seguir aproximadamente una línea recta. 

La Figura \@ref(fig:qqplots) muestra diferentes patrones de gráficos de normalidad para datos generados a partir de tres distribuciones diferentes: normal estándar (derecha), exponencial con $\lambda=1$ (centro), y $t$-Student con 2 grados de libertad (derecha). Aquí vemos que para los datos normales, los cuantiles muestrales y teóricos siguen aproximadamente la linea recta de referencia. Mientras que en los otros dos casos, los puntos se alejan en los extremos. En el caso de los datos exponenciales, es al lado izquierdo, mostrando que los datos presentan asímetrica. Mientras que con los datos $t$-Student, es a ambos lados, indicando que hay muchos valores en las colas (más de los esperados bajo normalidad).



```{r qqplots, echo=F, fig.height = 4, fig.width = 9,fig.align = "center",fig.cap = "Gráficos de normalidad para datos aleatorios generados a partir de una distribución normal estándar (izquierda), exponencial (centro), y t-Student con 2 grados de libertad (derecha)."}
set.seed(1)
x1 = rnorm(100)
x2 = rexp(100)
x3 = rt(100,2)
par(mfrow=c(1,3))
qqnorm(x1,xlab = 'cuartiles teóricos',ylab='cuartiles muestrales',main='datos normales')
qqline(x1)
qqnorm(x2,xlab = 'cuartiles teóricos',ylab='cuartiles muestrales',main='datos exponenciales',
       ylim=c(-1,3.5))
qqline(x2)
qqnorm(x3,xlab = 'cuartiles teóricos',ylab='cuartiles muestrales',main='datos t-Student')
qqline(x3)
```

\rule{\textwidth}{0.4pt}
#### Bajo peso al nacer - gráfico de normalidad {-}
El gráfico cuartil-cuartil de los residuos estudentizados se obtiene así:
```{r qqnormBWdata1, message=FALSE, warning=FALSE, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Datos de peso al nacer. Gráfico cuantil-cuantil para normalidad."}
car::qqPlot(mod.birthweight,xlab='cuantiles teóricos',ylab='residuos estudentizados',
       distribution = 'norm')
```
Note que la función `qqPlot` incluye intervalos del $95\%$ de confianza para los estadísticos de orden. Para más detalle de como se calculan, ver Sección 12.1.1 de @fox_applied_2016. 

La Figura \@ref(fig:qqnormBWdata1) muestra que los puntos siguen de forma aproximada una línea recta. Además, la mayoría de los puntos están dentro de las bandas de confianza. Por lo tanto, se estaría cumpliendo el supuesto de normalidad.
\rule{\textwidth}{0.4pt}

### Gráfico de residuos frente a el tiempo
Cómo se mencionó anteriormente, si las observaciones fueron tomadas de forma independiente, entonces se puede garantizar que los errores también lo sean. Para los datos del peso de los recién nacidos, tendríamos que asumir que los bebés fueron seleccionados de forma totalmente aleatoria, y así garantizar que los errores no están correlacionados. Por el contrario, los datos del consumo de helado fueron tomados a lo largo del tiempo (cada dos semanas), por lo que se puede presentar **correlación temporal**. Es decir, observaciones tomadas en tiempo cercanos se espera que estén altamente correlacionadas.

En caso de posible correlación temporal, se puede hacer un gráfico de los residuos con respecto al tiempo. Alternativamente, se puede hacer un gráfico de los residuos rezagados. Es decir, los residuos en el tiempo $t$ ($e_{t}$) contra los residuos en el tiempo inmediatamente anterior $(e_{t-1})$. 

La Figura \@ref(fig:residuosCorr) muestra diferentes patrones de comportamiento para residuos correlacionados temporalmente. En la columna (a) vemos el comportamiento de residuos incorrelacionados. Estos fluctúan alrededor de cero sin mostrar ningún patrón claro, además el gráfico de residuos rezagados no muestra niguna tendencia. Por el contrario, en las columnas (b) y (c) vemos patrones de comportamiento de residuos correlacionados de forma positiva y negativa, respectivamente. Cuando hay correlación positiva, los valores de los residuos que están cercanos en el tiempo tienden a ser muy similares, además el gráfico de los residuos rezagados muestra una relación positiva entre $e_{t}$ y $e_{t-1}$. Cuando la correlación es negativa, vemos que los residuos en el tiempo cambian de signo constantemente, además el gráfico de residuos rezagados muestra una tendencia negativa.

```{r residuosCorr, echo=F, fig.height = 5, fig.width = 7,fig.align = "center",fig.cap = "Patrones de correlación temporal de los residuos. En la columna (a) los residuos están incorrelacionados, (b) los residuos tienen correlación positiva, y en (c) los residuos tienen correlación negativa.",warning=FALSE,message = FALSE}
set.seed(1)
e1 <- arima.sim(list(order = c(1,0,0), ar = 0.0000001), n = 50, sd=20)
e2 <- arima.sim(list(order = c(1,0,0), ar = 0.8), n = 50, sd=20)
e3 <- arima.sim(list(order = c(1,0,0), ar = -0.8), n = 50, sd=20)

par(mfrow=c(2,3))
plot(e1-mean(e1),type='b',xlab='tiempo',ylab='residuos',xaxt='n',yaxt='n',main='(a)')
abline(h=0,lty=2)
axis(2,0)
plot(e2-mean(e2),type='b',xlab='tiempo',ylab='residuos',xaxt='n',yaxt='n',main='(b)')
abline(h=0,lty=2)
axis(2,0)
plot(e3-mean(e3),type='b',xlab='tiempo',ylab='residuos',xaxt='n',yaxt='n',main='(c)')
abline(h=0,lty=2)
axis(2,0)

plot(e1[-1],e1[-50],xlab='residuos (t-1)',ylab='residuos (t)',xaxt='n',yaxt='n')
abline(h=0,lty=2)
plot(e2[-1],e2[-50],xlab='residuos (t-1)',ylab='residuos (t)',xaxt='n',yaxt='n')
abline(lm(e2[-50]~e2[-1]),lty=2)
plot(e3[-1],e3[-50],xlab='residuos (t-1)',ylab='residuos (t)',xaxt='n',yaxt='n')
abline(lm(e3[-50]~e3[-1]),lty=2)
```
\rule{\textwidth}{0.4pt}
#### Consumo de helado - gráfico de los residuos contra el tiempo {-}
Los gráficos de los residuos para el modelo ajustado a los datos de consumo de helado se observan en la Figura \@ref(fig:heladosResiduos1). Aquí vemos que la relación entre la variable respuesta y las covariables es aproximadamente lineal, no hay problemas de heterocedasticidad, y que los residuos siguen una distribución normal. Aunque se ve la presencia de un punto atípico.
```{r heladosResiduos1, message=FALSE, warning=FALSE, fig.height = 4, fig.width = 9,fig.align = "center",fig.cap = "Datos de consumo de helado. Gráfico de los residuos estudentizados contra los valores ajustados (izquierda) y gráfico de normalidad (derecha)."}
res.stud.icecream = studres(mod.icecream)
mod.fit.icecream = mod.icecream$fitted.values
par(mfrow=c(1,2))
plot(mod.fit.icecream,res.stud.icecream, ylab='residuos estudentizados',
     xlab='valores ajustados')
abline(h=0,lty=2)
lines(lowess(res.stud.icecream~mod.fit.icecream), col = 2)
car::qqPlot(mod.icecream,xlab='cuantiles teóricos',ylab='residuos estudentizados',
       distribution = 'norm')
```

Dado que las observaciones del consumo de helado tienen un orden temporal (fueron tomadas cada 4 semanas), se pueden graficar los residuos contra el tiempo y de los residuos rezagados. Estos se pueden observar en la Figura \@ref(fig:heladosResiduos2). Aquí podemos ver que hay una correlación temporal positiva. Además, se tiene que $cor(e_{t},e_{t-1}) = `r round(cor(res.stud.icecream[-1],res.stud.icecream[-30]),3)`$.

```{r heladosResiduos2, message=FALSE, warning=FALSE, fig.height = 4, fig.width = 7,fig.align = "center",fig.cap = "Datos de consumo de helado. Gráfico de los residuos estudentizados contra el tiempo (izquierda) y gráfico de residuos rezagados (derecha)."}
par(mfrow=c(1,2))
plot(res.stud.icecream, ylab='residuos estudentizados',
     xlab='tiempo',type='b')
abline(h=0,lty=2)
plot(res.stud.icecream[-30],res.stud.icecream[-1], ylab='residuos estudentizados (t)',
     xlab='residuos estudentizados (t-1)')
abline(lm(res.stud.icecream[-1] ~ res.stud.icecream[-30]),lty=2)
```
\rule{\textwidth}{0.4pt}

## Pruebas de hipótesis para evaluar los supuestos

### Prueba de falta de ajuste
El objetivo de la prueba de falta de ajuste es determinar si la relación entre la variable respuesta y las covariables puede asumirse cómo lineal. Hay que tener en cuenta que esta prueba es sensible a ajamientos de los supuestos de normalidad, varianza constante e independencia de los errores. Además, requiere que se tengan mútiple observaciones de $y$ para diferentes niveles de $\bx$. Por ejemplo, el test puede implementarse en los datos de la longitud de los peces puesto que tenemos varios individuos con las mismas edades. En los otros casos, no es posible, por lo menos de forma exacta. El objetivo de estas observaciones replicadas es tener una estimación independiente de $\sigma^{2}$.

En regresión lineal simple, suponga que se tienen $n_{i}$ observaciones de la variable respuesta para el $i$-ésimo nivel de $x_{i}$, para $i=1,\ldots,m$, y denotemos $y_{ij}$ como la $j$-ésima observación de la respuesta en $x_{i}$, para $j=1,\ldots,n_{i}$. Por lo tanto, tenemos $n=\sum_{i=1}^{m}n_{i}$ observaciones en total.  Esto permite que se puede descomponer la suma de cuadrados de los residuos en dos:
\[
SS_{res} = SS_{PE} + SS_{LOF},
\]
donde $SS_{PE}$ y $SS_{LOF}$ son la suma de cuadrados del error puro y falta de ajuste, respectivamente.

Para esto, primero observemos que los residuos se pueden descomponer así:
\begin{equation}
y_{ij} - \haty_{i} = (y_{ij} - \bar{y}_{i}) + (\bar{y}_{i} - \haty_{i}),
(\#eq:ssresd)
\end{equation}
donde $\bar{y}_{i}$ es el promedio de las $n_{i}$ observaciones en $x_{i}$. Ahora, elevando al cuadrado ambos lados de \@ref(eq:ssresd), y sumando para todo $i$ y $j$, tenemos:
\begin{equation}
\begin{split}
\sum_{i=1}^{m} \sum_{j=1}^{n_{i}} (y_{ij} - \haty_{i})^{2} &= \sum_{i=1}^{m}\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y}_{i})^{2} + \sum_{i=1}^{m}n_{i}(\bar{y}_{i}-\haty_{i})^{2}, \\
SS_{res} &= SS_{PE} + SS_{LOF}.
\end{split}
\nonumber
\end{equation}

Note que la suma de cuadrados de la falta de ajuste es una suma ponderada de las diferencias entre el promedio de las observaciones en cada nivel de $x$ y el correspondiente valor ajustado. Por lo tanto, si la relación entre las variables es aproximadamente lineal, entonces se espera que $SS_{LOF}$ sea cercana a cero.

Los grados de libertad de $SS_{PE}$ y $SS_{LOF}$ son $\sum_{i=1}^{m}(n_{i}-1) = n-m$ y $m-2$, respectivamente. De aquí se define el cuadrado medio del error puro y el cuadrado medio de la falta de ajuste:

\[
MS_{PE} = \frac{SS_{PE}}{n-m} \mbox{ y } MS_{LOF} = \frac{SS_{LOF}}{m-2},
\]

respectivamente. Si se cumple el supuesto de homocedasticidad, el valor esperado de $MS_{PE}$ es:
\[
E(MS_{PE}) = \frac{1}{n-m}E\left[ \sum_{j=1}^{n_{i}}(y_{ij} - \haty_{i})^{2} \right] = \frac{1}{n-m}\sum_{j=1}^{n_{i}} E\left[ (y_{ij}-\haty_{i})^{2} \right] = \frac{\sigma^{2}}{n-m}\sum_{j=1}^{n_{i}}(n_{i}-1) = \sigma^{2}.
\]
Es decir que el cuadrado medio del error puro es una estimación de $\sigma^{2}$ independiente del ajuste del modelo.

Además, el valor esperado del cuadrado medio de la falta de ajuste es:
\[
E(MS_{LOF}) = \sigma^{2} + \frac{\sum_{i=1}^{m}n_{i}\left[ E(y_{i}|x_{i}) - \beta_{0}-\beta_{1}x_{i} \right]^{2}}{m-2}.
\]
Por lo tanto, si la función de la media es lineal, entonces $E(y_{i}|x_{i}) = \beta_{0}+\beta_{1}x_{i}$, y  $E(MS_{LOF}) = \sigma^{2}$. Si la función media no es lineal, entonces $E(MS_{LOF}) > \sigma^{2}$

La prueba de falta de ajuste plantea las siguiente hipótesis:
\[
H_{0}: \mbox{el lineal modelo proporciona buen ajuste} \qquad H_{1}: \mbox{el modelo lineal no proporciona buen ajuste},
\]
El estadístico de prueba es:
\[
F_{0}=\frac{SS_{LOF}/(m-2)}{SS_{PE}/(n-m)} = \frac{MS_{LOF}}{MS_{PE}}.
\]
Si $H_{0}$ es cierta, $F_{0}$ sigue una distribución $F_{m-2,n-m}$. Por lo tanto, se rechaza $H_{0}$ (es decir, la función de regresión no es lineal) si $F_{0} > F_{1-\alpha,m-2,n-m}$.

Como se mencionó antes, la prueba exige que se tenga múltiples observaciones para cada nivel de $x$. Lo cuál es un gran desventaja. En caso que esto no ocurra, se puede hacer agrupaciones de los valores de las covariables (vecinos más cercanos), y considerarlas como repeticiones; de esta manera la prueba es aproximada.

\rule{\textwidth}{0.4pt}
#### Base de datos de la longitud de los peces - prueba de falta de ajuste
La Figura \@ref(fig:pecesResiduos1) muestra los gráficos de los residuos para el modelo ajustado a los datos de la longitud de los peces.
```{r pecesResiduos1, message=FALSE, warning=FALSE, fig.height =4, fig.width = 5,fig.align = "center",fig.cap = "Datos de la longitud de los peces. Gráfico de los residuos estudentizados contra los valores ajustados."}
res.stud.bass = studres(mod.bass)
mod.fit.bass = mod.bass$fitted.values
plot(mod.fit.bass,res.stud.bass, ylab='residuos estudentizados',
     xlab='valores ajustados')
abline(h=0,lty=2)
lines(lowess(res.stud.bass~mod.fit.bass), col = 2)
```
La prueba de falta se ajuste se puede hacer usando la función `anovaPE` de la librería `EnvStats`:
```{r, message=FALSE,warning=FALSE}
library(EnvStats)
anovaPE(mod.bass)
```
A partir del valor-$p$ podemos concluir que no se rechaza $H_{0}$, por lo tanto no hay suficiente evidencia para dudar del ajuste lineal propuesto.
\rule{\textwidth}{0.4pt}

### Prueba de heterocedasticidad
Algunas pruebas de heterocedasticidad asumen que la varianza de los errores se compone de una parte constante y otra que varía según unas variables $(\bz)$:
\[
\sigma^{2}_{i} = f(\sigma^{2},\bz_{i}),
\]
donde $\sigma^{2}$ es la parte fija de la varianza, $\bz_{i}$ el conjunto de variables cuyos valores se asocian con los cambios en la varianza de los errores. Por lo general se asume que la función de varianza depende de algunas de las covariables del modelo, es decir que $\bz_{i}=\bx_{i}$.

La **prueba de Breusch-Pagan** asume que la varianza es una función aditiva de las covariables:
\[
\sigma^{2}_{i} = E(\varepsilon_{i}^{2}) = \gamma_{0} + \gamma_{1}x_{i1}+ \gamma_{1}x_{i2} + \ldots  + \gamma_{p-1}x_{i,p-1}.
\]

Por lo tanto, se pueden plantear las siguientes hipótesis:
\begin{equation}
\begin{split}
H_{0}:& \gamma_{1} = \gamma_{2} = \ldots = \gamma_{p-1} = 0 \qquad \mbox{ (homocedasticidad)}, \\
H_{1}:& \gamma_{j}\neq 0 \mbox{ para algún }j=1,\ldots,p-1 \mbox{ (heterocedasticidad)}.
\end{split}
\nonumber
\end{equation}
Dado que los errores no son observables, el estadístico de prueba se construye a partir de los residuos del modelo estimado. Primero, se ajusta el siguiente modelo de regresión:
\[
e_{i}^{2} = \gamma_{0} + \gamma_{1}x_{i1}+ \gamma_{1}x_{i2} + \ldots  + \gamma_{p-1}x_{i,p-1} + \upsilon_{i},
\]
donde se asume que $\upsilon_{i}\sim N(0,\sigma^{2}_{\upsilon})$, y se obtiene el coeficiente de determinación $R^{2}_{e}$.

si $H_{0}$ es cierta, se tiene que $W = n R^{2}_{e} \sim \chi^{2}_{p-1}$. Entonces, si $W > \chi^{2}_{1-\alpha,p-1}$ se rechaza $H_{0}$. Por lo tanto, hay heterocedasticidad.

El test de Breusch-Pagan sólo detecta formas lineales de heterocedasticidad. La **prueba de White** propone que la relación entre la varianza y las covariables es cuadrática:
\begin{equation}
\begin{split}
\sigma^{2}_{i} &= \left(\gamma_{0}^{*} + \sum_{j=1}^{p-1}\gamma_{j}^{*}x_{ij}\right)^{2} \\
&= \gamma_{0} + \sum_{j=1}^{p-1}\gamma_{j}x_{ij} + \sum_{j=1}^{p-1}\gamma_{jj}x_{ij}^{2} + \sum_{j=1}^{p-1}\sum_{k \neq j}\gamma_{jk}x_{ij}x_{ik}.
\end{split}
\nonumber
\end{equation}
Dado que el número de parámetros del modelo se incrementa rápidamente a medida que tengamos más covariables, se pueden omitir las interacciones entre las covariables.

Por ejemplo si se tiene un modelo con tres covariables, se plantea la siguiente función para la varianza:
\[
\sigma^{2}_{i} = \gamma_{0} + \gamma_{1}x_{i1} + \gamma_{1}x_{i2}  + \gamma_{1}x_{i3} + \gamma_{4}x_{i1}^{2}+\gamma_{5}x_{i2}^{2} + \gamma_{6}x_{i3}^{2} + \gamma_{7}x_{i1}x_{i2} + \gamma_{8}x_{i1}x_{i3} + \gamma_{9}x_{i2}x_{i3}
\]
y las hipótesis son:
\begin{equation}
\begin{split}
H_{0}:& \gamma_{1} = \gamma_{2} = \ldots = \gamma_{9} = 0 \qquad \mbox{ (homocedasticidad)}, \\
H_{1}:& \gamma_{j}\neq 0 \mbox{ para algún }j=1,\ldots,9 \mbox{ (heterocedasticidad)}.    
\end{split}
\nonumber
\end{equation}
Para calcular el estadístico de prueba, primero ajustamos el siguiente modelo auxiliar:
\[
e_{i}^{2} = \gamma_{0} + \gamma_{1}x_{i1} + \gamma_{1}x_{i2}  + \gamma_{1}x_{i3} + \gamma_{4}x_{i1}^{2}+\gamma_{5}x_{i2}^{2} + \gamma_{6}x_{i3}^{2} + \gamma_{7}x_{i1}x_{i2} + \gamma_{8}x_{i1}x_{i3} + \gamma_{9}x_{i2}x_{i3} + \upsilon_{i},
\]
y el coeficiente de determinación asociado $R_{e}^{2}$. El estadístico de prueba es $W=nR_{e}^{2}$, y rechazamos $H_{0}$ si $W > \chi^{2}_{1-\alpha,8}$.

\rule{\textwidth}{0.4pt}
#### Bajo peso al nacer - prueba de heterocedasticidad {-}
En la prueba de White se asume que:
\begin{equation}
\begin{split}
\sigma^{2}_{i} =& \gamma_{0} + \gamma_{1}\mbox{age}_{i} + \gamma_{2}\mbox{motherage}_{i}  + \gamma_{3}\mbox{mnocig}_{i} + \gamma_{4}\mbox{mppwt}_{i}+ \\
& \gamma_{5}\mbox{age}_{i}^{2} + \gamma_{6}\mbox{motherage}_{i}^{2}  + \gamma_{7}\mbox{mnocig}_{i}^{2} + \gamma_{8}\mbox{mppwt}_{i}^{2}.
\end{split}
\nonumber
\end{equation}
Dado que se tienen varias covariables, se omitieron las interacciones entre las covariables. Se plantean las siguientes hipótesis:
\begin{equation}
\begin{split}
H_{0}:& \gamma_{1} = \gamma_{2} = \ldots = \gamma_{8} = 0 \qquad \mbox{ (homocedasticidad)}, \\
H_{1}:& \gamma_{j}\neq 0 \mbox{ para algún }j=1,\ldots,8 \mbox{ (heterocedasticidad)}.
\end{split}
\nonumber
\end{equation}
El ajuste del modelo auxiliar es:
```{r}
res.stud.birthweight=mod.birthweight$residuals
mod.res.birthweight = lm(res.stud.birthweight^2 ~ age + motherage + mnocig + mppwt +
                           I(age^2) + I(motherage^2) + I(mnocig^2) + I(mppwt^2), 
                         data=birthweight)
summary(mod.res.birthweight)
```
Note que el estadístico $F_{0}$ es pequeño y que las pruebas individuales sobre los coeficientes no son significativas. Esto es un indicio que $H_{0}$ es cierta. El coeficiente de determinación es $R_{e}^{2} = `r round(summary(mod.res.birthweight)$r.squared,3)`$. Entonces, $W=  `r round(summary(mod.res.birthweight)$r.squared*nrow(birthweight),3)`$, con un valor-$p$ asociado de $`r round(1-pchisq(summary(mod.res.birthweight)$r.squared*nrow(birthweight),8),4)`$. Por lo tanto, no tenemos evidencia suficiente para determinar que hay heterocedasticidad.

La prueba se puede implementar directamente usando la función `bptest` de la librería `lmtest` llegando a los mismos resultados:
```{r}
bptest(mod.birthweight, ~ age + motherage + mnocig + mppwt +
                           I(age^2) + I(motherage^2) + I(mnocig^2) + I(mppwt^2), 
                         data=birthweight)
```
\rule{\textwidth}{0.4pt}

Otras pruebas de heterocedasticidad que se pueden utilizar son:

- Prueba de Goldfeld–Quandt.
- Prueba de Barlett.
- Prueba de Cochran.
- Prueba de Hartley.

Las últimas tres pruebas requieren que se tengan múltiples observaciones para cada nivel de $\bx$. En caso que no se tengan repeticiones, es posible agruparlas por los vecinos más cercanos e implementar la prueba de forma aproximada.

### Prueba de normalidad
Para probar normalidad podemos utilizar la prueba de Shapiro-Wilks. 

Suponga que se tiene una muestra aleatoria $x_{1},\ldots,x_{n}$ que se asumen sigue una distribución normal. Por lo cuál, se plantean las siguientes hipótesis:
\[
H_{0}: \mbox{la distribución de }X \mbox{ es normal} \qquad H_{0}: \mbox{la distribución de }X \mbox{ no es normal}
\]
El estadístico de prueba propuestos por Shapiro y Wilks es:
\[
W = \frac{ \sum_{i=1}^{[n/2]}a_{in}\left(x_{[n-i+1]}-x_{[i]}\right)  }{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}},
\]
donde $(x_{[1]},x_{[2]},\ldots,x_{[n]})$ son los estadísticos de orden y los valores $a_{in}$, así como los valores críticos, están dados en tablas tabuladas por los autores.


\rule{\textwidth}{0.4pt}
#### Bajo peso al nacer - prueba de normalidad {-}
La prueba de Shapiro-Wilks para ajuste de los datos de peso al nacer es:
`lmtest`:
```{r}
shapiro.test(res.stud.birthweight)
```
A partir de este resultado, no tenemos evidencia suficiente para rechazar que los errores se distribuyen normal. En esta función de R, el valor-$p$ es calculado usando una aproximación.

\rule{\textwidth}{0.4pt}

Otras pruebas de normalidad que se pueden utilizar son: 

- Modificaciones de la prueba Shapiro-Wilks, como son: D'agostino o Shapiro-Francia.
- Pruebas de bondad de ajuste generales, como son: Kolmogorov-Smirnov, Cramer-Von Mises, Anderson-Darling.

### Prueba de correlación temporal de los errores
Cuando la correlación es debido a que las observaciones fueron tomadas en el tiempo, se puede asumir que hay **autocorrelación**. Aquí se asume que los errores que están separados $t$ unidades de tiempo siempre tienen la misma correlación lineal. Además que la correlación disminuye a medida que las observaciones se separan en el tiempo.

El modelo de regresión, con errores autoregresivos de orden uno, es el siguiente:
\[
y_{t} = \bx_{t}'\bbeta + \varepsilon_{t}, \mbox{ con }\varepsilon_{t}= \phi\varepsilon_{t-1} + a_{t},
\]
donde $y_{t}$ y $\bx_{t}$ son la variable respuesta observada y el conjunto de covariables observadas en el tiempo $t$, respectivamente, y $\phi$ es el parámetro de autocorrelación $(|\phi| < 1)$. Además, se asume que $a_{t} \sim N(0,\sigma^{2}_{a})$ y $cov(a_{j},a_{k})=0$, para todo $j \neq k$. A partir de estos resultados, se tiene que:
\[
E(\varepsilon_{t}) = 0, V(\varepsilon_{t}) = \sigma^{2}_{a}\left( \frac{1}{1-\phi^2}\right), \mbox{ y } cov(\varepsilon_{t},\varepsilon_{t \pm k})= \phi^{k}\sigma^{2}_{a}\left( \frac{1}{1-\phi^2} \right).
\]
Por lo tanto, la correlación entre dos errores separados en $k$ periodos de tiempo es $cor(\varepsilon_{t},\varepsilon_{t\pm k}) = \phi^{k}$. Si $\phi > 0$, los errores están correlacionados positivamente, pero la magnitud de la correlación disminuye a medida que los errores se separan más. Por otro lado, si $\phi =0$ los errores están incorrelacionados.

La prueba de Durbin-Watson plantea las siguientes hipótesis:
\[
H_{0}: \phi = 0 \mbox{ (independencia)} \qquad H_{1}:\phi \neq 0  \mbox{ (autocorrelación)}
\]
El estadístico de prueba es:
\[
d = \frac{\sum_{t=2}^{T}\left( e_{t} - e_{t-1}\right)^{2}}{\sum_{t=1}^{T}e_{t}^{2}}.
\]
La distribución de probabilidad de $d$, bajo $H_{0}$, depende de la estructura de $\bX$ y es difícil de determinar. Por lo tanto, los valores críticos están tabulados para diferentes valores de significancia, tamaño de muestra y número de parámetros. Otra alternativa, usada por los paquetes estadísticos, es calcular la significancia a través de métodos de remuestreo y aproximaciones del estadístico de prueba a la distribución normal.

\rule{\textwidth}{0.4pt}
#### Ventas de helado - prueba de correlación temporal {-}
La prueba de Durbin-Watson para ajuste de los datos de ventas de helado se puede hacer a través de la función `durbinWatsonTest` de la librería `car`:
```{r}
durbinWatsonTest(mod.icecream,method='resample',reps=1000)
```
Estos resultados muestran que hay correlación serial en los datos. Note que con esta función, el valor-$p$ es calculado a partir de técnicas de remuestreo usando 1000 repeticiones.
\rule{\textwidth}{0.4pt}

## Comentarios finales
Algunas consideraciones, muchas de ellas tomadas de @behar_validacion_2002:

- La efectividad de las pruebas formales depende del tamaño de la muestra. Si $n$ es pequeño, la potencia es baja. Por lo tanto, es difícil detectar alejamientos de la hipótesis nula. Si por el contrario, la muestra es grande, la potencia es alta. Entonces, se rechaza la hipótesis nula ante cualquier alejamiento ligero. 
- El incumplimiento de un supuesto puede reflejarse como el incumplimiento de otros. Por ejemplo, la falta de ajuste del modelo puede reflejarse como heterogeneidad de los errores y/o como correlación de los mismos. 
- Hay que tener en cuenta que algunas pruebas de hipótesis suponen cierto alejamiento particular del supuesto que se quiere probar. Por ejemplo, el test de White asume que la varianza es una función cuadrática de las covariables. Por lo tanto si rechazamos esta prueba, no necesariamente podemos asegurar con total certeza que no hay heterocedasticidad. Es posible que la función de varianza tome otra forma. 
- Adicionalmente, varias pruebas son muy sensibles al alejamiento de la suposición de normalidad. Es decir que, si los errores no son normalmente distribuidos, el nivel real de significancia puede ser muy diferente del especificado. Sin embargo, el rechazo de la hipótesis nula podría sugerir que al menos uno de los dos supuestos no se cumple.
- A la hora de validar el supuesto de normalidad de los errores se está interesado en saber si el alejamiento de ese modelo normal, es aceptable desde el punto de vista de la conservación de las propiedades y ventajas que se heredan de la normalidad. Las estimaciones de $\hatbbeta$ son generalmente robustas a desviaciones de la normalidad (Teorema del límite central).

<!--chapter:end:03-EvaluacionSupuestos.Rmd-->

# Transformaciones y mínimos cuadrados ordinarios
\rule{\textwidth}{0.4pt}

## Ejemplo 1. Datos de la ONU {-}
La base de datos `UN11` de la librería `alr4` contiene las siguientes estadísticas de varios miembros de las naciones unidas (y otras regiones independientes) durante los años 2009-2011:

- **fertility**: Número esperado de nacidos vivos por mujer.
- **ppgdp**: producto nacional bruto per cápita (PNB, en dólares).
- **Purban**: el porcentaje de la población que vive en un área urbana.
- **lifeExpF**: esperanza de vida femenina (años).

El objetivo del estudio es ver la relación entre la fertilidad con las
otras variables. Por ahora, empecemos con un modelo de la fertilidad en función del producto nacional bruto y el porcentaje de población urbana.

La Figura \@ref(fig:UNdataFig) muestra la relación entre las variables. Aquí vemos que ambas covariables tienen una relación negativa con la fertilidad. Note, además, que la relación con el producto nacional bruto no es lineal. Esto último podría traer problemas a la hora de ajustar un modelo lineal.

```{r UNdataFig, echo=T, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de la ONU. Relación entre las variables.",warning=FALSE,message = FALSE}
library(alr4)
data("UN11")
pairs(UN11[,-c(1:2,5)])
```
Por ahora consideremos solamente el PNB y el \% de población en área urbana como covariables. Por lo tanto, el modelo propuesto es el siguiente:
\[
\mbox{fertility}_{i} = \beta_{0} + \beta_{1}\mbox{ppgdp}_{i} + \beta_{2}\mbox{pctUrban}_{i} + \varepsilon_{i},
\]
donde $\varepsilon_{i}\sim N(0,\sigma^{2})$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$.

Luego de ajustar el modelo se procede a hacer un análisis de residuos. El gráfico de los residuos estudentizados (Figura \@ref(fig:Un11res) muestra que el ajuste presenta problemas de no linealidad y heterocedasticidad. En la Figura \@ref(fig:Un11resPartial) de los residuos parciales podemos observar que estos problemas se deben a la covariable PNB. 

```{r Un11res, echo=T, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Datos de la ONU. Gráfico de los residuos estudentizados.",warning=FALSE,message = FALSE}
mod.UN11 = lm(fertility~ppgdp+pctUrban,data=UN11)
library(MASS)
res.UN11 = studres(mod.UN11)
plot(mod.UN11$fitted.values,res.UN11,
     xlab='valores ajustados',ylab='residuos estudentizados')
lines(lowess(res.UN11~mod.UN11$fitted.values),col=2)
abline(h=0,lty=2)
```

```{r Un11resPartial, echo=T, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de la ONU. Gráfico de los residuos  parciales.",warning=FALSE,message = FALSE}
library(car)
crPlots(mod.UN11,main='')
```

## Ejemplo 2. Datos de educación {-}
La base de datos `education` de la librería `robustbase` contiene información sobre gastos en educación de 50 estados de los EEUU en el año 1975. Las variables observadas son:

- **Y**: gasto per cápita en educación pública (dólares, proyectado para 1975).
- **X1**: número de residentes en áreas urbanas en 1970 (en miles).
- **X2**: ingreso per cápita en 1973 (en miles dolares).
- **X3**: número de residentes menores de 18 años en 1974 (en miles)

La relación entre las variables se observa en la Figura \@ref(fig:Educdata). Se observa una relación positiva aproximadamente lineal entre la variable respuesta y las covariables, aunque no es tan fuerte con la covariable número de residentes menores de 18 años. Además, vemos que hay por lo menos un posible valor atípico.

```{r Educdata, echo=T, fig.height = 5, fig.width = 7,fig.align = "center",fig.cap = "Datos de educación. Relación entre las variables.",warning=FALSE,message = FALSE}
library(robustbase)
data("education")
education$X2 = education$X2/1000 # cambio de unidad de medida (miles de dolares)
pairs(education[,c(6,3:5)])
```

El objetivo es ajustar un modelo de regresión para el gasto per cápita en educación pública en función de las demás variables:
\[
Y_{i} = \beta_{0} + \beta_{1}\mbox{X1}_{i} + \beta_{2}\mbox{X2}_{i}+ \beta_{3}\mbox{X3}_{i} + \varepsilon_{i},
\]
donde $\varepsilon_{i}\sim N(0,\sigma^{2})$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$.

Antes de hacer inferencias sobre el modelo hacemos un análisis de los residuos. La Figura \@ref(fig:Educres) exhibe el gráfico de los residuos estudentizados. Aunque la relación entre la variable respuesta y covariables es aproximadamente lineal, hay presencia de heterocedasticidad. La variabilidad de los residuos aumenta con los valores ajustados.

```{r Educres, echo=T, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de educación. Gráficos de los residuos estudentizados.",warning=FALSE,message = FALSE}
mod.educ = lm(Y~X1+X2+X3,data=education)
library(MASS)
res.educ = studres(mod.educ)
par(mfrow=c(1,2))
plot(mod.educ$fitted.values,res.educ,
     xlab='valores ajustados',ylab='residuos estudentizados')
lines(lowess(res.educ~mod.educ$fitted.values),col=2)
abline(h=0,lty=2)
plot(mod.educ$fitted.values,abs(res.educ),
     xlab='valores ajustados',ylab='| residuos estudentizados |')
lines(lowess(abs(res.educ)~mod.educ$fitted.values),col=2)
```

\rule{\textwidth}{0.4pt}

Dado que los modelos ajustados presentan desviaciones considerables de los supuestos asumidos, las inferencias que se hagan pueden ser invalidas. Por lo tanto, en este capítulo presentaremos dos herramientas para la corrección de estos problemas: (1) transformación de variables (incluyendo la transformación de Box-Cox) y (2) mínimos cuadrados ponderados.

## Transformación de los datos
Los objetivos de realizar transformaciones sobre los datos son:

- linealizar la relación de las variables,
- estabilizar la varianza,
- y corregir la normalidad.

Las transformaciones pueden hacerse sobre la variable respuesta, las covariables, o ambas. 

La desventaja de hacer transformaciones es que la interpretación del modelo estimado, así como las inferencias, se hacen sobre las variables transformadas, y no en su escala original.

### Transformaciones para linealizar el modelo
Recordemos que el modelo lineal asume que la relación entre la media y las covariables es aproximadamente lineal. En algunos casos, dada la naturaleza de los datos, este supuesto puede ser violado. Por lo tanto, para seguir utilizando la metodología de los modelos lineales, es posible linealizar funciones no-lineales por medio de transformaciones.

Algunas de estas funciones linealizables y su representación gráfica, se muestran en la Tabla \@ref(tab:funciones) y Figura \@ref(fig:linealPat), respectivamente. Por ejemplo, considere que el modelo generador de los datos es:
\[
y_{i} = \beta_{0}\exp\left(\beta_{1}x_{i1} \right)\varepsilon_{i}.
\]
Ver figura \@ref(fig:linealPat)(b). Esta relación no lineal se puede linealizar aplicando una transformación logaritmica a ambos lados:

\[
\log y_{i} = y_{i}^{*} = \log \left[ \beta_{0}\exp\left(\beta_{1}x_{i} \right)\varepsilon_{i} \right] = \log \beta_{0} + \beta_{1}x_{i} + \log\varepsilon_{i}.
\]
Note que estaríamos asumiendo que $\log\varepsilon_{i}$ está normalmente distribuido. Para que esto sea cierto, $\varepsilon_{i}$ debe seguir una distribución log-normal.


```{r funciones, echo=F, results='asis'}
tb1 = data.frame(x1=c("(a)","(b)","(c)",'(d)'),
           x2= c("$y = \\beta_{0}x^{\\beta_{1}}$","$y = \\beta_{0}e^{\\beta_{1}x}$","$y = \\beta_{0}+\\beta_{1}ln(x)$",'$y = \\frac{x}{\\beta_{0}x+\\beta_{1}}$'),
           x3=c("$y^{*}= \\log(y), x^{*} = \\log(x)$","$y^{*}= \\log(y)$","$x^{*}= \\log(x)$",'$x^{*}= \\frac{1}{x}, y^{*}= \\frac{1}{y}$'),
           x4=c('$y^{*} =  \\log(\\beta_{0}) + \\beta_{1} x^{*}$','$y^{*} = \\log(\\beta_{0}) + \\beta_{1} x$', '$y^{*} = \\beta_{0} + \\beta_{1} x^{*}$','$y^{*} = \\beta_{0} - \\beta_{1} x^{*}$') )
colnames(tb1) = c('','Función','Transformación','Forma lineal' )
knitr::kable(tb1, escape = FALSE,caption ='Funciones linealizables', booktabs = TRUE)
```

```{r linealPat, echo=FALSE,fig.height = 5, fig.width = 7,fig.align = "center",fig.cap = "Diferentes patrones linealizables.",warning=FALSE,message = FALSE}
par(mfrow=c(2,2))
## Figura 1
x1=seq(from=0,to=1.5,length.out = 1000)
y11 = 2*x1^0.5
y12 = 2*x1^2
y13 = 2*x1^-0.1

plot(x1,y11,type='l',ylim=c(0,4),xaxt='n',yaxt='n',main='(a)',
     xlab='covariable = X',ylab = 'variable respuesta = Y',lwd=2)
abline(v=1,lty=2)
abline(h=2,lty=2)
lines(x1,y12,col=2,lwd=2)
lines(x1,y13,col=3,lwd=2)
axis(1,c(0,1))
axis(2,0,las=2)
axis(2,2,label=expression(beta[0]),las=2)

### Figura 2
x2=seq(from=-5,to=5,length.out = 1000)
y21 = 0.5*exp(x2*-1)
y22 = 0.5*exp(x2*1)
plot(x2,y21,type='l',xaxt='n',yaxt='n',main='(b)',
     xlab='covariable = X',ylab = 'variable respuesta = Y',lwd=2)
lines(x2,y22,col=2,lwd=2)
axis(1,0)
axis(2,0,las=2)

### Figura 3
x3=seq(from=0,to=1,length.out = 1000)
y31 = 0.5 + log(x3)
y32 = -2 + log(x3)*-1
plot(x3,y31,type='l',ylim=c(-6,4.5),xaxt='n',yaxt='n',main='(c)',
     xlab='covariable = X',ylab = 'variable respuesta = Y',lwd=2)
abline(h=0,lty=2)
abline(v=0,lty=2)
lines(x3,y32,col=2,lwd=2)
axis(1,0)
axis(2,0)

### Figura 4
x4=seq(from=0.4,to=2,length.out = 1000)
y41 = x4/(0.5*x4-0.2)
y42 = x4/(-0.5*x4+1)
plot(x4,y41,type='l',lwd=2,ylim=c(0,100),xaxt='n',yaxt='n',main='(d)',
     xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(h=2,lty=2)
abline(v=0.4,lty=2)
axis(1,0.4,label=expression(beta[1]/beta[0]),las=1)
axis(2,2,label=expression(1/beta[0]),las=1)
```

### Transformaciones para estabilizar la varianza
Un caso frecuente es que la variable respuesta  sigue una distribución de probabilidad en la que la varianza se relaciona en forma funcional con la media:
\[
V(Y|X=x) = \sigma^2 g[E(Y|X=x)].
\]
Por ejemplo, en la distribución Poisson, la varianza es igual a la media. Algunas transformaciones comunes para estabilizar varianza se muestran en la Tabla \@ref(tab:funcionesVar) [@behar_validacion_2002].

```{r funcionesVar, echo=F, results='asis'}
tb1 = data.frame(
  x1 = c('$\\sigma^{2} \\propto C$','$\\sigma^{2} \\propto E(y)$','$\\sigma^{2} \\propto E(y)[1-E(y)]$',
         '$\\sigma^{2} \\propto E(y)^{2}$','$\\sigma^{2} \\propto E(y)^{3}$',
         '$\\sigma^{2} \\propto E(y)^{4}$'),
  x2 = c('$y^{*}=y$', '$y^{*}=\\sqrt{y}$','$y^{*}=\\sin^{-1}\\sqrt{y} (0 \\leq y_{i} \\leq 1)$',
         '$y^{*}=\\log y$  o también  $y^{*}=\\log (y+1)$ (si $y \\geq 0$)',
         '$y^{*}=\\frac{1}{\\sqrt{y}}$','$y^{*}=\\frac{1}{y}$'))

colnames(tb1) = c('Relación entre $\\sigma^{2}$ y $E(Y)$','Transformación')
knitr::kable(tb1, escape = FALSE,caption ='Algunas transformaciones para estabilizar la varianza', booktabs = TRUE)
```

Algunas consideraciones cuando se hacen transformaciones sobre las variables:

- Transformaciones pueden ser sugeridas por experiencia (o teoría).
En otros casos, la selección se hace empíricamente.
- Luego de realizar las transformaciones se debe verificar si el modelo transformado cumple los supuestos. 
- El estimador de MCO tiene propiedades de mínimos cuadrados con
respecto a los datos transformados. 
- Las predicciones son sobre las respuestas transformadas, no las
originales. Devolverse a la variable respuesta original no es fácil. Recordemos que 
\[
E[g(y)] \neq g[E(y)].
\]
Al aplicar la transformación inversa a las predicciones de la respuesta transformada estamos estimando la mediana, y no la media. Por otro lado, a las estimaciones por intervalos de confianza si se les puede aplicar la transformación inversa. Esto porque los percentiles no se ven afectados por transformaciones.

\rule{\textwidth}{0.4pt}
### Datos de la ONU. Transformación para linealizar los datos {-}
Al realizar un análisis de residuos del ajuste del modelo para los datos de la ONU, vimos que hay una relación no-lineal entre la fertilidad y las dos covariables propuestas. Particularmente, esto se debe a la covariable producto nacional bruto. 

Por lo tanto, podemos aplicar una transformación logarítmica tanto a la variable respuesta, así como la covariable producto nacional bruto. En la Figura \@ref(fig:UNtransFig) vemos como al aplicar esta transformación se linealiza la relación. 

```{r UNtransFig, echo=FALSE, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de la ONU. Relación entre las variables.",warning=FALSE,message = FALSE}
par(mfrow=c(1,2))
plot(UN11$ppgdp,UN11$fertility,xlab='PNB per cápita (dólares)',ylab='# esperado de nacidos vivos por mujer',
     cex.lab=0.8)
lines(lowess(UN11$fertility~UN11$ppgdp),col=2)
plot(log(UN11$ppgdp),log(UN11$fertility),xlab='log PNB per cápita (dólares)',ylab='log # esperado de nacidos vivos por mujer',
     cex.lab=0.8)
lines(lowess(log(UN11$fertility)~log(UN11$ppgdp)),col=2)
```
Producto de esta transformación, se propone el siguiente modelo:
\[
\log \mbox{fertility}_{i} = \beta_{0} + \beta_{1}\log \mbox{ppgdp}_{i} + \beta_{2}\mbox{pctUrban}_{i} + \varepsilon_{i},
\]
donde $\varepsilon_{i}\sim N(0,\sigma^{2})$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$.

La Figura \@ref(fig:UNtrans2Fig) muestra el gráfico de los residuos para este ajuste. Aquí vemos que los problemas de no linealidad y heterocedasticidad se corrigieron al realizar la transformación logarítmica.
```{r UNtrans2Fig, echo=TRUE, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de la ONU. Gráficos de los residuos para el modelo transformado.",warning=FALSE,message = FALSE}
mod.UN11.trans = lm(log(fertility)~log(ppgdp)+pctUrban,data = UN11)
res.UN11.trans = studres(mod.UN11.trans)
par(mfrow=c(1,2))
plot(mod.UN11.trans$fitted.values,res.UN11.trans,xlab='valores ajustados',
     ylab='residuos estudentizados')
lines(lowess(res.UN11.trans~mod.UN11.trans$fitted.values),col=2)
abline(h=0,lty=2)
plot(mod.UN11.trans$fitted.values,abs(res.UN11.trans),xlab='valores ajustados',
     ylab='| residuos estudentizados |')
lines(lowess(abs(res.UN11.trans)~mod.UN11.trans$fitted.values),col=2)
```
\rule{\textwidth}{0.4pt}

## Método de Box-Cox
Para la corrección del supuesto de normalidad y varianza constante, es posible implementar transformaciones en potencia para la variable respuesta. Esto es, $y^{*}=y^{\lambda}$. Dado que el valor de $\lambda$ es desconocido, la idea del **método de Box-Cox** es estimar el modelo lineal para diferentes valores de $\lambda$ y determinar el valor que proporciona el mejor ajuste. Sin embargo, aquí encontramos dos problemas.

Primero, la transformación en potencia tiene un problema de discontinuidad en $\lambda=0$. Puesto que cuando $\lambda$ tiende a cero, $y^{*}$ se acerca a $1$. Para resolver esto, se puede utilizar $y^{*} = (y^{\lambda}-1)/\lambda$. De esta forma, cuando $\lambda$ tiende a cero, $y^{*}$ se acerca a $\log y$. Segundo, cuando $\lambda$ cambia, los valores $y^{*}$ varía drásticamente. Esto hace que los modelos ajustados no se puedan comparar fácilmente. 

La transformación que permite que los modelos ajustados sean comparables es:
\begin{equation}
y^{(\lambda)} = \begin{cases}
\frac{y^{\lambda}-1}{\lambda \dot{y}^{\lambda-1}}, & \mbox{si }\lambda \neq 0, \\
\dot{y}\log y & \mbox{si } \lambda=0,
\end{cases}
(\#eq:boxcoxtrans)
\end{equation}
donde $\dot{y} = \log \left[1/n \sum_{i=1}^{n}\log y_{i}\right]$ es la media geométrica de la variable respuesta. 

Entonces, el método de Box-Cox es el siguiente:

1. Determinar una secuencia de valores para $\lambda$, $(\lambda_{1},\lambda_{2},\ldots,\lambda_{K})$. Por lo general, se seleccionan valores en el intervalo $[-2,2]$. 
2. Ajustar el modelo:
\[
y_{ij}^{(\lambda_k)} = \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \ldots + \beta_{p-1}x_{i,p-1} + \varepsilon_{i},
\]
para cada valor de $\lambda_{k}$, $k=1,\ldots,K$. Al utilizar la transformación \@ref(eq:boxcoxtrans), las sumas de cuadrados para modelos con diferentes valores de $\lambda$ son comparables.
3. Seleccionar el modelo que minimiza la suma de cuadrados de los residuos, $SS_{res}(\lambda)$. Equivalentemente, el $\lambda$ que maximiza la verosimilitud.
4. Luego de encontrar el valor de $\lambda$ óptimo, se ajusta el modelo transformando la variable respuesta $y^{\lambda}$, si $\lambda\neq 0$, o $\log y$ si $\lambda =0$. Es decir que (\@ref{eq:boxcoxtrans}) se utiliza solo en el paso de comparación de modelos ajustados.

Puesto que $\lambda$ es una variable aleatoria, también se puede hacer una estimación por intervalos de confianza. Para esto primero consideremos la función de log-verosimilitud:
\[
\ell(\bbeta,\sigma^{2} | \lambda) = -\frac{2}{n}\log \left( 2\pi\sigma^{2}\right) - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left( y_{i}^{\lambda} - \bx_{i}'\bbeta\right)^{2} = -\frac{2}{n}\log \left( 2\pi\sigma^{2}\right) - \frac{1}{2\sigma^{2}}SS_{res}(\lambda).
\]
Aquí vemos que el valor de $\lambda$ que maximiza la verosimilitud es el mismo que minimiza la suma de cuadrados de los residuos.

Un intervalo de confianza se puede construir a partir del siguiente estadístico de prueba:
\[
G_{0}^{2} = -2\left[ \ell(\bbeta,\sigma^{2} | \lambda=1) - \ell(\bbeta,\sigma^{2} | \lambda=\hatlambda) \right].
\]
Si $\lambda=1$, entonces asintóticamente $G_{0}^{2} \sim \chi^{2}_{1}$. Por lo tanto, el intervalo del $(1-\alpha)\times 100$\% de confianza para $\lambda$ está definido por los valores de $\lambda$ que cumplen con la condición:
\[
 \ell(\bbeta,\sigma^{2} | \lambda) \geq  \ell(\bbeta,\sigma^{2} | \lambda=\hatlambda) - \frac{1}{2}\chi^{2}_{1,1-\alpha}.
\]

\rule{\textwidth}{0.4pt}
### Datos de educación. Transformación de Box-Cox
Dado que el análisis de residuso para el ajuste del modelo para los datos de educación mostró que hay problemas de heterocedasticidad, vamos a encontrar una transformación que resuelva de problema usando el método de Box-Cox. Para esto usamos la función `boxcox` de la librería `MASS`:
```{r educLambda, echo=TRUE, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "\\label{fig:EducBC} Datos de educación. Perfiles de verosimilitud para $\\lambda$.",warning=FALSE,message = FALSE}
boxcox.educ = MASS::boxcox(mod.educ,lambda=seq(-3,3,length.out = 1000),
                               ylab='log-verosimilitud')
boxcox.educ$x[boxcox.educ$y ==max(boxcox.educ$y)] # valor que maximiza la log-verosimilitud
```
Estos resultados nos indican que $\hatlambda = `r round(boxcox.educ$x[boxcox.educ$y ==max(boxcox.educ$y)],3)`$. Por lo cuál, podemos utilizar una transformación inversa $(\lambda=-1)$. Entonces, el modelo propuesto es:
\[
1/y_{i} = \beta_{0} + \beta_{1}\mbox{X1}_{i}+ \beta_{2}\mbox{X2}_{i} + \beta_{3}\mbox{X3}_{i} + \varepsilon_{i},
\]
donde $\varepsilon_{i}\sim N(0,\sigma^{2})$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$.

Ahora procedemos a hacer el análisis de los residuos del modelo transformado:
```{r EducBC2Fig, echo=TRUE, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de educación. Graficos de los residuos para el modelo transformado.",warning=FALSE,message = FALSE}
mod.educ.trans = lm(1/Y~X1+X2+X3,data=education)
res.educ.trans = studres(mod.educ.trans)
par(mfrow=c(1,2))
plot(mod.educ.trans$fitted.values,res.educ.trans,
     xlab='valores ajustados',ylab='residuos estudentizados')
lines(lowess(res.educ.trans~mod.educ.trans$fitted.values),col=2)
abline(h=0,lty=2)
plot(mod.educ.trans$fitted.values,abs(res.educ.trans),
     xlab='valores ajustados',ylab='| residuos estudentizados |')
lines(lowess(abs(res.educ.trans)~mod.educ.trans$fitted.values),col=2)
```

En la Figura \@ref(fig:EducBC2Fig) vemos que la transformación propuesta corrigió el problema de heterocedasticidad. Adicionalmente, por medio de la Figura \@ref(fig:EducBC3Fig) podemos verificar que el supuesto de normalidad se cumple.

```{r EducBC3Fig, echo=TRUE, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Datos de educación. Graficos de normalidad de los residuos para el modelo transformado.",warning=FALSE,message = FALSE}
car::qqPlot(mod.educ.trans,distribution = 'norm',ylab='residuos estudentizados')
```
Puesto que se cumplen los supuestos del modelo transformado, ahora procedemos a interpretar los resultados.

```{r , echo=TRUE,warning=FALSE,message = FALSE}
summary(mod.educ.trans)
```
A partir de estos resultados podemos concluir que la población en áreas urbanas no tiene un aporte signficativo en el modelo cuando las otras dos covariables ya están incluidas. Mientras que, el ingreso per cápita y la población menor de 18 años tienen un efecto positivo significativo sobre el gasto en educación pública (recordemos que se hizo una transformación inversa). 

Puesto que hicimos transformaciones, la interpretación de los coeficientes se hace sobre la variable respuesta transformada.

Ahora, supogamos que queremos hacer la predicción del gasto medio en educación pública para los estados que tengan una población de $\mbox{X1}=650$, un ingreso per cápita de $\mbox{X2=4.5}$ y una población menor de 18 años de $\mbox{X3}=320$. A partir del modelo ajustado tenemos que:
```{r}
x0.educ = data.frame(X1=650,X2=4.5,X3=320)
pred.educ.trans = predict(mod.educ.trans,x0.educ,interval='confidence') 
1/pred.educ.trans
```
Por lo que intervalo del 95\% de confianza para el gasto medio en educación pública para los estados con las características expresadas anteriormente es (`r round(1/pred.educ.trans[3],2)`, `r round(1/pred.educ.trans[2],2)`).
\rule{\textwidth}{0.4pt}

## Mínimos cuadrados ponderados
El método de mínimos cuadrados ponderados (MCP) es una alternativa para estimar un modelo lineal en presencia de heterocedasticidad. La idea de esta técnica es calcular las desviaciones entre las observaciones $(y_{i})$ y los valores ajustados $(\haty_{i})$ usando pesos $(w_{i})$ inversamente proporcionales a la varianza de $y_{i}$.

Asumamos que el modelo generador de los datos es:
\begin{equation}
\by = \bX\bbeta + \bvarepsi, \mbox{ donde }\bvarepsi\sim N(\bZERO,\sigma^{2}\bV),
(\#eq:modV)
\end{equation}
donde $\bV$ es una matriz diagonal $(n \times n)$:
\[
\bV = \begin{pmatrix}
v_{11} & 0 & 0 & \ldots & 0 \\
0 & v_{22} & 0 & \ldots & 0 \\
0 & 0 & v_{33} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & v_{nn}
\end{pmatrix}.
\]
Esto quiere decir que tenemos presencia de hetorocedasticidad, donde $V(\varepsilon_{i}) = \sigma^{2}v_{ii}$. Recordemos que si aplicamos el estimador por MCO, las estimaciones siguen siendo insesgadas pero son ineficientes. Además, las varianzas estimadas de $\hatbbeta$, y de las predicciones, están mal calculadas. Lo que puede afectar la cobertura de los intervalos de confianza y el nivel de signficancia de las pruebas de hipótesis.

La función de verosimilitud del modelo \@ref(eq:modV), asumiendo que $\bV$ es conocida, es:
\[
L(\bbeta,\sigma^{2} | \bV) =  \frac{1}{(2\pi)^{n/2}|\sigma^{2}\bV|^{1/2}}\exp \left[-\frac{1}{2\sigma^{2}}(\by - \bX'\bbeta)'\bV^{-1}(\by - \bX'\bbeta) \right].
\]
Por lo que para encontrar el estimador de $\bbeta$ debemos minimizar la suma de cuadrados ponderada:
\begin{equation}
SS_{Wres} = (\by - \bX'\bbeta)'\bW(\by - \bX'\bbeta) = \sum_{i=1}^{n} w_{ii}(y_{i} - \bx_{i}'\bbeta)^{2},
(\#eq:SSWE)
\end{equation}
donde $\bW = \bV^{-1}$, es decir que $w_{ii}= \frac{1}{v_{ii}}$. Note que las observaciones con mayor varianza tienen menor peso en la estimación de $\bbeta$.

Al minimizar \@ref(eq:SSWE) se obtiene el estimador por MCP:
\[
\hatbbeta_{W} = (\bX'\bW\bX)^{-1}\bX'\bW\by.
\]
Además, 
\begin{equation}
V(\hatbbeta_{W}) = \sigma^{2}(\bX'\bW\bX)^{-1}.
(\#eq:VarBetaW)
\end{equation}
Si $\bV$ está correctamente especificada, se puede probar que $\hatbbeta_{W}$ es el mejor estimador insesgado de $\bbeta$.

Los residuos del ajuste del modelo son:
\[
e_{Wi} = \sqrt{w_{ii}}(y_{i} - \haty_{i}), \mbox{ de forma matricial }\be_{W} = \bW^{1/2}(\by - \bX\hatbbeta_{W}),
\]
donde $\bW = \bW^{1/2}\bW^{1/2}$. De este resultado obtenemos el estimador insesgado de $\sigma^{2}$:
\[
\hatsigma^{2} = \frac{1}{n-p}\sum_{i=1}^{n}w_{ii}(y_{i}-\bx_{i}'\hatbbeta)^{2} = \frac{1}{n-p}(\by - \bX\hatbbeta)'\bW(\by - \bX\hatbbeta).
\]
  
Aqui estamos asumiendo que los pesos $(w_{ii})$ son conocidos. Lo que en la práctica es poco común. Estos pesos pueden ser determinados por conocimiento de los datos, experiencia, o información teórica del modelo. Alternativamente, estos se pueden calcular a partir de los residuos obtenidos por el estimador MCO.

Es común que $\sigma^{2}_{i}$ varíe de acuerdo a una o varias covariables, o con respecto $E(y_{i}|\bx_{i})$. Por ejemplo, en los datos de educación parece que la varianza incrementa con $E(y_{i}| \bx_{i})$. En este caso podemos aplicar el siguiente procedimiento:

1. Ajustar el modelo por MCO y analizar los residuos.
2. Ajustar un modelo para el valor absoluto de los residuos $(|e_{i}|)$, o los residuso al cuadrado $(e^{2}_{i})$, en función de las covariables:
\[
|e_{i}| = \gamma_{0} + \gamma_{1}x_{i1}+ \ldots + \gamma_{p-1}x_{i,p-1} + \varepsilon_{ei}.
\]
De esta ajuste obtenemos una estimación para la desviación estándar $(\sigma_{i})$. Si ajustamos un modelo para los residuos al cuadrado, estamos estimando la varianza $(\sigma^{2})$.
4. Usar los valores ajustados el modelo anterior para estimar los pesos $(w_{ii})$.

Si $\hatbbeta_{W}$ difiere mucho de $\hatbbeta$, es posible repetir el proceso anterior una vez más (mínimos cuadrados iterativamente ponderados).

Dado que estamos estimando los pesos, la varianza de los coeficientes \@ref(eq:VarBetaW) es aproximada. Sin embargo, esta aproximación es muy buena si el tamaño de muestra no es muy pequeño.

\rule{\textwidth}{0.4pt}
### Datos de educación. Minímos cuadrados ponderados {-}
Retomemos el modelo para los datos de educación. En la Figura \@ref{fig:Educres} vemos que hay heterocedasticidad. Particularmente vemos que la variabilidad crece a medida que aumentan los valores ajustados. Ya vimos que por medio de una transformación inversa sobre la variable respuesta se corrige el problema. 

El problema de hacer transformaciones es que los coeficientes estimados pierden interpretación. Así que, alternativamente, vamos a implementar el método de minímos cuadrados ponderados. Dado que desconocemos los pesos, vamos a estimarlos usando el procedimiento que se explicó anteriormente.

Primero vamos a ajustar el siguiente modelo para el valor absoluto de los residuos:
\[
|e_{i}| = \gamma_{0} + \gamma_{1}\mbox{X1}_{i}+ \gamma_{2}\mbox{X2}_{i} + \gamma_{3}\mbox{X3}_{i} + \varepsilon_{ei}.
\]
```{r, echo=TRUE}
mod.stdev.educ = lm(abs(res.educ)~X1+X2+X3,data = education)
summary(mod.stdev.educ)
```
```{r, echo=F}
stdev.coef = round(mod.stdev.educ$coefficients,3)
```
Por lo tanto, las estimaciones de las desviaciones estándar de los errores son:
\[
\widetilde{\sigma}_{i} = s_{i} = `r stdev.coef[1]`  `r stdev.coef[2]`\mbox{X1}_{i} + `r stdev.coef[3]`\mbox{X2}_{i} + `r stdev.coef[4]`\mbox{X3}_{i}.
\]
De este ajuste obtenemos los pesos $w_{ii} = \frac{1}{s_{i}^{2}}$, y luego, estimamos el modelo por MCP:
```{r, echo=T}
w = 1/mod.stdev.educ$fitted.values^2
mod.educ.mcp = lm(Y~X1+X2+X3,data=education,weights = w)
summary(mod.educ.mcp)
```
La Figura \@ref(fig:EducMCPFig) muestra los gráficos de los residuos ponderados. Aquí podemos observar que los residuos ponderados no muestra heterocedasticidad.


```{r EducMCPFig, echo=TRUE, fig.height = 4, fig.width = 9,fig.align = "center",fig.cap = "Datos de educación. Graficos de los residuos ponderados.",warning=FALSE,message = FALSE}
res.educ.mcp = mod.educ.mcp$residuals*sqrt(w)
par(mfrow=c(1,2))
plot(mod.educ.mcp$fitted.values,res.educ.mcp,
     xlab='valores ajustados',ylab='residuos ponderados')
lines(lowess(res.educ.mcp~mod.educ.mcp$fitted.values),col=2)
abline(h=0,lty=2)
plot(mod.educ.mcp$fitted.values,abs(res.educ.mcp),
     xlab='valores ajustados',ylab='| residuos ponderados |')
lines(lowess(abs(res.educ.mcp)~mod.educ.mcp$fitted.values),col=2)
```
El ajuste por mínimos cuadrados ponderados es:
```{r}
summary(mod.educ.mcp)
```
Aquí llegamos a conclusiones similares a las obtenidas por medio del modelo transformado. El efecto de la población en áreas urbanas no es significativo. Las otras dos covariables si aportan significativamente al modelo. En este caso, dado que no aplicamos transformaciones, los coeficientes si tienen interpretación. Por ejemplo de la estimación de $\beta_{3}$ podemos concluir que: el gasto per cápita medio en educación pública aumenta en 1.117 USD por cada 1000 personas menores de 18 años.

Ahora, hagamos la predicción del gasto medio en educación pública para las características: $\mbox{X1}=650$, $\mbox{X2=4.5}$ y $\mbox{X3}=320$:
```{r}
predict(mod.educ.mcp,x0.educ,interval='confidence')
```

```{r,echo=FALSE}
pred.mcp = predict(mod.educ.mcp,x0.educ,interval='confidence')

```
Por medio del estimador de MCP obtenemos el siguiente intervalo del 95\% de confianza: (`r round(pred.mcp[2],3)`, `r round(pred.mcp[3],3)`). Este intervalo es parecido al calculado usando el modelo transformado, aunque tiene una longitud un poco más pequeña.

<!--chapter:end:04-Transformaciones.Rmd-->

# Evaluación de puntos influyentes y atípicos
```{r, include=FALSE}
library(MASS)
library(alr4)
data(UN11)
n= nrow(UN11)
Names = rownames(UN11)
mod.UN11 = lm(log(fertility)~log(ppgdp)+pctUrban,data=UN11)
p=3
res.UN11 = studres(mod.UN11)
Order.res = order(abs(res.UN11),decreasing = T)
```

\rule{\textwidth}{0.4pt}
## Datos de la ONU
Retomemos los datos de la ONU (`UN11` de la librería `alr4`). El modelo propuesto es:
\begin{equation}
\log \mbox{fertility}_{i} = \beta_{0} + \log \mbox{ppgdp}_{i}\beta_{1} + \mbox{pctUrban}_{i}\beta_{2} + \varepsilon_{i},
(\#eq:modUN)
\end{equation}
donde $\varepsilon_{i}\sim N(0,\sigma^{2})$ y $cov(\varepsilon_{j},\varepsilon_{k})=0$.

El análisis de los residuos (ver Figura \@ref(fig:Un11resFig)) muestra que el modelo está bien especificado y no hay problemas de heterocedasticidad. Sin embargo, podemos observar que algunos residuos presentan valores muy altos. La estimación de la $\log \mbox{fertility}$ para Guinea Ecuatorial es considerablemente baja en comparación con el valor observado. Caso contrario pasa con Corea del Norte, Bosnia y Herzegovina, y Moldavia.

```{r Un11resFig, echo=F, fig.height = 4, fig.width = 6,fig.align = "center",fig.cap = "Datos de la ONU. Gráfico de los residuos estudentizados.",warning=FALSE,message = FALSE}
plot(mod.UN11$fitted.values,res.UN11,
     xlab='valores ajustados',ylab='residuos estudentizados')
lines(lowess(res.UN11~mod.UN11$fitted.values),col=2)
abline(h=0,lty=2)
abline(h= c(-1,1)*qt(0.975,n-p-1),lty=2)
text(mod.UN11$fitted.values[Order.res[1]],res.UN11[Order.res[[1]]],'Guinea Ecuatorial',pos=4,cex=0.8)
text(mod.UN11$fitted.values[Order.res[2]],res.UN11[Order.res[[2]]],'Bosnia y Herz.',pos=2,cex=0.8)
text(mod.UN11$fitted.values[Order.res[3]],res.UN11[Order.res[[3]]],'Moldavia',pos=4,cex=0.8)
text(mod.UN11$fitted.values[Order.res[4]],res.UN11[Order.res[[4]]],'Corea del Norte',pos=4,cex=0.8)

```
\rule{\textwidth}{0.4pt}

## Importancia de detectar valores influyentes y atípicos
En el análisis de datos pueden observarse algunos valores atípicos. Como atípicas nos referimos a las observaciones que no siguen el patrón de la mayoría de los datos. En un análisis de regresión, pueden presentarse valores atípicos sobre la variable respuesta y/o sobre algunas covariables. Por lo tanto, se podría identificar diferentes tipos de puntos ``atípicos''. Estos se pueden identificar en la Figura \@ref(fig:puntosAI) para el caso de una regresión simple:

- **$A$ es un punto atípico:** valor que no se ajusta bien en $Y$, pero regular en $X$.
- **$B$ es un punto de balanceo:** observación que se ajusta bien en $Y$, pero es inusual en $X$.
- **$C$ es un punto de influyente:** medición que no se ajusta bien en $Y$ y es inusual en $X$.

Estos puntos inusuales pueden ser problemáticos a la hora de ajustar un modelo lineal por MCO, ya que pueden tener mucha influencia en los resultados, y su presencia puede ser una señal de que el modelo no captura caraterísticas importantes de los datos.

```{r puntosAI, echo=F, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Diferentes tipos de datos. A: Punto atípico, B: punto de balanceo, C: punto influyente.",warning=FALSE,message = FALSE}
set.seed(2)
x1 =rnorm(20,0,0.4)
y1 = 5+2*x1+rnorm(20)
x = c(x1,3,2.5,0)
y = c(y1,5+2*3,5+2*2.5-10,5+10)
xa=x[c(1:20,23)]
ya=y[c(1:20,23)]
moda0=lm(ya[-21]~xa[-21])
moda=lm(ya~xa)
xb=x[c(1:20,21)]
yb=y[c(1:20,21)]
modb0=lm(yb[-21]~xb[-21])
modb=lm(yb~xb)
xc=x[c(1:20,22)]
yc=y[c(1:20,22)]
modc0=lm(yc[-21]~xc[-21])
modc=lm(yc~xc)
plot(x,y,xaxt='n',yaxt='n', xlab='covariable = X',ylab = 'variable respuesta = Y')
text(x[21:23],y[21:23],c('B','C','A'),pos=2)
```
En la Figura \@ref(fig:puntosAIw)(a) vemos que un punto atípico no tiene un efecto grande en la estimación de la recta de regresión. Sin embargo, dado que los puntos atípicos generan valores altos (en valor absoluto) para los residuos, estas observaciones inflan la varianza de las estimaciones y afectan las inferencias. En este caso, la estimación de la varianza con todos los datos es de $\hatsigma^{2}=`r round(sum(moda$residuals^2)/(21-2),2)`$, y si se omite el dato $A$, tenemos que $\hatsigma^{2}=`r round(sum(moda0$residuals^2)/(19-2),2)`$. Como vemos en la Figura \@ref{fig:puntosAIw}(b) los puntos de balanceo tampoco tienen mucha influencia sobre las estimaciones por MCO ya que estos están en línea con el resto de los datos.

Por el otro lado, en la Figura \@ref(fig:puntosAIw)(c) vemos que los puntos influyentes afectan notablemente las estimaciones por MCO. Además, también inflan considerablemente la variabilidad. Con todos los datos tenemos que $\hatsigma^{2}=`r round(sum(modc$residuals^2)/(21-2),2)`$. Mientras que $\hatsigma^{2}=`r round(sum(modc0$residuals^2)/(19-2),2)`$ al eliminar la observación C.

```{r puntosAIw, echo=F, fig.height = 4, fig.width = 9,fig.align = "center",fig.cap = "Efecto de los puntos inusuales. En cada gráfico, la línea continua es la estimación por MCO con todos los datos, mientras que la línea discontinua es la estimación por MCO omitiendo el punto inusual (circulo rojo). Izquierda: efecto de un punto atípico. Centro: efecto de un punto de balanceo. Derecha: efecto de un punto influyente.",warning=FALSE,message = FALSE}
par(mfrow=c(1,3))
plot(xa,ya,xaxt='n',yaxt='n', ylim=range(y),xlim=range(x),main='(a)',
     xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(moda0,lty=2)
abline(moda)
points(xa[21],ya[21],col=2,pch=19)
text(xa[21],ya[21],c('A'),pos=2)

plot(xb,yb,xaxt='n',yaxt='n', ylim=range(y),xlim=range(x),main='(b)',
     xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(modb0,lty=2)
abline(modb)
points(xb[21],yb[21],col=2,pch=19)
text(xb[21],yb[21],c('B'),pos=3)

plot(xc,yc,xaxt='n',yaxt='n', ylim=range(y),xlim=range(x),main='(c)',
     xlab='covariable = X',ylab = 'variable respuesta = Y')
abline(modc0,lty=2)
abline(modc)
points(xc[21],yc[21],col=2,pch=19)
text(xc[21],yc[21],c('C'),pos=3)
```

## Valores atípicos
Para identificar valores atípicos podemos hacer uso de los residuos del ajuste. Recordemos que, aunque los errores tenga varianza constante y sean incorrelacionados, los residuos no cumplen con estas propiedades. Por lo tanto, es recomendado usar los residuos estudentizados (o los residuos R-Student):
\[
r_{i} = \frac{e_{i}}{\sqrt{\hatsigma^{2}(1-h_{ii})}}, i=1,\ldots,n,
\]
o los residuos R-Student:
\[
t_{i} = \frac{e_{i}}{\sqrt{\hatsigma^{2}_{(i)}(1-h_{ii})}} = r_{i} \sqrt{\frac{n-p-1}{n-p-r_{i}^{2}}},
\]
donde $\hatsigma^{2}_{(i)}$ es la estimación de $\sigma$ usando todas las observaciones excepto la $i$-ésima.

Si se cumplen los supuestos del modelo, se puede demostrar que $r_{i}\sim t_{n-p-1}$. Los residuos estudentizados siguen también está distribución pero de forma aproximada. Por lo tanto, se pueden identificar posibles valores atípicos haciendo un gráfico de los R-Student (o residuos estudentizados) contra los valores ajustados y trazar líneas de referencia en los percentiles $0.025$ y $0.975$ de la distribución $t$ con $n-p-1$ grados de libertad. 

Esta verificación no es estrictamente una prueba de hipótesis. Puesto que estamos haciendo múltiplescomparaciones de los residuos R-Student con los valores criticos de la distribución $t$. Por lo que es necesario hacer una corrección utilizando el método de Bonferroni.

\rule{\textwidth}{0.4pt}
### Datos de la ONU - valores atípicos {-}
En la Figura \@ref(fig:Un11res) podemos observar que hay varios residuos que sobrepasan los puntos de corte. Particularmente, la observación de Guinea Ecuatorial presenta un residuo muy alto.
\rule{\textwidth}{0.4pt}

## Puntos de balanceo 
Recordemos que la estimación de la recta de regresión es un promedio ponderado de las observaciones:
\[
\hatby = \bX\hatbbeta = \bX(\bX'\bX)^{-1}\bX'\by = \bH\by.
\]
Para la observación $y_{i}$ tenemos que $\haty_{i} = h_{ii}y_{i} + \sum_{k\neq i}h_{ik}y_{j}$. Particularmente, el elemento $h_{ij}$ pueden ser visto como la cantidad de balanceo o palanqueo ejercido por la j-ésima observación $(y_i)$ sobre el $i$-ésimo valor ajustado ($\haty_{i}$).

Entonces, para detectar valores influyentes vamos a centrarnos en la matriz \textit{hat} $(\bH)$.  El elemento $h_{ij}$ de esta matriz se calcula como:
\begin{equation}
h_{ij} = \bx_{i}'(\bX'\bX)\bx_{j}.
(\#eq:hij)
\end{equation}
Algunas propiedades de la matriz $\bH$ son:
\begin{itemize}
\item $\sum_{i=1}^{n} = p$.
\item $\sum_{i=1}^{n}h_{ij} = \sum_{j=1}^{n}h_{ij} = 1$
\item Cada valor $h_{ii}$ está acotado entre $1/n$ y $1/r$ ($r$ es el número de columnas de $\bX$ iguales a $\bx_{i}$).
\end{itemize}
Además, la diagonal de la matriz $\bH$ es una medida estandarizada de la distancia de las observaciones al centro (centroide) del espacio de $\bx$. Por lo tanto, valores altos en la diagonal de $\bH$ pueden indicarnos observaciones que son potencialmente influyentes porque están alejadas en el espacio de las covariables. Viendo \@ref(eq:hij), si $h_{ii}$ es muy cercano a uno, $\haty_{i}$ estará muy cerca de $y_{i}$ (dado que el peso de las demás observaciones será casi cero).

Dado que $\bar{h} = p/n$, observaciones con $h_{ii}$ superiores a $2p/n$ son considerados **puntos de balanceo** (y posibles puntos influyentes). Note que, si $2p/n > 1$, el punto de corte no aplica.

\rule{\textwidth}{0.4pt}
### Datos de la ONU - diagonal de la matrix hat {-}
La Figura \@ref(fig:Un11Hdiag)(a) muestra los valores de la diagonal de la matrix $\bH$. Aquí podemos observar que algunos países presentan valores más altos del punto de corte ($2\frac{p}{n} = `r round(2*3/199,4)`$). Comparado con los demás valores, el valor asociado a Trinidad y Tobago es muy alto. Por lo cuál este país lo podemos considerar como un punto de balanceo.

```{r Un11Hdiag, echo=T, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de la ONU. (a) Valores de la diagonal de lam matriz hat. (b) Diagrama de dispersión de las covariables (derecha). El punto rojo indica a Trinidad y Tobago.",warning=FALSE,message = FALSE}
library(alr4)
data(UN11)
Names = rownames(UN11)
mod.UN11 = lm(log(fertility)~log(ppgdp)+pctUrban,data=UN11)
hii = hatvalues(mod.UN11)
par(mfrow=c(1,2))
plot(hii,type='h',xlab='índice',ylab='valores de la diagonal de la matrix hat')
abline(h=2*3/199,lty=2)
hii[hii>2*3/199]
plot(log(UN11$ppgdp),UN11$pctUrban,xlab='log del PNB per cápita',
     ylab='% de población urbanas')
points(log(UN11$ppgdp)[Names=='Trinidad and Tobago'],
       UN11$pctUrban[Names=='Trinidad and Tobago'],col=2,pch=19)
```
En laFigura \@ref(fig:Un11Hdiag)(b) podemos observar que Trinidad y Tobago tiene un porcentaje muy pequeño de población en áreas urbanas y valor del PNB per cápita relativamente alto. Por esta razón el valor $h_{ii}$ asociado es alto.

La Figura \@ref(fig:Un11valAtip) muestra los valores de la diagonal de $\bH$ contra los residuos estudentizados. Aquí vemos que aunque Trinidad y Tobago tiene un valor $h_{ii}$ alto, el R-Student asociado es bajo. Por lo que no se puede considerar como un punto influyente. Por el contrario, Guinea Ecuatorial y Corea del Norte presentan ambos valores altos (residuos y $h_{ii}$), por lo que se pueden considerar como puntos influyentes. Las observaciones que tienen residuos altos pero valores $h_{ii}$ bajos se consideran como atípicos.

```{r Un11valAtip, echo=T, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Datos de la ONU. Gráfico de valores influyentes",warning=FALSE,message = FALSE}
rstud.UN11 = res.UN11*sqrt( (199-3-1)/(199-3-res.UN11^2) )
plot(hii,rstud.UN11,ylab='r Student',
     xlab='valores de la diagonal de la matrix hat')
abline(h=0,lty=2)
abline(h= c(-1,1)*qt(0.975,199-3-1),lty=2)
abline(v=2*3/199,lty=2)
text(hii[Names=="Trinidad and Tobago"],
     rstud.UN11[Names=='Trinidad and Tobago'],'Trinidad y Tobago',pos=2,cex=0.8)
text(hii[Names=="Equatorial Guinea"],
     rstud.UN11[Names=='Equatorial Guinea'],'Guinea Ecuatorial',pos=4,cex=0.8)
text(hii[Names=="North Korea"],
     rstud.UN11[Names=='North Korea'],'Corea del Norte',pos=4,cex=0.8)
text(hii[Names=="Somalia"],
     rstud.UN11[Names=='Somalia'],'Somalia',pos=4,cex=0.8)
text(hii[Names=="Bosnia and Herzegovina"],
     rstud.UN11[Names=='Bosnia and Herzegovina'],'Bosnia y Herz.',pos=4,cex=0.8)
```
\rule{\textwidth}{0.4pt}

### Medidas de influencia
El procedimiento para determinar si un punto es influyente se puede hacer evaluando los cambios que ocurren en el modelo ajustado cuando se elimina dicha observación.

Por ejemplo, la Figura \@ref(fig:Un11valAtipElim) muestra cuanto cambian las estimaciones de $\beta_{1}$ y $\beta_{2}$ al eliminar un país a la vez. Aquí vemos que los cambios mas grandes ocurren cuando se eliminan a Guinea Ecuatorial o a Corea del Norte. Mientras que los países que identificamos como puntos de balanceo (Trinidad y Tobago y Somalia) o atípicos (Bosnia y Herzegovina) no tienen mucha influencia sobre las estimaciones.
....
```{r Un11valAtipElim, echo=F, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Datos de la ONU. Diferencias entre las estimaciones con todos los datos y las estimaciones obtenidas al eliminar una observación a la vez",warning=FALSE,message = FALSE}
est = mapply(function(x){
  UN11_i = UN11[-x,]
  mod = lm(log(fertility)~log(ppgdp) + pctUrban,data=UN11_i)
  mod$coefficients - mod.UN11$coefficients
},x=1:n)
est_i = t(est)
plot(est_i[,2],est_i[,3],ylim=range(est_i[,3])+c(-0.00005,+0.00005),
     xlim=range(est_i[,2])+c(-0.0005,+0.0051),
     xlab='log del PNB per cápita',
     ylab='% de población urbanas')

text(est_i[Names == 'Equatorial Guinea',2],est_i[Names == 'Equatorial Guinea',3],
     'Guinea Ecuatorial',pos=4,cex=0.7)
points(est_i[Names == 'Equatorial Guinea',2],est_i[Names == 'Equatorial Guinea',3],col=2)
text(est_i[Names == 'North Korea',2],est_i[Names == 'North Korea',3],'Corea del Norte',pos=4,cex=0.7)
points(est_i[Names == 'North Korea',2],est_i[Names == 'North Korea',3],col=2)


text(est_i[Names == 'Trinidad and Tobago',2],est_i[Names == 'Trinidad and Tobago',3],
     'Trinidad y Tobago',pos=4,cex=0.7)
points(est_i[Names == 'Trinidad and Tobago',2],est_i[Names == 'Trinidad and Tobago',3],col=2)
text(est_i[Names == 'Somalia',2],est_i[Names == 'Somalia',3],'Somalia',pos=4,cex=0.7)
points(est_i[Names == 'Somalia',2],est_i[Names == 'Somalia',3],col=2)
text(est_i[Names == 'Bosnia and Herzegovina',2],est_i[Names == 'Bosnia and Herzegovina',3],'Bosnia y Herz.',pos=4,cex=0.7)
points(est_i[Names == 'Bosnia and Herzegovina',2],est_i[Names == 'Bosnia and Herzegovina',3],col=2)
abline(h=0,lty=2)
abline(v=0,lty=2)
```

De igual forma se podría evaluar cuanto cambian las estimaciones de $E(y|\bx_{i})$ o las varianzas de los coeficientes $V(\beta_{j})$ al eliminar observaciones una a una.

A continuación se presentan algunos indicadores estadísticos para detectar puntos influyentes:

#### Distancia de Cook
Esta es una medida de la distancia entre las estimaciones por MCO basado en los $n$ puntos ($\hat{\bbeta}$), y el estimado obtenido eliminando el $i$-ésimo punto ($\hat{\bbeta}_{(i)}$). Es decir que es un indicador global de cuanto cambian todas las estimaciones de los coeficientes de regresión en conjunto. Esta medida se expresa de la siguiente forma:

\begin{equation}
\begin{split}
D_{i} &= \frac{(\hat{\bbeta}_{(i)}-\hat{\bbeta})'\bX'\bX(\hat{\bbeta}_{(i)}-\hat{\bbeta})'}{p \hat{\sigma}^{2}}   \\
      &= \frac{(\hat{\by}_{(i)} - \hat{\by} )'(\hat{\by}_{(i)} - \hat{\by} )}{p\hat{\sigma}^{2}} = \frac{r_{i}^{2}}{p}\frac{h_{ii}}{1-h_{ii}}, \qquad i=1,2,\ldots,n.
\end{split} \nonumber
\end{equation} 

Aquí podemos observar que $D_{i}$ consta de dos componentes, uno asociado a el residuo $(r_{i})$ y otro a la distancia del vector $\bx_{i}$ al centroide de la matriz de las covariables. Ambos (o alguno de ellos) puede contribuir a valores altos de este indicador.

Entonces, los puntos asociados a valores altos de $D_{i}$ tienen gran influencia sobre la estimación de $\bbeta$ por MCO. Se considera como un punto influyente si tiene asociado un $D_{i} > 4/n$ (algunos textos sugieren $D_{i} > 1$).

#### DFBETAS
Esta medida indica cuánto cambia el coeficiente de regresión $\hat{\beta}_{j}$, en unidades de desviaciones estándar, si se omitiera la $i$-ésima observación. Se calcula como:
\[
DFBETAS_{(i,j)} = \frac{\hat{\beta}_{j} - \hat{\beta}_{j(i)}}{\hat{\sigma}^{2}_{(i)}C_{jj}} = \frac{r_{(j,i)}}{\sqrt{\br_{j}'\br_{j}}}\frac{t_{i}}{\sqrt{1-h_{ii}}},
\]

donde $C_{jj}$ es el $j$-ésimo valor de la diagonal de $(\bX'\bX)^{2}$. $\br_{j}$ es la $j$-ésima fila de $\bR=(\bX'\bX)^{-1}\bX$.

Un valor grande de $DFBETAS_{(j,i)}$ indica que la observación $i$ tiene gran influencia sobre el $j$-ésimo coeficiente de regresión. Se sugiere que si $|DFBETAS_{(j,i)}| > 2/\sqrt{n}$ es necesario examinar la $i$-ésima observación.

#### DFFITS
Una medida que indica la influencia de la observación $i$-ésima sobre el valor ajustado ($\hat{y}_{i}$). Esta se calcula así:

\[
DFFITS_{i} = \frac{\hat{y}_{i} - \hat{y}_{(i)}}{\hat{\sigma}_{(i)}^{2}h_{ii}} = \sqrt{ \frac{h_{ii}}{1-h_{ii}} }t_{i}.
\]

El $DFFITS_{i}$ puede ser grande si el dato es atípico ($t_{i}$ grande) o si el dato tiene gran balanceo ($h_{ii}$ grande). Se sugiere que si $|DFFITS_{(j,i)}| > 2\sqrt{p/n}$ es necesario examinar la $i$-ésima observación.

#### COVRATIO
Los diagnósticos anteriores permiten ver el efecto de las observaciones sobre $\hat{\bbeta}$ o $\hat{\by}$. Pero no proporcionan información sobre la precisión general de la estimación. Una medida global de la precisión es la varianza generalizada:

$$ GV(\hat{\bbeta}) = | Var(\hat {\bbeta}) | = |\sigma^{2}(\bX'\bX)^{-1}|.$$

Para determinar la influencia de la $i$-ésima observación en la precisión de la estimación se define la razón de covarianzas:

\[
COVRATIO_{i} = \frac{|(\bX'_{(i)}\bX_{(i)})^{-1}\hat{\sigma}_{(i)}^{2}|}{| (\bX'\bX)^{-1}\hat{\sigma}^{2} |}= \left( \frac{S^{2}_{(i)}}{S^{2}} \right)^{p} \left( \frac{1}{1-h_{ii}} \right).
\]

Si $COVRATIO_{i} > 1 + 3p/n$ o $COVRATIO_{i} > 1 - 3p/n$ se debería considerar a la $i$-ésima observación como influyente para la precisión de $\hatbbeta$.

\rule{\textwidth}{0.4pt}
#### Datos de la ONU - medidas de influencia {-}
En R las medidas de influencia se pueden calcular usando la función `influence.measures`. Aunque también se pueden calcular cada indicador de forma independiente.  

```{r , echo=F,warning=FALSE,message = FALSE}
CD.UN11 = cooks.distance(mod.UN11)
```

La distancia de Cook se observa en la Figura \@ref(fig:Un11Cook). Aquí vemos que Guinea Ecuatorial y Corea del Norte son los países que más influyen en las estimaciones de los parámetros del modelo. Lo demás países presentan valores de la distancia de Cook mucho más bajos. Note que aunque observamos que Trinidad y Tobago es un punto de balanceo, este país no es influyente (valor de la distancia de Cook de $`r  round(CD.UN11[Names=='Trinidad and Tobago'],4)`$).

```{r Un11Cook, echo=T, fig.height = 4, fig.width = 5,fig.align = "center",fig.cap = "Datos de la ONU. Distancia de Cook.",warning=FALSE,message = FALSE}
CD.UN11 = cooks.distance(mod.UN11)
OrderCD.UN11 = order(CD.UN11,decreasing = T)
plot(CD.UN11,type='h',ylab = 'distancia de Cook',xlab='índice')
text(OrderCD.UN11[1:2],CD.UN11[OrderCD.UN11[1:2]],Names[OrderCD.UN11[1:2]],pos=4,cex=0.6)
abline(h=4/199,lty=2)
```

Ya identificamos que Guinea Ecuatorial y Corea del Norte son influyentes y pueden afectar considerablemente las estimaciones de los coeficientes de regresión. Pero esta influencia puede ser solamente sobre algún o algunos parámetros. Para ver la influencia sobre cada parámetro, la Figura \@ref(fig:UNdataDfBetas) muestra los DFBETAS para los coeficientes $\beta_{1}$ y $\beta_{2}$. En ambos gráficos podemos ver que estos dos países son influyentes para ambos parámetros. También se pude observar que otros pocos países tienen cierta influencia en la estimación de $\beta_{2}$, aunque los valores de los DFBETAS están muy cerca de los puntos de corte.

```{r UNdataDfBetas, echo=T, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de la ONU. DFBetas.",warning=FALSE,message = FALSE}
DFBetas.UN11 = dfbetas(mod.UN11)
OrderDB1.UN11 = order(abs(DFBetas.UN11[,2]),decreasing = T)
OrderDB2.UN11 = order(abs(DFBetas.UN11[,3]),decreasing = T)
par(mfrow=c(1,2))
plot(DFBetas.UN11[,2],ylab=quote('DFBETA'~(beta[1])),xlab='índice',main='(a)')
text(OrderDB1.UN11[1:2],DFBetas.UN11[OrderDB1.UN11[1:2],2],
     Names[OrderDB1.UN11[1:2]],pos=4,cex=0.6)
abline(h = c(-1,1)*2/sqrt(199),lty=2)
plot(DFBetas.UN11[,3],ylab = quote('DFBETA'~(beta[2])),xlab='índice',main='(b)')
text(OrderDB2.UN11[1:2],DFBetas.UN11[OrderDB2.UN11[1:2],3],
     Names[OrderDB2.UN11[1:2]],pos=4,cex=0.6)
abline(h = c(-1,1)*2/sqrt(199),lty=2)
```
Los DFFITS y COVRATIO se observan en la Figura \@ref(fig:UNdataOtros). A partir de los DFFITS se puede concluir que Guinea Ecuatorial y Corea del Norte también tienen gran influencia sobre las predicciones. A partir de los COVRATIO vemos que Trinidad y Tobago, Guinea Ecuatorial, Bosnia y Herzegovina, y Moldavia tienen gran influencia en la varianza de las estimaciones de los coeficientes de regresión. Note que aunque Corea del Norte fue influyente para las estimaciones, este país no influye en la varianza de estas.

```{r UNdataOtros, echo=T, fig.height = 4, fig.width = 8,fig.align = "center",fig.cap = "Datos de la ONU. DFFITS y COVRATIO.",warning=FALSE,message = FALSE}
DFfits.UN11 = dffits(mod.UN11)
OrderDFF.UN11 = order(abs(DFfits.UN11),decreasing = T)

Covratio.UN11 = covratio(mod.UN11)
OrderCR.UN11 = order(abs(1-Covratio.UN11),decreasing = T)
par(mfrow=c(1,2))
plot(DFfits.UN11,ylab='DFFITS',xlab='índice',main='(a)')
text(OrderDFF.UN11[1:2],DFfits.UN11[OrderDFF.UN11[1:2]],
     Names[OrderDFF.UN11[1:2]],pos=4,cex=0.6)
abline(h = c(-1,1)*2*sqrt(3/199),lty=2)
plot(Covratio.UN11,ylab = 'COVRATIO',xlab='índice',main='(b)')
text(OrderCR.UN11[1:4],Covratio.UN11[OrderCR.UN11[1:4]],
     Names[OrderCR.UN11[1:4]],pos=c(4,2,4,4),cex=0.6)
abline(h = 1+c(-1,1)*3*3/199,lty=2)
```
A partir de estos indicadores encontramos que Guinea Ecuatorial y Corea del Norte son observaciones influyentes en la estimación del modelo \@ref(eq:modUN). Particularmente, Guinea Ecuatorial tiene una tasa de fertilidad (4.98) muy superior a la estimada por el modelo. Esto se debe que los países con PNB y porcentaje de población urbana similares a este país tienen tasas de fertilidad más baja. Caso contrario pasa con Corea del Norte. 

Para disminuir la influencia de estos países se pueden incorporar nuevas covariables dentro del modelo que ayuden a explicar estas discrepancias. Por ejemplo se podría ingresar una covariable asociada al continente. 
\rule{\textwidth}{0.4pt}


## Comentarios finales
- Dentro de la literatura hay muchos puntos de corte diferentes para los indicadores de observaciones influyentes. Esto es porque es difícil de determinar las distribuciones muestrales. Por lo que se recomienda verificar si hay algunas observaciones que tenga valores muy altos con respecto a los demás.
- Las observaciones influyentes o atípicas se deben descartar si estas corresponden a errores de medición o si son inválidas (por ejemplo, si pertenecen a otra población). 
- Pero si las observaciones influyentes o atípicas son válidas, no hay justificación para eliminarla. Lo que se puede hacer es incluir nuevas covariables que puedan explicar mejor los datos y reducir la influencia de estas observaciones. En los datos de la ONU podríamos incluir covariables relacionadas con el continente.

<!--chapter:end:05-PuntosInfluyentes.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

